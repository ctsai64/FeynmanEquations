{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(tensor, epsilon=1e-5):\n",
    "    shape = tensor.shape\n",
    "    flat_tensor = tensor.view(-1)\n",
    "    grad = torch.zeros_like(flat_tensor)\n",
    "    \n",
    "    for i in range(flat_tensor.numel()):\n",
    "        original_value = flat_tensor[i].item()\n",
    "        \n",
    "        flat_tensor[i] = original_value + epsilon\n",
    "        perturbed_tensor = flat_tensor.view(shape)\n",
    "        f_plus = perturbed_tensor\n",
    "        \n",
    "        flat_tensor[i] = original_value - epsilon\n",
    "        perturbed_tensor = flat_tensor.view(shape)\n",
    "        f_minus = perturbed_tensor\n",
    "        \n",
    "        grad[i] = (f_plus - f_minus).mean() / (2 * epsilon)\n",
    "        flat_tensor[i] = original_value\n",
    "    \n",
    "    return grad.view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(self, output, target):\n",
    "        target_range = torch.max(target, dim=-1, keepdim=True)[0] - torch.min(target, dim=-1, keepdim=True)[0]\n",
    "        target_range = torch.clamp(target_range, min=1e-6).squeeze(-1)\n",
    "        loss = ((target - output)**2)/target_range\n",
    "        l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        total_loss = loss + 0.01*l2_reg + 0.01*l1_reg\n",
    "        return total_loss\n",
    "\n",
    "def model_evaluate_function(self, params, symbols, formula):\n",
    "    var_values = {symbols[j]: params[:, j] for j in range(len(symbols))}\n",
    "    lambda_func = sp.lambdify(symbols, formula, modules=['numpy'])\n",
    "    np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "    with np.errstate(all='ignore'): #need to fix this at some point\n",
    "        evaluated = lambda_func(**np_values)\n",
    "    evaluated = np.nan_to_num(evaluated, nan=0.0)\n",
    "    return torch.tensor(evaluated, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.num_params = num_params\n",
    "        self.symbols = symbols\n",
    "        self.input_channels = input_channels\n",
    "        self.params = sum(self.num_params)\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.params),\n",
    "        )\n",
    "\n",
    "    def loss_func(self, output, target):\n",
    "        target_range = torch.max(target, dim=-1, keepdim=True)[0] - torch.min(target, dim=-1, keepdim=True)[0]\n",
    "        target_range = torch.clamp(target_range, min=1e-6).squeeze(-1)\n",
    "        #print(f\"target_range: {target_range}\")\n",
    "        loss = ((target - output)**2)/target_range\n",
    "        l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        total_loss = loss + 0.01*l2_reg + 0.01*l1_reg\n",
    "        return total_loss\n",
    "\n",
    "    def evaluate_function(self, params, symbols, formula):\n",
    "        var_values = {symbols[j]: params[:, j] for j in range(len(symbols))}\n",
    "        lambda_func = sp.lambdify(symbols, formula, modules=['numpy'])\n",
    "        np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "        with np.errstate(all='ignore'): #need to fix this at some point\n",
    "            evaluated = lambda_func(**np_values)\n",
    "        evaluated = np.nan_to_num(evaluated, nan=0.0)\n",
    "        return torch.tensor(evaluated, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        out = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(out, (n, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        out = torch.reshape(out, (n, 2, 128))\n",
    "        out = self.hidden_x2(out)\n",
    "        cnn_flat = self.flatten_layer(out)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "        \n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        #print(self.params)\n",
    "        #print(f\"whole embedding: {embedding.shape}\")\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            #print(f\"embedding: {embedding[:, start_index:start_index+self.num_params[f]].shape}\")\n",
    "            output = self.evaluate_function(\n",
    "                embedding[:, start_index:start_index+self.num_params[f]],\n",
    "                self.symbols[f],\n",
    "                self.functions[f]\n",
    "            ).to(device)\n",
    "            outputs.append(output)\n",
    "            loss = self.loss_func(output, x[:,0,0])\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "        \n",
    "        #print(f\"loss len: {len(losses)}\")      \n",
    "        '''best_index = torch.argmin(torch.tensor(losses))\n",
    "        best_func = self.functions[best_index]\n",
    "        best_loss, best_out = losses[best_index], outputs[best_index]'''\n",
    "\n",
    "        stacked_outputs = torch.stack(outputs)\n",
    "        stacked_losses = torch.stack(losses)\n",
    "        #print(f\"stacked_outputs shape: {stacked_outputs.shape}\")\n",
    "        #print(f\"stacked_losses shape: {stacked_losses.shape}\")\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0) \n",
    "        #print(f\"best_loss shape: {best_loss.shape}\")\n",
    "        #print(f\"best_indexes shape: {best_indexes.shape}\")\n",
    "        #print(f\"best indexes 0: {best_indexes[0]}\")\n",
    "        best_out = stacked_outputs[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "        #print(f\"best_func for 0: {best_func[0]}\")\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.num_params = num_params\n",
    "        self.symbols = symbols\n",
    "        self.input_channels = input_channels\n",
    "        self.params = sum(self.num_params)\n",
    "        self.sequence_length=96\n",
    "        self.input_channel=2\n",
    "        self.cov1d_size=128\n",
    "\n",
    "        self.cov1d = nn.Conv1d(self.input_channel, self.cov1d_size, 3, stride=1, padding=1)\n",
    "        self.flattened_size = self.cov1d_size * self.sequence_length\n",
    "        self.dense = nn.Linear(self.flattened_size, self.num_params)\n",
    "        \n",
    "        self.selu_1 = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"out initial: {x.shape}\")\n",
    "        out = self.cov1d(x)\n",
    "        print(f\"out after cov1d: {out.shape}\")\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(f\"out before dense: {out.shape}\")\n",
    "        out = self.dense(out)\n",
    "        out = self.selu_1(out)\n",
    "        \n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            output = model_evaluate_function(\n",
    "                out[:, start_index:start_index+self.num_params[f]],\n",
    "                self.symbols[f],\n",
    "                self.functions[f]\n",
    "            ).to(device)\n",
    "            outputs.append(output)\n",
    "            loss = loss_func(output, x[:,0,0])\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "        \n",
    "        stacked_outputs = torch.stack(outputs)\n",
    "        stacked_losses = torch.stack(losses)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0) \n",
    "        best_out = stacked_outputs[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func_ThreeChannels(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.formulas = functions\n",
    "        self.num_params = num_params\n",
    "        self.symbols = symbols\n",
    "        self.params = sum(self.num_params)\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3, out_channels=8, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.params),\n",
    "        )\n",
    "\n",
    "    def loss_func(self, outputs, targets):\n",
    "        # outputs and targets lists of tensors (one for each channel)\n",
    "        losses = []\n",
    "        for output, target in zip(outputs, targets):\n",
    "            #target_range = torch.max(target, dim=-1, keepdim=True)[0] - torch.min(target, dim=-1, keepdim=True)[0]\n",
    "            #target_range = torch.clamp(target_range, min=1e-6).squeeze(-1)\n",
    "            #print(target.shape, output.shape, target_range.shape)\n",
    "            loss = ((target - output)**2)# / target_range\n",
    "            losses.append(loss.mean(dim=1))\n",
    "\n",
    "        total_losses = torch.stack(losses).mean(dim=1)\n",
    "        #l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        #l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        #total_losses += 0.01 * l2_reg + 0.01 * l1_reg\n",
    "        return total_losses\n",
    "\n",
    "    def evaluate_function(self, params, symbols, formula):\n",
    "        var_values = {symbols[j]: params[:, j] for j in range(len(symbols))}\n",
    "        lambda_func = sp.lambdify(symbols, formula, modules=['numpy'])\n",
    "        np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "        with np.errstate(all='ignore'): #need to fix this at some point\n",
    "            evaluated = lambda_func(**np_values)\n",
    "        evaluated = np.nan_to_num(evaluated, nan=0.0)\n",
    "        return torch.tensor(evaluated, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def compute_derivative(self, params, index, epsilon=1e-6):\n",
    "        batch_size = params.shape[0]\n",
    "        num_params = params.shape[1]\n",
    "        gradients = torch.zeros(batch_size, num_params)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            param_tensor = params[i].clone().detach().requires_grad_(True)\n",
    "            if param_tensor.dim() == 1:\n",
    "                param_tensor = param_tensor.unsqueeze(0)\n",
    "                perturbed_params_pos = param_tensor.clone()\n",
    "                perturbed_params_neg = param_tensor.clone()\n",
    "                \n",
    "                perturbed_params_pos[0] += epsilon\n",
    "                forward_value = self.evaluate_function(perturbed_params_pos, self.symbols[index], self.formulas[index])\n",
    "                \n",
    "                perturbed_params_neg[0] -= epsilon\n",
    "                backward_value = self.evaluate_function(perturbed_params_neg, self.symbols[index], self.formulas[index])\n",
    "                gradients[i, 0] = (forward_value - backward_value) / (2 * epsilon)\n",
    "                break\n",
    "            \n",
    "            for j in range(num_params):\n",
    "                perturbed_params_pos = param_tensor.clone()\n",
    "                perturbed_params_neg = param_tensor.clone()\n",
    "                \n",
    "                perturbed_params_pos[:, j] += epsilon\n",
    "                forward_value = self.evaluate_function(perturbed_params_pos, self.symbols[index], self.formulas[index])\n",
    "                \n",
    "                perturbed_params_neg[:, j] -= epsilon\n",
    "                backward_value = self.evaluate_function(perturbed_params_neg, self.symbols[index], self.formulas[index])\n",
    "                gradients[i, j] = (forward_value - backward_value) / (2 * epsilon)\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "    def compute_hessians(self, parameters, index, epsilon=1e-6):\n",
    "        batch_size = parameters.shape[0]\n",
    "        max_num_params = parameters.shape[1]\n",
    "        hessians = torch.zeros((batch_size, max_num_params, max_num_params))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            param_tensor = parameters[i].clone().detach().requires_grad_(True)\n",
    "            perturbed_params = param_tensor.clone()\n",
    "            if param_tensor.dim() == 1:\n",
    "                perturbed_params = perturbed_params.unsqueeze(0)\n",
    "                f_plus_minus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])\n",
    "                perturbed_params[0] += epsilon\n",
    "                f_plus_plus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])                    \n",
    "                perturbed_params[0] -= 2 * epsilon\n",
    "                f_minus_minus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])                   \n",
    "                hessians[i, 0, 0] = (f_plus_plus - (2 * f_plus_minus) + f_minus_minus) / (4 * epsilon**2)\n",
    "                break\n",
    "        \n",
    "            for j in range(max_num_params):\n",
    "                for k in range(max_num_params):\n",
    "                    perturbed_params[:,j,:] += epsilon\n",
    "                    perturbed_params[:,k,:] += epsilon\n",
    "                    f_plus_plus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])                    \n",
    "                    perturbed_params[:,k,:] -= 2 * epsilon\n",
    "                    f_plus_minus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])                   \n",
    "                    perturbed_params[:,j,:] -= 2 * epsilon\n",
    "                    perturbed_params[:,k,:] += 2 * epsilon\n",
    "                    f_minus_plus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])                    \n",
    "                    perturbed_params[:,k,:] -= 2 * epsilon\n",
    "                    f_minus_minus = self.evaluate_function(perturbed_params, self.symbols[index], self.formulas[index])\n",
    "                    hessians[i, j, k] = (f_plus_plus - f_plus_minus - f_minus_plus + f_minus_minus) / (4 * epsilon**2)\n",
    "        return hessians\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden_x1(x.swapaxes(1, 2))\n",
    "        xfc = torch.reshape(out, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        out = torch.reshape(out, (-1, 2, 128))\n",
    "        out = self.hidden_x2(out)\n",
    "        cnn_flat = self.flatten_layer(out)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "        \n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "\n",
    "        for f in range(len(self.formulas)):\n",
    "            output = self.evaluate_function(\n",
    "                embedding[:, start_index:start_index+self.num_params[f]],\n",
    "                self.symbols[f],\n",
    "                self.formulas[f]\n",
    "            ).to(self.device)\n",
    "            outputs.append(output)\n",
    "            der = self.compute_derivative(embedding[:, start_index:start_index+self.num_params[f]], f).to(self.device)\n",
    "            hess = self.compute_hessians(embedding[:, start_index:start_index+self.num_params[f]], f).to(self.device)\n",
    "\n",
    "            hess = torch.flatten(hess, start_dim=1)\n",
    "            output = output.unsqueeze(1)\n",
    "            output = F.pad(output, (0, hess.size(1) - output.size(1)))\n",
    "            der = F.pad(der, (0, hess.size(-1) - der.size(-1)))\n",
    "\n",
    "            pred = torch.stack([output, der, hess], dim=2)\n",
    "            pred = F.pad(pred, (0, 0, (x.size(1) - pred.size(1)), 0, 0, 0))\n",
    "\n",
    "            loss = self.loss_func(pred, x)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "\n",
    "        stacked_outputs = torch.stack(outputs)\n",
    "        stacked_losses = torch.stack(losses)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0)\n",
    "        best_out = stacked_outputs[best_indexes, -1]\n",
    "        best_func = [self.formulas[idx] for idx in best_indexes]\n",
    "        print(best_func[0])\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1490621/2816734550.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load('hold_data.pth')\n"
     ]
    }
   ],
   "source": [
    "loaded_data = torch.load('hold_data.pth')\n",
    "\n",
    "results = loaded_data['results']\n",
    "formulas = loaded_data['formulas']\n",
    "symbols = loaded_data['symbols']\n",
    "params = loaded_data['params']\n",
    "num_params = loaded_data['num_params']\n",
    "derivatives = loaded_data['derivatives']\n",
    "hessians = loaded_data['hessians']\n",
    "full_data = loaded_data['full_data']\n",
    "flat_data = loaded_data['flattened_data']\n",
    "\n",
    "dataloader = DataLoader(flat_data, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 81, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_function(params, symbols, formula):\n",
    "    if params.shape[0] != 1:\n",
    "        var_values = {symbols[j]: params[j].item() for j in range(len(symbols))}\n",
    "    else:\n",
    "        var_values = {symbols[j]: params[:, j, :].item() for j in range(len(symbols))}\n",
    "    evaluated = formula.subs(var_values)\n",
    "    return float(evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss_func(model, output, target):\n",
    "    target_max = torch.max(target, dim=-1, keepdim=True)[0]\n",
    "    target_min = torch.min(target, dim=-1, keepdim=True)[0]\n",
    "    target_range = torch.clamp(target_max - target_min, min=1e-6).squeeze(-1)\n",
    "    \n",
    "    mse_loss = torch.mean((output - target) ** 2, dim=-1)\n",
    "    normalized_loss = mse_loss / target_range\n",
    "    \n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    l1_reg = sum(p.abs().sum() for p in model.parameters())\n",
    "    \n",
    "    output_derivative = numerical_derivative(output)\n",
    "    target_derivative = numerical_derivative(target)\n",
    "    \n",
    "    derivative_diff = torch.mean((output_derivative - target_derivative) ** 2, dim=-1)\n",
    "    \n",
    "    total_loss = torch.mean(normalized_loss) + 0.01 * l2_reg + 0.01 * l1_reg + 0.1 * torch.mean(derivative_diff)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "Nn*mu\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "Nn*mu\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "Nn*mu\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "x1*y1 + x2*y2 + x3*y3\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "G*m1*m2/((-x1 + x2)**2 + (-y1 + y2)**2 + (-z1 + z2)**2)\n",
      "G*m1*m2/((-x1 + x2)**2 + (-y1 + y2)**2 + (-z1 + z2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-theta**2/2)/(2*sqrt(pi))\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-(theta - theta1)**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n",
      "m_0/sqrt(1 - v**2/c**2)\n",
      "sqrt((-x1 + x2)**2 + (-y1 + y2)**2)\n",
      "sqrt(2)*exp(-theta**2/(2*sigma**2))/(2*sqrt(pi)*sigma)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m best_out,_,_,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print(f\"best_out: {best_out}\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(f\"train_batch: {train_batch[:, 0, 0]}\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m training_loss_func(model, best_out, train_batch[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 178\u001b[0m, in \u001b[0;36mMulti_Func_ThreeChannels.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    177\u001b[0m der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_derivative(embedding[:, start_index:start_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_params[f]], f)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 178\u001b[0m hess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_hessians\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    180\u001b[0m hess \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(hess, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 132\u001b[0m, in \u001b[0;36mMulti_Func_ThreeChannels.compute_hessians\u001b[0;34m(self, parameters, index, epsilon)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    131\u001b[0m     perturbed_params \u001b[38;5;241m=\u001b[39m perturbed_params\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m     f_plus_minus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbols\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformulas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     perturbed_params[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon\n\u001b[1;32m    134\u001b[0m     f_plus_plus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_function(perturbed_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbols[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformulas[index])                    \n",
      "Cell \u001b[0;32mIn[16], line 82\u001b[0m, in \u001b[0;36mMulti_Func_ThreeChannels.evaluate_function\u001b[0;34m(self, params, symbols, formula)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, symbols, formula):\n\u001b[1;32m     81\u001b[0m     var_values \u001b[38;5;241m=\u001b[39m {symbols[j]: params[:, j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(symbols))}\n\u001b[0;32m---> 82\u001b[0m     lambda_func \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambdify\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     np_values \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(sym): var_values[sym]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m sym \u001b[38;5;129;01min\u001b[39;00m symbols}\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;66;03m#need to fix this at some point\u001b[39;00m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/utilities/lambdify.py:880\u001b[0m, in \u001b[0;36mlambdify\u001b[0;34m(args, expr, modules, printer, use_imps, dummify, cse, docstring_limit)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    879\u001b[0m     cses, _expr \u001b[38;5;241m=\u001b[39m (), expr\n\u001b[0;32m--> 880\u001b[0m funcstr \u001b[38;5;241m=\u001b[39m \u001b[43mfuncprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuncname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# Collect the module imports from the code printers.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m imp_mod_lines \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/utilities/lambdify.py:1145\u001b[0m, in \u001b[0;36m_EvaluatorPrinter.doprint\u001b[0;34m(self, funcname, args, expr, cses)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     cses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(subvars, subexprs)\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1145\u001b[0m     argstrs, expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# Generate argument unpacking and final argument list\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m funcargs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/utilities/lambdify.py:1207\u001b[0m, in \u001b[0;36m_EvaluatorPrinter._preprocess\u001b[0;34m(self, args, expr)\u001b[0m\n\u001b[1;32m   1203\u001b[0m dummify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dummify \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(arg, Dummy) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m flatten(args))\n\u001b[1;32m   1206\u001b[0m argstrs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(args)\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(ordered(\u001b[38;5;28mzip\u001b[39m(args, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args)))))):\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iterable(arg):\n\u001b[1;32m   1209\u001b[0m         s, expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(arg, expr)\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/core/sorting.py:312\u001b[0m, in \u001b[0;36mordered\u001b[0;34m(seq, keys, default, warn)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(u) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot enough keys to break ties: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m u)\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m value\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/core/sorting.py:294\u001b[0m, in \u001b[0;36mordered\u001b[0;34m(seq, keys, default, warn)\u001b[0m\n\u001b[1;32m    292\u001b[0m         keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[0;32m--> 294\u001b[0m         d[\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m default:\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/core/sorting.py:195\u001b[0m, in \u001b[0;36m_nodes\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _node_count(e)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m iterable(e):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(_nodes(ei) \u001b[38;5;28;01mfor\u001b[39;00m ei \u001b[38;5;129;01min\u001b[39;00m e)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(_nodes(k) \u001b[38;5;241m+\u001b[39m _nodes(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/core/sorting.py:195\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _node_count(e)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m iterable(e):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43m_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mei\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ei \u001b[38;5;129;01min\u001b[39;00m e)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(_nodes(k) \u001b[38;5;241m+\u001b[39m _nodes(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/sympy/core/sorting.py:187\u001b[0m, in \u001b[0;36m_nodes\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03mA helper for ordered() which returns the node count of ``e`` which\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mfor Basic objects is the number of Basic nodes in the expression tree\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mbut for other objects is 1 (unless the object is an iterable or dict\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mfor which the sum of nodes is returned).\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Basic\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Derivative\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, Basic):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, Derivative):\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Multi_Func_ThreeChannels(functions=formulas, num_params=num_params, symbols=symbols, device=device).to(device)\n",
    "\n",
    "#loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    model.train()\n",
    "    \n",
    "    for train_batch in dataloader:\n",
    "        train_batch = train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        best_out,_,_,_,_ = model(train_batch)\n",
    "        #print(f\"best_out: {best_out}\")\n",
    "        #print(f\"train_batch: {train_batch[:, 0, 0]}\")\n",
    "        loss = training_loss_func(model, best_out, train_batch[:, 0, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r = np.random.randint(data.shape[0])\\nprint(data[r].shape)\\nplt.plot(data[r].detach().numpy());'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''r = np.random.randint(data.shape[0])\n",
    "print(data[r].shape)\n",
    "plt.plot(data[r].detach().numpy());'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
