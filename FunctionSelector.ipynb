{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(tensor, epsilon=1e-5):\n",
    "    shape = tensor.shape\n",
    "    flat_tensor = tensor.view(-1)\n",
    "    grad = torch.zeros_like(flat_tensor)\n",
    "    \n",
    "    for i in range(flat_tensor.numel()):\n",
    "        original_value = flat_tensor[i].item()\n",
    "        \n",
    "        flat_tensor[i] = original_value + epsilon\n",
    "        perturbed_tensor = flat_tensor.view(shape)\n",
    "        f_plus = perturbed_tensor\n",
    "        \n",
    "        flat_tensor[i] = original_value - epsilon\n",
    "        perturbed_tensor = flat_tensor.view(shape)\n",
    "        f_minus = perturbed_tensor\n",
    "        \n",
    "        grad[i] = (f_plus - f_minus).mean() / (2 * epsilon)\n",
    "        flat_tensor[i] = original_value\n",
    "    \n",
    "    return grad.view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(self, output, target):\n",
    "        target_range = torch.max(target, dim=-1, keepdim=True)[0] - torch.min(target, dim=-1, keepdim=True)[0]\n",
    "        target_range = torch.clamp(target_range, min=1e-6).squeeze(-1)\n",
    "        loss = ((target - output)**2)/target_range\n",
    "        l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        total_loss = loss + 0.01*l2_reg + 0.01*l1_reg\n",
    "        return total_loss\n",
    "\n",
    "def model_evaluate_function(self, params, symbols, formula):\n",
    "    var_values = {symbols[j]: params[:, j] for j in range(len(symbols))}\n",
    "    lambda_func = sp.lambdify(symbols, formula, modules=['numpy'])\n",
    "    np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "    with np.errstate(all='ignore'): #need to fix this at some point\n",
    "        evaluated = lambda_func(**np_values)\n",
    "    evaluated = np.nan_to_num(evaluated, nan=0.0)\n",
    "    return torch.tensor(evaluated, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.num_params = num_params\n",
    "        self.symbols = symbols\n",
    "        self.input_channels = input_channels\n",
    "        self.params = sum(self.num_params)\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.params),\n",
    "        )\n",
    "\n",
    "    def loss_func(self, output, target):\n",
    "        target_range = torch.max(target, dim=-1, keepdim=True)[0] - torch.min(target, dim=-1, keepdim=True)[0]\n",
    "        target_range = torch.clamp(target_range, min=1e-6).squeeze(-1)\n",
    "        #print(f\"target_range: {target_range}\")\n",
    "        loss = ((target - output)**2)/target_range\n",
    "        l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        total_loss = loss + 0.01*l2_reg + 0.01*l1_reg\n",
    "        return total_loss\n",
    "\n",
    "    def evaluate_function(self, params, symbols, formula):\n",
    "        var_values = {symbols[j]: params[:, j] for j in range(len(symbols))}\n",
    "        lambda_func = sp.lambdify(symbols, formula, modules=['numpy'])\n",
    "        np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "        with np.errstate(all='ignore'): #need to fix this at some point\n",
    "            evaluated = lambda_func(**np_values)\n",
    "        evaluated = np.nan_to_num(evaluated, nan=0.0)\n",
    "        return torch.tensor(evaluated, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "        out = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(out, (n, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        out = torch.reshape(out, (n, 2, 128))\n",
    "        out = self.hidden_x2(out)\n",
    "        cnn_flat = self.flatten_layer(out)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "        \n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        #print(self.params)\n",
    "        #print(f\"whole embedding: {embedding.shape}\")\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            #print(f\"embedding: {embedding[:, start_index:start_index+self.num_params[f]].shape}\")\n",
    "            output = self.evaluate_function(\n",
    "                embedding[:, start_index:start_index+self.num_params[f]],\n",
    "                self.symbols[f],\n",
    "                self.functions[f]\n",
    "            ).to(device)\n",
    "            outputs.append(output)\n",
    "            loss = self.loss_func(output, x[:,0,0])\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "        \n",
    "        #print(f\"loss len: {len(losses)}\")      \n",
    "        '''best_index = torch.argmin(torch.tensor(losses))\n",
    "        best_func = self.functions[best_index]\n",
    "        best_loss, best_out = losses[best_index], outputs[best_index]'''\n",
    "\n",
    "        stacked_outputs = torch.stack(outputs)\n",
    "        stacked_losses = torch.stack(losses)\n",
    "        #print(f\"stacked_outputs shape: {stacked_outputs.shape}\")\n",
    "        #print(f\"stacked_losses shape: {stacked_losses.shape}\")\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0) \n",
    "        #print(f\"best_loss shape: {best_loss.shape}\")\n",
    "        #print(f\"best_indexes shape: {best_indexes.shape}\")\n",
    "        #print(f\"best indexes 0: {best_indexes[0]}\")\n",
    "        best_out = stacked_outputs[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "        #print(f\"best_func for 0: {best_func[0]}\")\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.num_params = num_params\n",
    "        self.symbols = symbols\n",
    "        self.input_channels = input_channels\n",
    "        self.params = sum(self.num_params)\n",
    "        self.sequence_length=96\n",
    "        self.input_channel=2\n",
    "        self.cov1d_size=128\n",
    "\n",
    "        self.cov1d = nn.Conv1d(self.input_channel, self.cov1d_size, 3, stride=1, padding=1)\n",
    "        self.flattened_size = self.cov1d_size * self.sequence_length\n",
    "        self.dense = nn.Linear(self.flattened_size, self.num_params)\n",
    "        \n",
    "        self.selu_1 = nn.SELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"out initial: {x.shape}\")\n",
    "        out = self.cov1d(x)\n",
    "        print(f\"out after cov1d: {out.shape}\")\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(f\"out before dense: {out.shape}\")\n",
    "        out = self.dense(out)\n",
    "        out = self.selu_1(out)\n",
    "        \n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            output = model_evaluate_function(\n",
    "                out[:, start_index:start_index+self.num_params[f]],\n",
    "                self.symbols[f],\n",
    "                self.functions[f]\n",
    "            ).to(device)\n",
    "            outputs.append(output)\n",
    "            loss = loss_func(output, x[:,0,0])\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "        \n",
    "        stacked_outputs = torch.stack(outputs)\n",
    "        stacked_losses = torch.stack(losses)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0) \n",
    "        best_out = stacked_outputs[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func_ThreeChannels(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.num_params = num_params\n",
    "        self.symbols = symbols\n",
    "        self.params = sum(self.num_params)\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3, out_channels=8, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.params),\n",
    "        )\n",
    "\n",
    "    def loss_func(self, outputs, targets):\n",
    "        # outputs and targets lists of tensors (one for each channel)\n",
    "        losses = []\n",
    "        for output, target in zip(outputs, targets):\n",
    "            target_range = torch.max(target, dim=-1, keepdim=True)[0] - torch.min(target, dim=-1, keepdim=True)[0]\n",
    "            target_range = torch.clamp(target_range, min=1e-6).squeeze(-1)\n",
    "            loss = ((target - output)**2) / target_range\n",
    "            losses.append(loss)\n",
    "\n",
    "        total_loss = sum(losses)\n",
    "        l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        total_loss += 0.01 * l2_reg + 0.01 * l1_reg\n",
    "        return total_loss\n",
    "\n",
    "    def evaluate_function(self, params, symbols, formula):\n",
    "        var_values = {symbols[j]: params[:, j] for j in range(len(symbols))}\n",
    "        lambda_func = sp.lambdify(symbols, formula, modules=['numpy'])\n",
    "        np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "        with np.errstate(all='ignore'): #need to fix this at some point\n",
    "            evaluated = lambda_func(**np_values)\n",
    "        evaluated = np.nan_to_num(evaluated, nan=0.0)\n",
    "        return torch.tensor(evaluated, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "        out = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(out, (n, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        out = torch.reshape(out, (n, 2, 128))\n",
    "        out = self.hidden_x2(out)\n",
    "        cnn_flat = self.flatten_layer(out)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "        \n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "\n",
    "        for f in range(len(self.functions)):\n",
    "            output = self.evaluate_function(\n",
    "                embedding[:, start_index:start_index+self.num_params[f]],\n",
    "                self.symbols[f],\n",
    "                self.functions[f]\n",
    "            ).to(self.device)\n",
    "            outputs.append(output)\n",
    "            target = x[:, f, :]  # fix\n",
    "            loss = self.loss_func([output], [target])\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "\n",
    "        stacked_outputs = torch.stack(outputs)\n",
    "        stacked_losses = torch.stack(losses)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0)\n",
    "        best_out = stacked_outputs[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "\n",
    "        return best_out, best_loss, best_func, outputs, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i, func in enumerate(functions):\\n    print(f\"Function {i+1}:\")\\n    print(f\"  Formula: {func[\\'formula\\']}\")\\n    print(f\"  Number of Variables: {num_vars_per_func[i]}\")\\n    print(\"  Variables:\")\\n    for var in func[\\'variables\\']:\\n        print(f\"    - Name: {var[\\'name\\']}, Range: ({var[\\'low\\']}, {var[\\'high\\']})\")\\n    print()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('FeynmanEquations.csv')\n",
    "\n",
    "functions = []\n",
    "num_vars_per_func = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    formula = row['Formula']\n",
    "    num_vars = row['# variables']\n",
    "    function_details = {\n",
    "        'formula': formula,\n",
    "        'variables': []\n",
    "    }\n",
    "    \n",
    "    for i in range(1, 11):  \n",
    "        v_name = row.get(f'v{i}_name', None)\n",
    "        v_low = row.get(f'v{i}_low', None)\n",
    "        v_high = row.get(f'v{i}_high', None)\n",
    "        \n",
    "        if pd.notna(v_name):\n",
    "            function_details['variables'].append({\n",
    "                'name': v_name,\n",
    "                'low': v_low,\n",
    "                'high': v_high\n",
    "            })\n",
    "    \n",
    "    functions.append(function_details)\n",
    "    num_vars_per_func.append(num_vars)\n",
    "\n",
    "'''for i, func in enumerate(functions):\n",
    "    print(f\"Function {i+1}:\")\n",
    "    print(f\"  Formula: {func['formula']}\")\n",
    "    print(f\"  Number of Variables: {num_vars_per_func[i]}\")\n",
    "    print(\"  Variables:\")\n",
    "    for var in func['variables']:\n",
    "        print(f\"    - Name: {var['name']}, Range: ({var['low']}, {var['high']})\")\n",
    "    print()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_function(function, sample_size, device):\n",
    "    hold = []\n",
    "    sympy_symbols = []\n",
    "    \n",
    "    for var in function[\"variables\"]:\n",
    "        sym = sp.symbols(var[\"name\"])\n",
    "        sympy_symbols.append(sym)\n",
    "        min_val = var[\"low\"]\n",
    "        max_val = var[\"high\"]\n",
    "        v = (max_val - min_val) * torch.rand(sample_size, 1, 1) + min_val\n",
    "        hold.append(v)\n",
    "    \n",
    "    params = torch.stack(hold)\n",
    "    params = torch.atleast_2d(params)\n",
    "    params = torch.transpose(params, 0, 1)\n",
    "    params = torch.transpose(params, 1, 2)\n",
    "    params = params.to(device)\n",
    "\n",
    "    formula = sp.sympify(function[\"formula\"])``\n",
    "    \n",
    "    results = []\n",
    "    for i in range(sample_size):\n",
    "        var_values = {sympy_symbols[j]: params[i, 0, j].item() for j in range(len(sympy_symbols))}\n",
    "        evaluated = formula.subs(var_values)\n",
    "        results.append(float(evaluated))\n",
    "    \n",
    "    return results, formula, sympy_symbols, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "hold = []\n",
    "for f in functions[0:10]:\n",
    "    try:\n",
    "        results = generate_function(f, sample_size, device)\n",
    "        hold.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing function {f}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_data = [torch.tensor(l[0]) for l in hold]\n",
    "data = torch.stack(hold_data).reshape(-1)\n",
    "hold_formulas = [l[1] for l in hold]\n",
    "hold_symbols = [l[2] for l in hold]\n",
    "hold_params = [l[3] for l in hold]\n",
    "hold_num_params = [len(l[2]) for l in hold]\n",
    "\n",
    "dataloader = DataLoader(data, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Gradients:\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def compute_derivative(parameters, function_values, epsilon=1e-6): #need to fix\n",
    "    batch_size, max_num_params = parameters.shape\n",
    "    output_dim = function_values.shape[1]\n",
    "    gradients = torch.zeros_like(parameters)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        param_tensor = parameters[i].clone().detach().requires_grad_(True)\n",
    "        func_values = function_values[i]\n",
    "\n",
    "        for j in range(max_num_params):\n",
    "            perturbed_params = param_tensor.clone()\n",
    "            \n",
    "            perturbed_params[j] += epsilon\n",
    "            forward_value = func_values.sum()\n",
    "\n",
    "            perturbed_params[j] -= 2 * epsilon\n",
    "            backward_value = func_values.sum()\n",
    "            \n",
    "            gradients[i, j] = (forward_value - backward_value) / (2 * epsilon)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "batch_size = 3\n",
    "max_num_params = 5\n",
    "output_dim = 2\n",
    "\n",
    "parameters = torch.tensor([\n",
    "    [1.0, 2.0, 0.0, 0.0, 0.0],\n",
    "    [1.5, 2.5, 3.5, 0.0, 0.0],\n",
    "    [4.0, 5.0, 6.0, 7.0, 8.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "function_values = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 2.5],\n",
    "    [4.0, 5.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "numerical_gradients = compute_derivative(parameters, function_values)\n",
    "print(\"Numerical Gradients:\\n\", numerical_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessians:\n",
      " tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def compute_hessians(parameters, function_values, epsilon=1e-6): #need to fix\n",
    "    batch_size, max_num_params = parameters.shape\n",
    "    output_dim = function_values.shape[1]\n",
    "    \n",
    "    hessians = torch.zeros((batch_size, max_num_params, max_num_params))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        param_tensor = parameters[i].clone().detach().requires_grad_(True)\n",
    "        func_values = function_values[i]\n",
    "        \n",
    "        for j in range(max_num_params):\n",
    "            for k in range(max_num_params):\n",
    "                # Perturb j-th and k-th parameters\n",
    "                perturbed_params = param_tensor.clone()\n",
    "                \n",
    "                # Compute f(x + epsilon * e_j + epsilon * e_k)\n",
    "                perturbed_params_jk = perturbed_params.clone()\n",
    "                perturbed_params_jk[j] += epsilon\n",
    "                perturbed_params_jk[k] += epsilon\n",
    "                f_plus_plus = func_values.sum()\n",
    "                \n",
    "                # Compute f(x + epsilon * e_j - epsilon * e_k)\n",
    "                perturbed_params_jk[k] -= 2 * epsilon\n",
    "                f_plus_minus = func_values.sum()\n",
    "                \n",
    "                # Compute f(x - epsilon * e_j + epsilon * e_k)\n",
    "                perturbed_params_jk[j] -= 2 * epsilon\n",
    "                perturbed_params_jk[k] += 2 * epsilon\n",
    "                f_minus_plus = func_values.sum()\n",
    "                \n",
    "                # Compute f(x - epsilon * e_j - epsilon * e_k)\n",
    "                perturbed_params_jk[k] -= 2 * epsilon\n",
    "                f_minus_minus = func_values.sum()\n",
    "                hessians[i, j, k] = (f_plus_plus - f_plus_minus - f_minus_plus + f_minus_minus) / (4 * epsilon**2)\n",
    "    \n",
    "    return hessians\n",
    "\n",
    "batch_size = 3\n",
    "max_num_params = 5\n",
    "output_dim = 2\n",
    "\n",
    "parameters = torch.tensor([\n",
    "    [1.0, 2.0, 0.0, 0.0, 0.0],\n",
    "    [1.5, 2.5, 3.5, 0.0, 0.0],\n",
    "    [4.0, 5.0, 6.0, 7.0, 8.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "function_values = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 2.5],\n",
    "    [4.0, 5.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "hessians = compute_hessians(parameters, function_values)\n",
    "print(\"Hessians:\\n\", hessians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_function(params, symbols, formula):\n",
    "    print(len(symbols))\n",
    "    print(params.shape)\n",
    "    var_values = {symbols[j]: params[:, j].item() for j in range(len(symbols))}\n",
    "    print(var_values)\n",
    "    print(formula)\n",
    "    evaluated = formula.subs(var_values)\n",
    "    return float(evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_function(hold_params[10][0], hold_symbols[10], hold_formulas[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss_func(model, output, target):\n",
    "    target_max = torch.max(target, dim=-1, keepdim=True)[0]\n",
    "    target_min = torch.min(target, dim=-1, keepdim=True)[0]\n",
    "    target_range = torch.clamp(target_max - target_min, min=1e-6).squeeze(-1)\n",
    "    \n",
    "    mse_loss = torch.mean((output - target) ** 2, dim=-1)\n",
    "    normalized_loss = mse_loss / target_range\n",
    "    \n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    l1_reg = sum(p.abs().sum() for p in model.parameters())\n",
    "    \n",
    "    output_derivative = numerical_derivative(output)\n",
    "    target_derivative = numerical_derivative(target)\n",
    "    \n",
    "    derivative_diff = torch.mean((output_derivative - target_derivative) ** 2, dim=-1)\n",
    "    \n",
    "    total_loss = torch.mean(normalized_loss) + 0.01 * l2_reg + 0.01 * l1_reg + 0.1 * torch.mean(derivative_diff)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/10, loss = 5.85216095\n",
      "--- 14.031864643096924 seconds ---\n",
      "epoch : 1/10, loss = 2.27836243\n",
      "--- 14.46689748764038 seconds ---\n",
      "epoch : 2/10, loss = 2.27817616\n",
      "--- 13.696606159210205 seconds ---\n",
      "epoch : 3/10, loss = 2.23481708\n",
      "--- 14.677008152008057 seconds ---\n",
      "epoch : 4/10, loss = 2.20694181\n",
      "--- 13.260950803756714 seconds ---\n",
      "epoch : 5/10, loss = 2.20544566\n",
      "--- 12.729428052902222 seconds ---\n",
      "epoch : 6/10, loss = 2.22934910\n",
      "--- 12.689801216125488 seconds ---\n",
      "epoch : 7/10, loss = 2.28487759\n",
      "--- 12.859886884689331 seconds ---\n",
      "epoch : 8/10, loss = 2.26011226\n",
      "--- 13.009649515151978 seconds ---\n",
      "epoch : 9/10, loss = 2.28290470\n",
      "--- 13.230972290039062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions=hold_formulas, num_params=hold_num_params, symbols=hold_symbols, input_channels=1, device=device).to(device)\n",
    "#model = Encoder(functions=hold_formulas, num_params=hold_num_params, symbols=hold_symbols, input_channels=1, device=device).to(device)\n",
    "\n",
    "#loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    model.train()\n",
    "    \n",
    "    for train_batch in dataloader:\n",
    "        train_batch = train_batch.unsqueeze(1).unsqueeze(2).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        best_out,_,_,_,_ = model(train_batch)\n",
    "        #print(f\"best_out: {best_out}\")\n",
    "        #print(f\"train_batch: {train_batch[:, 0, 0]}\")\n",
    "        loss = training_loss_func(model, best_out, train_batch[:, 0, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r = np.random.randint(data.shape[0])\\nprint(data[r].shape)\\nplt.plot(data[r].detach().numpy());'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''r = np.random.randint(data.shape[0])\n",
    "print(data[r].shape)\n",
    "plt.plot(data[r].detach().numpy());'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
