{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.selu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device).requires_grad_(True)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def sympy_to_torch(self, expr, symbols):\n",
    "        torch_funcs = {\n",
    "            sp.Add: lambda *args: reduce(torch.add, args),\n",
    "            sp.Mul: lambda *args: reduce(torch.mul, args),\n",
    "            sp.Pow: torch.pow,\n",
    "            sp.sin: torch.sin,\n",
    "            sp.cos: torch.cos,\n",
    "        }\n",
    "\n",
    "        def torch_func(*args):\n",
    "            def _eval(ex):\n",
    "                if isinstance(ex, sp.Symbol):\n",
    "                    return args[symbols.index(ex)]\n",
    "                elif isinstance(ex, sp.Number):\n",
    "                    return torch.full_like(args[0], float(ex))\n",
    "                elif isinstance(ex, sp.Expr):\n",
    "                    op = type(ex)\n",
    "                    if op in torch_funcs:\n",
    "                        return torch_funcs[op](*[_eval(arg) for arg in ex.args])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported operation: {op}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type: {type(ex)}\")\n",
    "            \n",
    "            return _eval(expr)\n",
    "\n",
    "        return torch_func\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        torch_func = self.sympy_to_torch(formula, symbols)\n",
    "        var_values = [params[:, j] for j in range(len(symbols)-1)] + [x.unsqueeze(1)]\n",
    "        results = torch_func(*var_values)\n",
    "        return results.swapaxes(0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.requires_grad_(True)\n",
    "        outs = inputs.unsqueeze(1).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        weights = F.softmax(-stacked_losses, dim=0)\n",
    "        best_out = torch.sum(weights.unsqueeze(2) * stacked_preds, dim=0)\n",
    "        best_loss = torch.sum(weights * stacked_losses, dim=0)        \n",
    "        best_func = weights.t()\n",
    "        best_params = torch.sum(weights.unsqueeze(2) * torch.stack(all_params), dim=0)\n",
    "        return best_out, best_loss, best_func, weights, best_params, outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1642122/2683534046.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_1642122/2683534046.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_1642122/2683534046.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)\n",
    "target_funcs = hold1['target_funcs'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n",
      "target_funcs: torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")\n",
    "print(f\"target_funcs: {target_funcs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values[0:2000, :], params[0:2000, :], target_funcs[:, 0:2])\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.72062459\n",
      "Avg Grad Norm: 0.0121, Avg Grad Mean: 0.0002, Avg Grad Std: 0.0011\n",
      "--- 0.7648184299468994 seconds ---\n",
      "epoch : 1/100, loss = 0.71853787\n",
      "Avg Grad Norm: 0.0114, Avg Grad Mean: 0.0001, Avg Grad Std: 0.0010\n",
      "--- 0.652897834777832 seconds ---\n",
      "epoch : 2/100, loss = 0.71648463\n",
      "Avg Grad Norm: 0.0114, Avg Grad Mean: 0.0001, Avg Grad Std: 0.0010\n",
      "--- 0.6437990665435791 seconds ---\n",
      "epoch : 3/100, loss = 0.71427331\n",
      "Avg Grad Norm: 0.0125, Avg Grad Mean: 0.0001, Avg Grad Std: 0.0011\n",
      "--- 0.8119955062866211 seconds ---\n",
      "epoch : 4/100, loss = 0.71178803\n",
      "Avg Grad Norm: 0.0137, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0012\n",
      "--- 0.8739879131317139 seconds ---\n",
      "epoch : 5/100, loss = 0.70879449\n",
      "Avg Grad Norm: 0.0153, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0014\n",
      "--- 0.7547287940979004 seconds ---\n",
      "epoch : 6/100, loss = 0.70531061\n",
      "Avg Grad Norm: 0.0171, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0015\n",
      "--- 0.7404313087463379 seconds ---\n",
      "epoch : 7/100, loss = 0.70136534\n",
      "Avg Grad Norm: 0.0187, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9414691925048828 seconds ---\n",
      "epoch : 8/100, loss = 0.69700681\n",
      "Avg Grad Norm: 0.0197, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.8031046390533447 seconds ---\n",
      "epoch : 9/100, loss = 0.69248409\n",
      "Avg Grad Norm: 0.0200, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.7481229305267334 seconds ---\n",
      "epoch : 10/100, loss = 0.68991751\n",
      "Avg Grad Norm: 0.0200, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.7063610553741455 seconds ---\n",
      "epoch : 11/100, loss = 0.68949295\n",
      "Avg Grad Norm: 0.0200, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.5980477333068848 seconds ---\n",
      "epoch : 12/100, loss = 0.68908363\n",
      "Avg Grad Norm: 0.0198, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.6562800407409668 seconds ---\n",
      "epoch : 13/100, loss = 0.68868720\n",
      "Avg Grad Norm: 0.0196, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6300127506256104 seconds ---\n",
      "epoch : 14/100, loss = 0.68829855\n",
      "Avg Grad Norm: 0.0197, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.7429640293121338 seconds ---\n",
      "epoch : 15/100, loss = 0.68791709\n",
      "Avg Grad Norm: 0.0195, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 1.2087593078613281 seconds ---\n",
      "epoch : 16/100, loss = 0.68754123\n",
      "Avg Grad Norm: 0.0196, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.5994911193847656 seconds ---\n",
      "epoch : 17/100, loss = 0.68717372\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7948668003082275 seconds ---\n",
      "epoch : 18/100, loss = 0.68681180\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7030627727508545 seconds ---\n",
      "epoch : 19/100, loss = 0.68645089\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6331067085266113 seconds ---\n",
      "epoch : 20/100, loss = 0.68624846\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6885812282562256 seconds ---\n",
      "epoch : 21/100, loss = 0.68621317\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9265768527984619 seconds ---\n",
      "epoch : 22/100, loss = 0.68617830\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7614622116088867 seconds ---\n",
      "epoch : 23/100, loss = 0.68614334\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9127581119537354 seconds ---\n",
      "epoch : 24/100, loss = 0.68610879\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8693850040435791 seconds ---\n",
      "epoch : 25/100, loss = 0.68607399\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8356053829193115 seconds ---\n",
      "epoch : 26/100, loss = 0.68603977\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8296618461608887 seconds ---\n",
      "epoch : 27/100, loss = 0.68600555\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6403427124023438 seconds ---\n",
      "epoch : 28/100, loss = 0.68597113\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7489528656005859 seconds ---\n",
      "epoch : 29/100, loss = 0.68593719\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6686601638793945 seconds ---\n",
      "epoch : 30/100, loss = 0.68591745\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8400719165802002 seconds ---\n",
      "epoch : 31/100, loss = 0.68591413\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7843923568725586 seconds ---\n",
      "epoch : 32/100, loss = 0.68591082\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7931623458862305 seconds ---\n",
      "epoch : 33/100, loss = 0.68590748\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6802854537963867 seconds ---\n",
      "epoch : 34/100, loss = 0.68590418\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6077873706817627 seconds ---\n",
      "epoch : 35/100, loss = 0.68590087\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0017\n",
      "--- 0.7432727813720703 seconds ---\n",
      "epoch : 36/100, loss = 0.68589753\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8993253707885742 seconds ---\n",
      "epoch : 37/100, loss = 0.68589424\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7053205966949463 seconds ---\n",
      "epoch : 38/100, loss = 0.68589095\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7428884506225586 seconds ---\n",
      "epoch : 39/100, loss = 0.68588765\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.642878532409668 seconds ---\n",
      "epoch : 40/100, loss = 0.68588586\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6553983688354492 seconds ---\n",
      "epoch : 41/100, loss = 0.68588571\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8183045387268066 seconds ---\n",
      "epoch : 42/100, loss = 0.68588557\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6702220439910889 seconds ---\n",
      "epoch : 43/100, loss = 0.68588541\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6024961471557617 seconds ---\n",
      "epoch : 44/100, loss = 0.68588527\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8716607093811035 seconds ---\n",
      "epoch : 45/100, loss = 0.68588515\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.717688798904419 seconds ---\n",
      "epoch : 46/100, loss = 0.68588501\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7399535179138184 seconds ---\n",
      "epoch : 47/100, loss = 0.68588485\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6741108894348145 seconds ---\n",
      "epoch : 48/100, loss = 0.68588471\n",
      "Avg Grad Norm: 0.0190, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7986936569213867 seconds ---\n",
      "epoch : 49/100, loss = 0.68588456\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8869171142578125 seconds ---\n",
      "epoch : 50/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7103021144866943 seconds ---\n",
      "epoch : 51/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6981112957000732 seconds ---\n",
      "epoch : 52/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6185016632080078 seconds ---\n",
      "epoch : 53/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6570136547088623 seconds ---\n",
      "epoch : 54/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7085275650024414 seconds ---\n",
      "epoch : 55/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6796314716339111 seconds ---\n",
      "epoch : 56/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0190, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7336306571960449 seconds ---\n",
      "epoch : 57/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6210544109344482 seconds ---\n",
      "epoch : 58/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6054699420928955 seconds ---\n",
      "epoch : 59/100, loss = 0.68588450\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6121604442596436 seconds ---\n",
      "epoch : 60/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7511625289916992 seconds ---\n",
      "epoch : 61/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6839814186096191 seconds ---\n",
      "epoch : 62/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8656759262084961 seconds ---\n",
      "epoch : 63/100, loss = 0.68588446\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.758751392364502 seconds ---\n",
      "epoch : 64/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7962501049041748 seconds ---\n",
      "epoch : 65/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6288995742797852 seconds ---\n",
      "epoch : 66/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0194, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9238748550415039 seconds ---\n",
      "epoch : 67/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6233665943145752 seconds ---\n",
      "epoch : 68/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6544721126556396 seconds ---\n",
      "epoch : 69/100, loss = 0.68588446\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9151015281677246 seconds ---\n",
      "epoch : 70/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9739639759063721 seconds ---\n",
      "epoch : 71/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8411405086517334 seconds ---\n",
      "epoch : 72/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7041645050048828 seconds ---\n",
      "epoch : 73/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7556896209716797 seconds ---\n",
      "epoch : 74/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8899838924407959 seconds ---\n",
      "epoch : 75/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.727790117263794 seconds ---\n",
      "epoch : 76/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.874969482421875 seconds ---\n",
      "epoch : 77/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8824160099029541 seconds ---\n",
      "epoch : 78/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8451740741729736 seconds ---\n",
      "epoch : 79/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6344287395477295 seconds ---\n",
      "epoch : 80/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.705019474029541 seconds ---\n",
      "epoch : 81/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7132854461669922 seconds ---\n",
      "epoch : 82/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8032646179199219 seconds ---\n",
      "epoch : 83/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.983259916305542 seconds ---\n",
      "epoch : 84/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8374271392822266 seconds ---\n",
      "epoch : 85/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6332213878631592 seconds ---\n",
      "epoch : 86/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6492676734924316 seconds ---\n",
      "epoch : 87/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6417112350463867 seconds ---\n",
      "epoch : 88/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6370601654052734 seconds ---\n",
      "epoch : 89/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6838760375976562 seconds ---\n",
      "epoch : 90/100, loss = 0.68588447\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6647460460662842 seconds ---\n",
      "epoch : 91/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.6578640937805176 seconds ---\n",
      "epoch : 92/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7715635299682617 seconds ---\n",
      "epoch : 93/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7477376461029053 seconds ---\n",
      "epoch : 94/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.9769699573516846 seconds ---\n",
      "epoch : 95/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0191, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7370591163635254 seconds ---\n",
      "epoch : 96/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8068764209747314 seconds ---\n",
      "epoch : 97/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7411613464355469 seconds ---\n",
      "epoch : 98/100, loss = 0.68588449\n",
      "Avg Grad Norm: 0.0193, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.8368675708770752 seconds ---\n",
      "epoch : 99/100, loss = 0.68588448\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.7215206623077393 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    loss_class = nn.BCEWithLogitsLoss()\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = loss_class(output_function.swapaxes(0,1), target_function)#torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        if check_nan(inputs, \"inputs\") or check_nan(true_params, \"true_params\") or check_nan(true_func, \"true_func\"):\n",
    "            continue\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        if check_nan(best_out, \"best_out\") or check_nan(best_index, \"best_index\") or check_nan(best_params, \"best_params\"):\n",
    "            continue\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        if check_nan(loss, \"loss\"):\n",
    "            continue\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if(param.grad is None):\n",
    "                print(f\"None detected in {name}\")\n",
    "            if check_nan(param.grad, f\"gradient of {name}\"):\n",
    "                continue\n",
    "        \n",
    "        '''for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: grad norm = {param.grad.norm()}, grad std = {param.grad.std()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")'''\n",
    "\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    " \n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137582/182172253.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  best_indexes = torch.tensor(best_indexes, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.31649999\n",
      "--- 0.1766524314880371 seconds ---\n",
      "epoch : 1/100, loss = 0.31649999\n",
      "--- 0.1309223175048828 seconds ---\n",
      "epoch : 2/100, loss = 0.31649999\n",
      "--- 0.13064241409301758 seconds ---\n",
      "epoch : 3/100, loss = 0.31649999\n",
      "--- 0.130615234375 seconds ---\n",
      "epoch : 4/100, loss = 0.31649999\n",
      "--- 0.13306736946105957 seconds ---\n",
      "epoch : 5/100, loss = 0.31649999\n",
      "--- 0.13086271286010742 seconds ---\n",
      "epoch : 6/100, loss = 0.31649999\n",
      "--- 0.1311173439025879 seconds ---\n",
      "epoch : 7/100, loss = 0.31649999\n",
      "--- 0.13053512573242188 seconds ---\n",
      "epoch : 8/100, loss = 0.31649999\n",
      "--- 0.13073253631591797 seconds ---\n",
      "epoch : 9/100, loss = 0.31649999\n",
      "--- 0.1305248737335205 seconds ---\n",
      "epoch : 10/100, loss = 0.31649999\n",
      "--- 0.1305999755859375 seconds ---\n",
      "epoch : 11/100, loss = 0.31649999\n",
      "--- 0.13042736053466797 seconds ---\n",
      "epoch : 12/100, loss = 0.31649999\n",
      "--- 0.1331171989440918 seconds ---\n",
      "epoch : 13/100, loss = 0.31649999\n",
      "--- 0.13069987297058105 seconds ---\n",
      "epoch : 14/100, loss = 0.31649999\n",
      "--- 0.13083815574645996 seconds ---\n",
      "epoch : 15/100, loss = 0.31649999\n",
      "--- 0.13071084022521973 seconds ---\n",
      "epoch : 16/100, loss = 0.31649999\n",
      "--- 0.13096094131469727 seconds ---\n",
      "epoch : 17/100, loss = 0.31649999\n",
      "--- 0.13074302673339844 seconds ---\n",
      "epoch : 18/100, loss = 0.31649999\n",
      "--- 0.13081097602844238 seconds ---\n",
      "epoch : 19/100, loss = 0.31650000\n",
      "--- 0.13070344924926758 seconds ---\n",
      "epoch : 20/100, loss = 0.31649999\n",
      "--- 0.1305985450744629 seconds ---\n",
      "epoch : 21/100, loss = 0.31649999\n",
      "--- 0.1327042579650879 seconds ---\n",
      "epoch : 22/100, loss = 0.31649999\n",
      "--- 0.13103437423706055 seconds ---\n",
      "epoch : 23/100, loss = 0.31649999\n",
      "--- 0.13060665130615234 seconds ---\n",
      "epoch : 24/100, loss = 0.31649999\n",
      "--- 0.13081717491149902 seconds ---\n",
      "epoch : 25/100, loss = 0.31649999\n",
      "--- 0.13053345680236816 seconds ---\n",
      "epoch : 26/100, loss = 0.31649999\n",
      "--- 0.13057780265808105 seconds ---\n",
      "epoch : 27/100, loss = 0.31649999\n",
      "--- 0.1304335594177246 seconds ---\n",
      "epoch : 28/100, loss = 0.31649999\n",
      "--- 0.1307222843170166 seconds ---\n",
      "epoch : 29/100, loss = 0.31649999\n",
      "--- 0.1316075325012207 seconds ---\n",
      "epoch : 30/100, loss = 0.31649999\n",
      "--- 0.130659818649292 seconds ---\n",
      "epoch : 31/100, loss = 0.31649999\n",
      "--- 0.13101935386657715 seconds ---\n",
      "epoch : 32/100, loss = 0.31649999\n",
      "--- 0.1308891773223877 seconds ---\n",
      "epoch : 33/100, loss = 0.31649999\n",
      "--- 0.13057327270507812 seconds ---\n",
      "epoch : 34/100, loss = 0.31649999\n",
      "--- 0.13056111335754395 seconds ---\n",
      "epoch : 35/100, loss = 0.31649999\n",
      "--- 0.1309494972229004 seconds ---\n",
      "epoch : 36/100, loss = 0.31649999\n",
      "--- 0.1306157112121582 seconds ---\n",
      "epoch : 37/100, loss = 0.31649999\n",
      "--- 0.13081860542297363 seconds ---\n",
      "epoch : 38/100, loss = 0.31649999\n",
      "--- 0.13215184211730957 seconds ---\n",
      "epoch : 39/100, loss = 0.31649999\n",
      "--- 0.13072443008422852 seconds ---\n",
      "epoch : 40/100, loss = 0.31649999\n",
      "--- 0.1312863826751709 seconds ---\n",
      "epoch : 41/100, loss = 0.31649999\n",
      "--- 0.1303246021270752 seconds ---\n",
      "epoch : 42/100, loss = 0.31649999\n",
      "--- 0.13068008422851562 seconds ---\n",
      "epoch : 43/100, loss = 0.31649999\n",
      "--- 0.13045144081115723 seconds ---\n",
      "epoch : 44/100, loss = 0.31649999\n",
      "--- 0.1310417652130127 seconds ---\n",
      "epoch : 45/100, loss = 0.31649999\n",
      "--- 0.1306288242340088 seconds ---\n",
      "epoch : 46/100, loss = 0.31649999\n",
      "--- 0.13291430473327637 seconds ---\n",
      "epoch : 47/100, loss = 0.31650000\n",
      "--- 0.13050341606140137 seconds ---\n",
      "epoch : 48/100, loss = 0.31649999\n",
      "--- 0.1305253505706787 seconds ---\n",
      "epoch : 49/100, loss = 0.31650000\n",
      "--- 0.13025403022766113 seconds ---\n",
      "epoch : 50/100, loss = 0.31649999\n",
      "--- 0.1305522918701172 seconds ---\n",
      "epoch : 51/100, loss = 0.31649999\n",
      "--- 0.13028430938720703 seconds ---\n",
      "epoch : 52/100, loss = 0.31649999\n",
      "--- 0.1310412883758545 seconds ---\n",
      "epoch : 53/100, loss = 0.31649999\n",
      "--- 0.13139724731445312 seconds ---\n",
      "epoch : 54/100, loss = 0.31649999\n",
      "--- 0.1305103302001953 seconds ---\n",
      "epoch : 55/100, loss = 0.31649999\n",
      "--- 0.13022804260253906 seconds ---\n",
      "epoch : 56/100, loss = 0.31649999\n",
      "--- 0.13213729858398438 seconds ---\n",
      "epoch : 57/100, loss = 0.31649999\n",
      "--- 0.1303858757019043 seconds ---\n",
      "epoch : 58/100, loss = 0.31649999\n",
      "--- 0.13059186935424805 seconds ---\n",
      "epoch : 59/100, loss = 0.31650000\n",
      "--- 0.1305701732635498 seconds ---\n",
      "epoch : 60/100, loss = 0.31649999\n",
      "--- 0.13086700439453125 seconds ---\n",
      "epoch : 61/100, loss = 0.31649999\n",
      "--- 0.13031864166259766 seconds ---\n",
      "epoch : 62/100, loss = 0.31649999\n",
      "--- 0.13055706024169922 seconds ---\n",
      "epoch : 63/100, loss = 0.31649999\n",
      "--- 0.13066554069519043 seconds ---\n",
      "epoch : 64/100, loss = 0.31649999\n",
      "--- 0.13054728507995605 seconds ---\n",
      "epoch : 65/100, loss = 0.31649999\n",
      "--- 0.13042259216308594 seconds ---\n",
      "epoch : 66/100, loss = 0.31649999\n",
      "--- 0.13277578353881836 seconds ---\n",
      "epoch : 67/100, loss = 0.31649999\n",
      "--- 0.13048028945922852 seconds ---\n",
      "epoch : 68/100, loss = 0.31649999\n",
      "--- 0.13069677352905273 seconds ---\n",
      "epoch : 69/100, loss = 0.31649999\n",
      "--- 0.13023710250854492 seconds ---\n",
      "epoch : 70/100, loss = 0.31649999\n",
      "--- 0.13062143325805664 seconds ---\n",
      "epoch : 71/100, loss = 0.31649999\n",
      "--- 0.13083457946777344 seconds ---\n",
      "epoch : 72/100, loss = 0.31649999\n",
      "--- 0.13061952590942383 seconds ---\n",
      "epoch : 73/100, loss = 0.31649999\n",
      "--- 0.13042807579040527 seconds ---\n",
      "epoch : 74/100, loss = 0.31649999\n",
      "--- 0.13066816329956055 seconds ---\n",
      "epoch : 75/100, loss = 0.31649999\n",
      "--- 0.1304323673248291 seconds ---\n",
      "epoch : 76/100, loss = 0.31649999\n",
      "--- 0.1314527988433838 seconds ---\n",
      "epoch : 77/100, loss = 0.31649999\n",
      "--- 0.1304159164428711 seconds ---\n",
      "epoch : 78/100, loss = 0.31649999\n",
      "--- 0.13069844245910645 seconds ---\n",
      "epoch : 79/100, loss = 0.31649999\n",
      "--- 0.13054132461547852 seconds ---\n",
      "epoch : 80/100, loss = 0.31649999\n",
      "--- 0.13072991371154785 seconds ---\n",
      "epoch : 81/100, loss = 0.31650000\n",
      "--- 0.13035297393798828 seconds ---\n",
      "epoch : 82/100, loss = 0.31649999\n",
      "--- 0.13052630424499512 seconds ---\n",
      "epoch : 83/100, loss = 0.31649999\n",
      "--- 0.13027524948120117 seconds ---\n",
      "epoch : 84/100, loss = 0.31649999\n",
      "--- 0.13066339492797852 seconds ---\n",
      "epoch : 85/100, loss = 0.31649999\n",
      "--- 0.13039779663085938 seconds ---\n",
      "epoch : 86/100, loss = 0.31649999\n",
      "--- 0.13201189041137695 seconds ---\n",
      "epoch : 87/100, loss = 0.31649999\n",
      "--- 0.13043832778930664 seconds ---\n",
      "epoch : 88/100, loss = 0.31649999\n",
      "--- 0.13071775436401367 seconds ---\n",
      "epoch : 89/100, loss = 0.31649999\n",
      "--- 0.1305100917816162 seconds ---\n",
      "epoch : 90/100, loss = 0.31649999\n",
      "--- 0.13068771362304688 seconds ---\n",
      "epoch : 91/100, loss = 0.31649999\n",
      "--- 0.13059544563293457 seconds ---\n",
      "epoch : 92/100, loss = 0.31649999\n",
      "--- 0.13066577911376953 seconds ---\n",
      "epoch : 93/100, loss = 0.31649999\n",
      "--- 0.1304018497467041 seconds ---\n",
      "epoch : 94/100, loss = 0.31649999\n",
      "--- 0.13051533699035645 seconds ---\n",
      "epoch : 95/100, loss = 0.31649999\n",
      "--- 0.1302812099456787 seconds ---\n",
      "epoch : 96/100, loss = 0.31649999\n",
      "--- 0.13255715370178223 seconds ---\n",
      "epoch : 97/100, loss = 0.31649999\n",
      "--- 0.13053345680236816 seconds ---\n",
      "epoch : 98/100, loss = 0.31649999\n",
      "--- 0.13080811500549316 seconds ---\n",
      "epoch : 99/100, loss = 0.31649999\n",
      "--- 0.1306912899017334 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    optimizer.zero_grad()\n",
    "    for inputs, true_params, true_func in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: tensor([0.5135, 0.4865], device='cuda:3', grad_fn=<SelectBackward0>)\n",
      "best_loss: 0.40928488969802856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fba6a330490>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "ran = np.random.randint(0, 10)\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values[0:2000])\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"best_loss: {best_loss[ran]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
