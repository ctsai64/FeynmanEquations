{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.selu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device).requires_grad_(True)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([8, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([6, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.LayerNorm(20),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),           \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def sympy_to_torch(self, expr, symbols):\n",
    "        torch_funcs = {\n",
    "            sp.Add: lambda *args: reduce(torch.add, args),\n",
    "            sp.Mul: lambda *args: reduce(torch.mul, args),\n",
    "            sp.Pow: torch.pow,\n",
    "            sp.sin: torch.sin,\n",
    "            sp.cos: torch.cos,\n",
    "        }\n",
    "\n",
    "        def torch_func(*args):\n",
    "            def _eval(ex):\n",
    "                if isinstance(ex, sp.Symbol):\n",
    "                    return args[symbols.index(ex)]\n",
    "                elif isinstance(ex, sp.Number):\n",
    "                    return torch.full_like(args[0], float(ex))\n",
    "                elif isinstance(ex, sp.Expr):\n",
    "                    op = type(ex)\n",
    "                    if op in torch_funcs:\n",
    "                        return torch_funcs[op](*[_eval(arg) for arg in ex.args])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported operation: {op}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type: {type(ex)}\")\n",
    "            \n",
    "            return _eval(expr)\n",
    "\n",
    "        return torch_func\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        torch_func = self.sympy_to_torch(formula, symbols)\n",
    "        var_values = [params[:, j] for j in range(len(symbols)-1)] + [x.unsqueeze(1)]\n",
    "        results = torch_func(*var_values)\n",
    "        return results.swapaxes(0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.requires_grad_(True)\n",
    "        outs = inputs.unsqueeze(1).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        weights = F.softmax(-stacked_losses, dim=0)\n",
    "        best_out = torch.sum(weights.unsqueeze(2) * stacked_preds, dim=0)\n",
    "        best_loss = torch.sum(weights * stacked_losses, dim=0)        \n",
    "        best_func = weights.t()\n",
    "        best_params = torch.sum(weights.unsqueeze(2) * torch.stack(all_params), dim=0)\n",
    "        return best_out, best_loss, best_func, weights, best_params, outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3702781/2683534046.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_3702781/2683534046.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_3702781/2683534046.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)\n",
    "target_funcs = hold1['target_funcs'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n",
      "target_funcs: torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")\n",
    "print(f\"target_funcs: {target_funcs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values[0:8000], params[0:8000], target_funcs[:, 0:8])\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/500, loss = 0.27618045\n",
      "Avg Grad Norm: 0.0413, Avg Grad Mean: -0.0001, Avg Grad Std: 0.0037\n",
      "--- 2.0348899364471436 seconds ---\n",
      "epoch : 1/500, loss = 0.11238551\n",
      "Avg Grad Norm: 0.0233, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0016\n",
      "--- 1.746446132659912 seconds ---\n",
      "epoch : 2/500, loss = 0.07827501\n",
      "Avg Grad Norm: 0.0053, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 1.6701180934906006 seconds ---\n",
      "epoch : 3/500, loss = 0.07637055\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.6087217330932617 seconds ---\n",
      "epoch : 4/500, loss = 0.07561423\n",
      "Avg Grad Norm: 0.0035, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3568181991577148 seconds ---\n",
      "epoch : 5/500, loss = 0.07420957\n",
      "Avg Grad Norm: 0.0072, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0008\n",
      "--- 1.4641211032867432 seconds ---\n",
      "epoch : 6/500, loss = 0.07306586\n",
      "Avg Grad Norm: 0.0045, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 1.433511734008789 seconds ---\n",
      "epoch : 7/500, loss = 0.07272078\n",
      "Avg Grad Norm: 0.0032, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4767577648162842 seconds ---\n",
      "epoch : 8/500, loss = 0.07266317\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.5822124481201172 seconds ---\n",
      "epoch : 9/500, loss = 0.07256183\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7090363502502441 seconds ---\n",
      "epoch : 10/500, loss = 0.07254104\n",
      "Avg Grad Norm: 0.0036, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.5048494338989258 seconds ---\n",
      "epoch : 11/500, loss = 0.07251311\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.5432322025299072 seconds ---\n",
      "epoch : 12/500, loss = 0.07251009\n",
      "Avg Grad Norm: 0.0038, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.5098981857299805 seconds ---\n",
      "epoch : 13/500, loss = 0.07244480\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4937732219696045 seconds ---\n",
      "epoch : 14/500, loss = 0.07246054\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.76828932762146 seconds ---\n",
      "epoch : 15/500, loss = 0.07243527\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5291271209716797 seconds ---\n",
      "epoch : 16/500, loss = 0.07242115\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6398160457611084 seconds ---\n",
      "epoch : 17/500, loss = 0.07238392\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5270519256591797 seconds ---\n",
      "epoch : 18/500, loss = 0.07238370\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5354862213134766 seconds ---\n",
      "epoch : 19/500, loss = 0.07237933\n",
      "Avg Grad Norm: 0.0025, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7436566352844238 seconds ---\n",
      "epoch : 20/500, loss = 0.07241079\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.5772452354431152 seconds ---\n",
      "epoch : 21/500, loss = 0.07238650\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8891594409942627 seconds ---\n",
      "epoch : 22/500, loss = 0.07237973\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3014602661132812 seconds ---\n",
      "epoch : 23/500, loss = 0.07237420\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4418084621429443 seconds ---\n",
      "epoch : 24/500, loss = 0.07255330\n",
      "Avg Grad Norm: 0.0060, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 1.616159439086914 seconds ---\n",
      "epoch : 25/500, loss = 0.07237510\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8543362617492676 seconds ---\n",
      "epoch : 26/500, loss = 0.07238520\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7344560623168945 seconds ---\n",
      "epoch : 27/500, loss = 0.07239437\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5067470073699951 seconds ---\n",
      "epoch : 28/500, loss = 0.07236944\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5118048191070557 seconds ---\n",
      "epoch : 29/500, loss = 0.07233295\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.43760347366333 seconds ---\n",
      "epoch : 30/500, loss = 0.07232219\n",
      "Avg Grad Norm: 0.0017, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4357709884643555 seconds ---\n",
      "epoch : 31/500, loss = 0.07233568\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7817656993865967 seconds ---\n",
      "epoch : 32/500, loss = 0.07232163\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4824802875518799 seconds ---\n",
      "epoch : 33/500, loss = 0.07251731\n",
      "Avg Grad Norm: 0.0048, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 1.3304779529571533 seconds ---\n",
      "epoch : 34/500, loss = 0.07239806\n",
      "Avg Grad Norm: 0.0031, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3047430515289307 seconds ---\n",
      "epoch : 35/500, loss = 0.07240339\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.4875736236572266 seconds ---\n",
      "epoch : 36/500, loss = 0.07233850\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.3356995582580566 seconds ---\n",
      "epoch : 37/500, loss = 0.07234005\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.3481392860412598 seconds ---\n",
      "epoch : 38/500, loss = 0.07232338\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.3034114837646484 seconds ---\n",
      "epoch : 39/500, loss = 0.07231554\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4299237728118896 seconds ---\n",
      "epoch : 40/500, loss = 0.07234154\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5045692920684814 seconds ---\n",
      "epoch : 41/500, loss = 0.07236855\n",
      "Avg Grad Norm: 0.0033, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.4582736492156982 seconds ---\n",
      "epoch : 42/500, loss = 0.07243383\n",
      "Avg Grad Norm: 0.0047, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 1.2973201274871826 seconds ---\n",
      "epoch : 43/500, loss = 0.07234015\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4342057704925537 seconds ---\n",
      "epoch : 44/500, loss = 0.07230013\n",
      "Avg Grad Norm: 0.0016, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.560750961303711 seconds ---\n",
      "epoch : 45/500, loss = 0.07230393\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4017443656921387 seconds ---\n",
      "epoch : 46/500, loss = 0.07231148\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.498821496963501 seconds ---\n",
      "epoch : 47/500, loss = 0.07230041\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7288854122161865 seconds ---\n",
      "epoch : 48/500, loss = 0.07231694\n",
      "Avg Grad Norm: 0.0025, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6418859958648682 seconds ---\n",
      "epoch : 49/500, loss = 0.07230521\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7951815128326416 seconds ---\n",
      "epoch : 50/500, loss = 0.07231495\n",
      "Avg Grad Norm: 0.0023, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.408299207687378 seconds ---\n",
      "epoch : 51/500, loss = 0.07250901\n",
      "Avg Grad Norm: 0.0048, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 1.5582308769226074 seconds ---\n",
      "epoch : 52/500, loss = 0.07231783\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7322194576263428 seconds ---\n",
      "epoch : 53/500, loss = 0.07229138\n",
      "Avg Grad Norm: 0.0017, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7584564685821533 seconds ---\n",
      "epoch : 54/500, loss = 0.07228500\n",
      "Avg Grad Norm: 0.0017, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.6705811023712158 seconds ---\n",
      "epoch : 55/500, loss = 0.07228835\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.6363909244537354 seconds ---\n",
      "epoch : 56/500, loss = 0.07230302\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3952820301055908 seconds ---\n",
      "epoch : 57/500, loss = 0.07231434\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4928500652313232 seconds ---\n",
      "epoch : 58/500, loss = 0.07230718\n",
      "Avg Grad Norm: 0.0023, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4961142539978027 seconds ---\n",
      "epoch : 59/500, loss = 0.07229483\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.479719877243042 seconds ---\n",
      "epoch : 60/500, loss = 0.07229313\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.589770793914795 seconds ---\n",
      "epoch : 61/500, loss = 0.07227301\n",
      "Avg Grad Norm: 0.0016, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5937254428863525 seconds ---\n",
      "epoch : 62/500, loss = 0.07226982\n",
      "Avg Grad Norm: 0.0015, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5097830295562744 seconds ---\n",
      "epoch : 63/500, loss = 0.07230263\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.8198139667510986 seconds ---\n",
      "epoch : 64/500, loss = 0.07232019\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6982579231262207 seconds ---\n",
      "epoch : 65/500, loss = 0.07226397\n",
      "Avg Grad Norm: 0.0014, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.6081643104553223 seconds ---\n",
      "epoch : 66/500, loss = 0.07229695\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4031167030334473 seconds ---\n",
      "epoch : 67/500, loss = 0.07229080\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.6124606132507324 seconds ---\n",
      "epoch : 68/500, loss = 0.07231071\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.450932264328003 seconds ---\n",
      "epoch : 69/500, loss = 0.07240448\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.422490119934082 seconds ---\n",
      "epoch : 70/500, loss = 0.07231425\n",
      "Avg Grad Norm: 0.0028, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.366560935974121 seconds ---\n",
      "epoch : 71/500, loss = 0.07225789\n",
      "Avg Grad Norm: 0.0011, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.363328456878662 seconds ---\n",
      "epoch : 72/500, loss = 0.07225611\n",
      "Avg Grad Norm: 0.0013, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.345646858215332 seconds ---\n",
      "epoch : 73/500, loss = 0.07226946\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.3655610084533691 seconds ---\n",
      "epoch : 74/500, loss = 0.07228643\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4026923179626465 seconds ---\n",
      "epoch : 75/500, loss = 0.07229264\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5518441200256348 seconds ---\n",
      "epoch : 76/500, loss = 0.07228508\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.303544521331787 seconds ---\n",
      "epoch : 77/500, loss = 0.07226712\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4543545246124268 seconds ---\n",
      "epoch : 78/500, loss = 0.07234429\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3409197330474854 seconds ---\n",
      "epoch : 79/500, loss = 0.07227452\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5208141803741455 seconds ---\n",
      "epoch : 80/500, loss = 0.07248710\n",
      "Avg Grad Norm: 0.0049, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 1.4878041744232178 seconds ---\n",
      "epoch : 81/500, loss = 0.07234818\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5896918773651123 seconds ---\n",
      "epoch : 82/500, loss = 0.07225253\n",
      "Avg Grad Norm: 0.0012, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.3927254676818848 seconds ---\n",
      "epoch : 83/500, loss = 0.07224690\n",
      "Avg Grad Norm: 0.0012, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.4100456237792969 seconds ---\n",
      "epoch : 84/500, loss = 0.07224684\n",
      "Avg Grad Norm: 0.0012, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.553931713104248 seconds ---\n",
      "epoch : 85/500, loss = 0.07224401\n",
      "Avg Grad Norm: 0.0011, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.5548343658447266 seconds ---\n",
      "epoch : 86/500, loss = 0.07225488\n",
      "Avg Grad Norm: 0.0016, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.4772920608520508 seconds ---\n",
      "epoch : 87/500, loss = 0.07227747\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.6297707557678223 seconds ---\n",
      "epoch : 88/500, loss = 0.07226233\n",
      "Avg Grad Norm: 0.0018, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7136456966400146 seconds ---\n",
      "epoch : 89/500, loss = 0.07226020\n",
      "Avg Grad Norm: 0.0016, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5941407680511475 seconds ---\n",
      "epoch : 90/500, loss = 0.07223873\n",
      "Avg Grad Norm: 0.0012, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.271026611328125 seconds ---\n",
      "epoch : 91/500, loss = 0.07228257\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7030560970306396 seconds ---\n",
      "epoch : 92/500, loss = 0.07227280\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5357379913330078 seconds ---\n",
      "epoch : 93/500, loss = 0.07225208\n",
      "Avg Grad Norm: 0.0015, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7405436038970947 seconds ---\n",
      "epoch : 94/500, loss = 0.07225688\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.5876662731170654 seconds ---\n",
      "epoch : 95/500, loss = 0.07229257\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8100924491882324 seconds ---\n",
      "epoch : 96/500, loss = 0.07223060\n",
      "Avg Grad Norm: 0.0009, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.605949878692627 seconds ---\n",
      "epoch : 97/500, loss = 0.07225106\n",
      "Avg Grad Norm: 0.0018, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.378502368927002 seconds ---\n",
      "epoch : 98/500, loss = 0.07224562\n",
      "Avg Grad Norm: 0.0016, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.7583913803100586 seconds ---\n",
      "epoch : 99/500, loss = 0.07222956\n",
      "Avg Grad Norm: 0.0011, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0001\n",
      "--- 1.904855489730835 seconds ---\n",
      "epoch : 100/500, loss = 0.07228041\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 1.705000638961792 seconds ---\n",
      "epoch : 101/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.6889731884002686 seconds ---\n",
      "epoch : 102/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.6136748790740967 seconds ---\n",
      "epoch : 103/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.39237642288208 seconds ---\n",
      "epoch : 104/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.464109182357788 seconds ---\n",
      "epoch : 105/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5633227825164795 seconds ---\n",
      "epoch : 106/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.682399034500122 seconds ---\n",
      "epoch : 107/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.6683621406555176 seconds ---\n",
      "epoch : 108/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.601304531097412 seconds ---\n",
      "epoch : 109/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.661417007446289 seconds ---\n",
      "epoch : 110/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.6480963230133057 seconds ---\n",
      "epoch : 111/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4755473136901855 seconds ---\n",
      "epoch : 112/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4620044231414795 seconds ---\n",
      "epoch : 113/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4247581958770752 seconds ---\n",
      "epoch : 114/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.3816077709197998 seconds ---\n",
      "epoch : 115/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4646296501159668 seconds ---\n",
      "epoch : 116/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5020670890808105 seconds ---\n",
      "epoch : 117/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4868037700653076 seconds ---\n",
      "epoch : 118/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5873854160308838 seconds ---\n",
      "epoch : 119/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4884464740753174 seconds ---\n",
      "epoch : 120/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.3114466667175293 seconds ---\n",
      "epoch : 121/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4444670677185059 seconds ---\n",
      "epoch : 122/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.9441275596618652 seconds ---\n",
      "epoch : 123/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5519945621490479 seconds ---\n",
      "epoch : 124/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.3826770782470703 seconds ---\n",
      "epoch : 125/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5228979587554932 seconds ---\n",
      "epoch : 126/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.6217119693756104 seconds ---\n",
      "epoch : 127/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5857596397399902 seconds ---\n",
      "epoch : 128/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4539668560028076 seconds ---\n",
      "epoch : 129/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.3330748081207275 seconds ---\n",
      "epoch : 130/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5852177143096924 seconds ---\n",
      "epoch : 131/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.567262887954712 seconds ---\n",
      "epoch : 132/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.335082769393921 seconds ---\n",
      "epoch : 133/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5044009685516357 seconds ---\n",
      "epoch : 134/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.3313486576080322 seconds ---\n",
      "epoch : 135/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.493565559387207 seconds ---\n",
      "epoch : 136/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4482226371765137 seconds ---\n",
      "epoch : 137/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4231085777282715 seconds ---\n",
      "epoch : 138/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.4767413139343262 seconds ---\n",
      "epoch : 139/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.5044798851013184 seconds ---\n",
      "epoch : 140/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.6328742504119873 seconds ---\n",
      "epoch : 141/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.7213530540466309 seconds ---\n",
      "epoch : 142/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 1.7886877059936523 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m lam_val \u001b[38;5;241m=\u001b[39m lambda_scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(best_out, inputs, best_params, true_params, best_index\u001b[38;5;241m.\u001b[39mfloat(), true_func\u001b[38;5;241m.\u001b[39mfloat(), lam_val)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     61\u001b[0m grad_norm, grad_mean, grad_std \u001b[38;5;241m=\u001b[39m compute_grad_stats(model)\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions[0:8], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "total_epochs = 500\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    loss_class = nn.BCEWithLogitsLoss()\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = loss_class(output_function.swapaxes(0,1), target_function)\n",
    "    return params_loss*lam + y_loss*(1-lam)\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "\n",
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "grad_norms = []\n",
    "grad_means = []\n",
    "grad_stds = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "        '''for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: grad norm = {param.grad.norm()}, grad std = {param.grad.std()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")'''\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    grad_norms.append(avg_grad_norm)\n",
    "    grad_means.append(avg_grad_mean)\n",
    "    grad_stds.append(avg_grad_std)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, total_epochs + 1), train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot gradient norm\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, total_epochs + 1), grad_norms)\n",
    "plt.title('Average Gradient Norm')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Gradient Norm')\n",
    "\n",
    "# Plot gradient mean and std\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, total_epochs + 1), grad_means, label='Mean')\n",
    "plt.plot(range(1, total_epochs + 1), grad_stds, label='Std')\n",
    "plt.title('Gradient Mean and Std')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: tensor([0.5171, 0.0705, 0.2923, 0.0000, 0.1201], device='cuda:3',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "true_func: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:3')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3bea7cd590>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "ran = np.random.randint(50000)\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values)\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"true_func: {target_funcs[ran]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
