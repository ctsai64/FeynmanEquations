{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.selu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device).requires_grad_(True)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([8, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([6, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.LayerNorm(20),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),           \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def sympy_to_torch(self, expr, symbols):\n",
    "        torch_funcs = {\n",
    "            sp.Add: lambda *args: reduce(torch.add, args),\n",
    "            sp.Mul: lambda *args: reduce(torch.mul, args),\n",
    "            sp.Pow: torch.pow,\n",
    "            sp.sin: torch.sin,\n",
    "            sp.cos: torch.cos,\n",
    "        }\n",
    "\n",
    "        def torch_func(*args):\n",
    "            def _eval(ex):\n",
    "                if isinstance(ex, sp.Symbol):\n",
    "                    return args[symbols.index(ex)]\n",
    "                elif isinstance(ex, sp.Number):\n",
    "                    return torch.full_like(args[0], float(ex))\n",
    "                elif isinstance(ex, sp.Expr):\n",
    "                    op = type(ex)\n",
    "                    if op in torch_funcs:\n",
    "                        return torch_funcs[op](*[_eval(arg) for arg in ex.args])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported operation: {op}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type: {type(ex)}\")\n",
    "            \n",
    "            return _eval(expr)\n",
    "\n",
    "        return torch_func\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        torch_func = self.sympy_to_torch(formula, symbols)\n",
    "        var_values = [params[:, j] for j in range(len(symbols)-1)] + [x.unsqueeze(1)]\n",
    "        results = torch_func(*var_values)\n",
    "        return results.swapaxes(0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.requires_grad_(True)\n",
    "        outs = inputs.unsqueeze(1).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        weights = F.softmax(-stacked_losses, dim=0)\n",
    "        best_out = torch.sum(weights.unsqueeze(2) * stacked_preds, dim=0)\n",
    "        best_loss = torch.sum(weights * stacked_losses, dim=0)        \n",
    "        best_func = weights.t()\n",
    "        best_params = torch.sum(weights.unsqueeze(2) * torch.stack(all_params), dim=0)\n",
    "        return best_out, best_loss, best_func, weights, best_params, outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2147868/2683534046.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_2147868/2683534046.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_2147868/2683534046.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)\n",
    "target_funcs = hold1['target_funcs'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n",
      "target_funcs: torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")\n",
    "print(f\"target_funcs: {target_funcs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values, params, target_funcs)\n",
    "dataloader = DataLoader(dataset, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/500, loss = 16119.26813843\n",
      "Avg Grad Norm: 258.7582, Avg Grad Mean: 0.9612, Avg Grad Std: 20.5278\n",
      "--- 28.29412078857422 seconds ---\n",
      "epoch : 1/500, loss = 16435.11603638\n",
      "Avg Grad Norm: 85.8064, Avg Grad Mean: 0.5349, Avg Grad Std: 10.8408\n",
      "--- 27.956777572631836 seconds ---\n",
      "epoch : 2/500, loss = 121378.29899902\n",
      "Avg Grad Norm: 346663.2791, Avg Grad Mean: 23958.1242, Avg Grad Std: 67240.3267\n",
      "--- 27.975975513458252 seconds ---\n",
      "epoch : 3/500, loss = 306633.77936523\n",
      "Avg Grad Norm: 38318.9414, Avg Grad Mean: 247.8972, Avg Grad Std: 4947.4873\n",
      "--- 27.968069553375244 seconds ---\n",
      "epoch : 4/500, loss = 28752.25557129\n",
      "Avg Grad Norm: 2151.0141, Avg Grad Mean: 38.9819, Avg Grad Std: 333.0461\n",
      "--- 27.956170558929443 seconds ---\n",
      "epoch : 5/500, loss = 131539.95526367\n",
      "Avg Grad Norm: 30729.8428, Avg Grad Mean: -475.5305, Avg Grad Std: 5008.9858\n",
      "--- 27.953333377838135 seconds ---\n",
      "epoch : 6/500, loss = 48337.70453125\n",
      "Avg Grad Norm: 855.9665, Avg Grad Mean: 36.7598, Avg Grad Std: 118.9046\n",
      "--- 28.29119324684143 seconds ---\n",
      "epoch : 7/500, loss = 26592.89819336\n",
      "Avg Grad Norm: 940.9900, Avg Grad Mean: 13.4079, Avg Grad Std: 139.4943\n",
      "--- 28.452150583267212 seconds ---\n",
      "epoch : 8/500, loss = 20842.41584717\n",
      "Avg Grad Norm: 329.8460, Avg Grad Mean: 5.9603, Avg Grad Std: 40.2372\n",
      "--- 28.75320839881897 seconds ---\n",
      "epoch : 9/500, loss = 32405.84511475\n",
      "Avg Grad Norm: 2698.9895, Avg Grad Mean: 18.5496, Avg Grad Std: 240.3146\n",
      "--- 28.39991307258606 seconds ---\n",
      "epoch : 10/500, loss = 23379.61912109\n",
      "Avg Grad Norm: 234.0786, Avg Grad Mean: 9.0719, Avg Grad Std: 35.6211\n",
      "--- 28.87269639968872 seconds ---\n",
      "epoch : 11/500, loss = 19842.50285645\n",
      "Avg Grad Norm: 140.2156, Avg Grad Mean: 4.9378, Avg Grad Std: 21.3871\n",
      "--- 28.408206939697266 seconds ---\n",
      "epoch : 12/500, loss = 17947.43777832\n",
      "Avg Grad Norm: 241.7408, Avg Grad Mean: 2.9125, Avg Grad Std: 37.2719\n",
      "--- 28.379467964172363 seconds ---\n",
      "epoch : 13/500, loss = 19155.47033447\n",
      "Avg Grad Norm: 467.9856, Avg Grad Mean: 7.0464, Avg Grad Std: 74.3472\n",
      "--- 28.41079568862915 seconds ---\n",
      "epoch : 14/500, loss = 17453.32270752\n",
      "Avg Grad Norm: 103.8196, Avg Grad Mean: 1.6549, Avg Grad Std: 14.4937\n",
      "--- 28.394598722457886 seconds ---\n",
      "epoch : 15/500, loss = 16366.91904907\n",
      "Avg Grad Norm: 48.0148, Avg Grad Mean: 0.4413, Avg Grad Std: 6.5637\n",
      "--- 28.385972023010254 seconds ---\n",
      "epoch : 16/500, loss = 18046.03502930\n",
      "Avg Grad Norm: 584.0689, Avg Grad Mean: 2.8146, Avg Grad Std: 84.1145\n",
      "--- 28.727794647216797 seconds ---\n",
      "epoch : 17/500, loss = 16804.98955566\n",
      "Avg Grad Norm: 71.8054, Avg Grad Mean: 1.2573, Avg Grad Std: 10.5113\n",
      "--- 28.407458305358887 seconds ---\n",
      "epoch : 18/500, loss = 16388.40895752\n",
      "Avg Grad Norm: 22.9984, Avg Grad Mean: 0.5999, Avg Grad Std: 2.7707\n",
      "--- 28.712984323501587 seconds ---\n",
      "epoch : 19/500, loss = 17478.27984985\n",
      "Avg Grad Norm: 196.7352, Avg Grad Mean: 1.9844, Avg Grad Std: 29.5323\n",
      "--- 28.420637845993042 seconds ---\n",
      "epoch : 20/500, loss = 16804.00150146\n",
      "Avg Grad Norm: 49.1379, Avg Grad Mean: 1.1435, Avg Grad Std: 7.4105\n",
      "--- 28.70440363883972 seconds ---\n",
      "epoch : 21/500, loss = 16271.72350342\n",
      "Avg Grad Norm: 14.0807, Avg Grad Mean: 0.2743, Avg Grad Std: 2.1195\n",
      "--- 28.378408432006836 seconds ---\n",
      "epoch : 22/500, loss = 16195.61172241\n",
      "Avg Grad Norm: 11.7784, Avg Grad Mean: 0.2116, Avg Grad Std: 1.7500\n",
      "--- 28.36889934539795 seconds ---\n",
      "epoch : 23/500, loss = 16144.10145264\n",
      "Avg Grad Norm: 9.8492, Avg Grad Mean: 0.1614, Avg Grad Std: 1.3990\n",
      "--- 28.369733095169067 seconds ---\n",
      "epoch : 24/500, loss = 16145.50912842\n",
      "Avg Grad Norm: 30.7307, Avg Grad Mean: 0.1661, Avg Grad Std: 4.1851\n",
      "--- 28.024507761001587 seconds ---\n",
      "epoch : 25/500, loss = 18009.40941895\n",
      "Avg Grad Norm: 292.8260, Avg Grad Mean: 0.7332, Avg Grad Std: 41.3474\n",
      "--- 28.324930906295776 seconds ---\n",
      "epoch : 26/500, loss = 17449.82208984\n",
      "Avg Grad Norm: 46.1917, Avg Grad Mean: 1.6515, Avg Grad Std: 6.6488\n",
      "--- 28.181066751480103 seconds ---\n",
      "epoch : 27/500, loss = 16510.97603027\n",
      "Avg Grad Norm: 29.5223, Avg Grad Mean: 0.6188, Avg Grad Std: 3.3239\n",
      "--- 28.470449924468994 seconds ---\n",
      "epoch : 28/500, loss = 19559.12178223\n",
      "Avg Grad Norm: 596.8622, Avg Grad Mean: 4.3218, Avg Grad Std: 72.9350\n",
      "--- 28.81917119026184 seconds ---\n",
      "epoch : 29/500, loss = 19997.22510986\n",
      "Avg Grad Norm: 123.9171, Avg Grad Mean: 4.6713, Avg Grad Std: 18.3414\n",
      "--- 28.411092281341553 seconds ---\n",
      "epoch : 30/500, loss = 16780.06593140\n",
      "Avg Grad Norm: 40.3914, Avg Grad Mean: 1.1144, Avg Grad Std: 5.4821\n",
      "--- 28.90478014945984 seconds ---\n",
      "epoch : 31/500, loss = 16086.61329590\n",
      "Avg Grad Norm: 6.2188, Avg Grad Mean: 0.0681, Avg Grad Std: 0.5380\n",
      "--- 28.41910433769226 seconds ---\n",
      "epoch : 32/500, loss = 27653544.93969727\n",
      "Avg Grad Norm: 9896301.1430, Avg Grad Mean: 592835.6485, Avg Grad Std: 2232494.5589\n",
      "--- 28.41206121444702 seconds ---\n",
      "epoch : 33/500, loss = 348860087.45070314\n",
      "Avg Grad Norm: 18701488.7916, Avg Grad Mean: 48837.2850, Avg Grad Std: 1562418.5661\n",
      "--- 28.41696310043335 seconds ---\n",
      "epoch : 34/500, loss = 342890.42916504\n",
      "Avg Grad Norm: 63648.9289, Avg Grad Mean: 92.3722, Avg Grad Std: 8960.2362\n",
      "--- 28.431431531906128 seconds ---\n",
      "epoch : 35/500, loss = 31551.12167480\n",
      "Avg Grad Norm: 1631.8928, Avg Grad Mean: -1.6925, Avg Grad Std: 157.9182\n",
      "--- 28.73384404182434 seconds ---\n",
      "epoch : 36/500, loss = 57592.16442627\n",
      "Avg Grad Norm: 5591.4284, Avg Grad Mean: 4.2190, Avg Grad Std: 383.8919\n",
      "--- 28.41444158554077 seconds ---\n",
      "epoch : 37/500, loss = 18231.02585449\n",
      "Avg Grad Norm: 994.3758, Avg Grad Mean: -0.5694, Avg Grad Std: 72.6152\n",
      "--- 28.423763275146484 seconds ---\n",
      "epoch : 38/500, loss = 19773.82830200\n",
      "Avg Grad Norm: 1737.1158, Avg Grad Mean: 0.8140, Avg Grad Std: 109.4102\n",
      "--- 28.753873586654663 seconds ---\n",
      "epoch : 39/500, loss = 16724.15205566\n",
      "Avg Grad Norm: 454.2018, Avg Grad Mean: -0.4945, Avg Grad Std: 35.5358\n",
      "--- 28.413450002670288 seconds ---\n",
      "epoch : 40/500, loss = 16386.92141357\n",
      "Avg Grad Norm: 486.5486, Avg Grad Mean: 0.1188, Avg Grad Std: 31.3644\n",
      "--- 28.7377290725708 seconds ---\n",
      "epoch : 41/500, loss = 16172.10348999\n",
      "Avg Grad Norm: 162.7590, Avg Grad Mean: -0.1799, Avg Grad Std: 11.5415\n",
      "--- 28.41911554336548 seconds ---\n",
      "epoch : 42/500, loss = 28703.22320068\n",
      "Avg Grad Norm: 3758.2523, Avg Grad Mean: 1.7822, Avg Grad Std: 259.9122\n",
      "--- 28.399946928024292 seconds ---\n",
      "epoch : 43/500, loss = 2411594.43729492\n",
      "Avg Grad Norm: 110119.0512, Avg Grad Mean: 238.0305, Avg Grad Std: 10222.8545\n",
      "--- 28.39610743522644 seconds ---\n",
      "epoch : 44/500, loss = 713788.72328125\n",
      "Avg Grad Norm: 25544.8558, Avg Grad Mean: 74.9223, Avg Grad Std: 2098.8505\n",
      "--- 28.404719829559326 seconds ---\n",
      "epoch : 45/500, loss = 397769.41460938\n",
      "Avg Grad Norm: 9513.5980, Avg Grad Mean: 56.8686, Avg Grad Std: 828.9823\n",
      "--- 28.737287282943726 seconds ---\n",
      "epoch : 46/500, loss = 5374215.17519531\n",
      "Avg Grad Norm: 439045.4202, Avg Grad Mean: -550.9862, Avg Grad Std: 68501.3571\n",
      "--- 28.4087553024292 seconds ---\n",
      "epoch : 47/500, loss = 904191.86007813\n",
      "Avg Grad Norm: 36022.3116, Avg Grad Mean: 208.3101, Avg Grad Std: 5073.3524\n",
      "--- 28.021888971328735 seconds ---\n",
      "epoch : 48/500, loss = 133150.73371094\n",
      "Avg Grad Norm: 3961.3469, Avg Grad Mean: 15.9872, Avg Grad Std: 509.5220\n",
      "--- 28.33065891265869 seconds ---\n",
      "epoch : 49/500, loss = 93544.02250000\n",
      "Avg Grad Norm: 3046.4979, Avg Grad Mean: 10.7151, Avg Grad Std: 387.4671\n",
      "--- 28.3457989692688 seconds ---\n",
      "epoch : 50/500, loss = 696981.97423828\n",
      "Avg Grad Norm: 163539.5362, Avg Grad Mean: 58.1000, Avg Grad Std: 16960.1039\n",
      "--- 28.031031131744385 seconds ---\n",
      "epoch : 51/500, loss = 2338655.44800781\n",
      "Avg Grad Norm: 366020.8590, Avg Grad Mean: -57.5003, Avg Grad Std: 58917.1528\n",
      "--- 28.011163473129272 seconds ---\n",
      "epoch : 52/500, loss = 1016786.15812500\n",
      "Avg Grad Norm: 18810.2395, Avg Grad Mean: 222.6020, Avg Grad Std: 2467.6243\n",
      "--- 28.012325763702393 seconds ---\n",
      "epoch : 53/500, loss = 579978.28593750\n",
      "Avg Grad Norm: 14567.0135, Avg Grad Mean: 127.3706, Avg Grad Std: 2059.3097\n",
      "--- 28.007060050964355 seconds ---\n",
      "epoch : 54/500, loss = 372105.88578125\n",
      "Avg Grad Norm: 11422.8721, Avg Grad Mean: 82.7037, Avg Grad Std: 1692.2341\n",
      "--- 28.008503437042236 seconds ---\n",
      "epoch : 55/500, loss = 264750.97195312\n",
      "Avg Grad Norm: 41009.5706, Avg Grad Mean: 79.2580, Avg Grad Std: 6891.4702\n",
      "--- 28.31997847557068 seconds ---\n",
      "epoch : 56/500, loss = 367732.10234375\n",
      "Avg Grad Norm: 16332.4071, Avg Grad Mean: 40.5194, Avg Grad Std: 2549.0003\n",
      "--- 27.99780583381653 seconds ---\n",
      "epoch : 57/500, loss = 168781.91824219\n",
      "Avg Grad Norm: 6067.6624, Avg Grad Mean: 28.0315, Avg Grad Std: 882.5201\n",
      "--- 28.00280475616455 seconds ---\n",
      "epoch : 58/500, loss = 102031.91328125\n",
      "Avg Grad Norm: 8313.7968, Avg Grad Mean: 13.3675, Avg Grad Std: 1230.4331\n",
      "--- 28.31542158126831 seconds ---\n",
      "epoch : 59/500, loss = 110599.35142578\n",
      "Avg Grad Norm: 18833.8915, Avg Grad Mean: 20.7753, Avg Grad Std: 3175.0551\n",
      "--- 28.322707891464233 seconds ---\n",
      "epoch : 60/500, loss = 89378.53529297\n",
      "Avg Grad Norm: 14690.0902, Avg Grad Mean: 7.6013, Avg Grad Std: 2260.6927\n",
      "--- 27.99187207221985 seconds ---\n",
      "epoch : 61/500, loss = 96834.85894531\n",
      "Avg Grad Norm: 6597.1722, Avg Grad Mean: 14.2911, Avg Grad Std: 1087.1130\n",
      "--- 28.001862287521362 seconds ---\n",
      "epoch : 62/500, loss = 46103.37496094\n",
      "Avg Grad Norm: 5633.8741, Avg Grad Mean: 7.4741, Avg Grad Std: 968.9099\n",
      "--- 27.98811388015747 seconds ---\n",
      "epoch : 63/500, loss = 68438.65823242\n",
      "Avg Grad Norm: 12631.3195, Avg Grad Mean: 8.8795, Avg Grad Std: 2181.6183\n",
      "--- 27.995237350463867 seconds ---\n",
      "epoch : 64/500, loss = 42939.66733398\n",
      "Avg Grad Norm: 2340.0272, Avg Grad Mean: 3.1362, Avg Grad Std: 392.8956\n",
      "--- 27.99276614189148 seconds ---\n",
      "epoch : 65/500, loss = 8356015.30664551\n",
      "Avg Grad Norm: 581302.9917, Avg Grad Mean: -2493.0343, Avg Grad Std: 100595.1620\n",
      "--- 28.327908754348755 seconds ---\n",
      "epoch : 66/500, loss = 242498.62316406\n",
      "Avg Grad Norm: 6253.7114, Avg Grad Mean: 10.1006, Avg Grad Std: 1063.0617\n",
      "--- 28.02406072616577 seconds ---\n",
      "epoch : 67/500, loss = 44948.87477539\n",
      "Avg Grad Norm: 1737.9532, Avg Grad Mean: 3.1456, Avg Grad Std: 281.0916\n",
      "--- 28.32325530052185 seconds ---\n",
      "epoch : 68/500, loss = 42919.18238770\n",
      "Avg Grad Norm: 3575.9521, Avg Grad Mean: -4.8373, Avg Grad Std: 590.6269\n",
      "--- 28.004014015197754 seconds ---\n",
      "epoch : 69/500, loss = 30101.39697754\n",
      "Avg Grad Norm: 1738.0323, Avg Grad Mean: -4.1887, Avg Grad Std: 206.5373\n",
      "--- 28.293805599212646 seconds ---\n",
      "epoch : 70/500, loss = 28046.61457031\n",
      "Avg Grad Norm: 585.1725, Avg Grad Mean: 0.4505, Avg Grad Std: 85.1219\n",
      "--- 28.006200075149536 seconds ---\n",
      "epoch : 71/500, loss = 23783.28346680\n",
      "Avg Grad Norm: 327.3258, Avg Grad Mean: 1.4070, Avg Grad Std: 46.9643\n",
      "--- 28.012773275375366 seconds ---\n",
      "epoch : 72/500, loss = 21496.12162598\n",
      "Avg Grad Norm: 200.6824, Avg Grad Mean: 0.3836, Avg Grad Std: 28.3598\n",
      "--- 28.006511449813843 seconds ---\n",
      "epoch : 73/500, loss = 19257.93584717\n",
      "Avg Grad Norm: 312.9459, Avg Grad Mean: 0.5319, Avg Grad Std: 45.5397\n",
      "--- 28.00433111190796 seconds ---\n",
      "epoch : 74/500, loss = 17699.01804932\n",
      "Avg Grad Norm: 195.4744, Avg Grad Mean: 0.2774, Avg Grad Std: 31.2714\n",
      "--- 28.001060485839844 seconds ---\n",
      "epoch : 75/500, loss = 17537.12750488\n",
      "Avg Grad Norm: 302.9293, Avg Grad Mean: 0.3986, Avg Grad Std: 47.0311\n",
      "--- 28.323869705200195 seconds ---\n",
      "epoch : 76/500, loss = 17311.10875488\n",
      "Avg Grad Norm: 89.7016, Avg Grad Mean: 0.2298, Avg Grad Std: 12.6513\n",
      "--- 27.997999906539917 seconds ---\n",
      "epoch : 77/500, loss = 17028.41468262\n",
      "Avg Grad Norm: 63.5625, Avg Grad Mean: 0.2412, Avg Grad Std: 7.5634\n",
      "--- 28.316307306289673 seconds ---\n",
      "epoch : 78/500, loss = 16883.07156494\n",
      "Avg Grad Norm: 71.0400, Avg Grad Mean: 0.2407, Avg Grad Std: 8.4087\n",
      "--- 27.987796545028687 seconds ---\n",
      "epoch : 79/500, loss = 16770.87538330\n",
      "Avg Grad Norm: 115.7730, Avg Grad Mean: 0.1844, Avg Grad Std: 15.3030\n",
      "--- 28.26738166809082 seconds ---\n",
      "epoch : 80/500, loss = 17026.19516113\n",
      "Avg Grad Norm: 532.9944, Avg Grad Mean: -1.9733, Avg Grad Std: 72.5314\n",
      "--- 27.985287189483643 seconds ---\n",
      "epoch : 81/500, loss = 22696.13161133\n",
      "Avg Grad Norm: 1134.0074, Avg Grad Mean: 3.9762, Avg Grad Std: 156.3737\n",
      "--- 27.994212865829468 seconds ---\n",
      "epoch : 82/500, loss = 17811.68949219\n",
      "Avg Grad Norm: 65.4960, Avg Grad Mean: 0.1211, Avg Grad Std: 8.5763\n",
      "--- 27.994120597839355 seconds ---\n",
      "epoch : 83/500, loss = 17216.30149902\n",
      "Avg Grad Norm: 150.0196, Avg Grad Mean: -0.0625, Avg Grad Std: 20.2440\n",
      "--- 28.009225845336914 seconds ---\n",
      "epoch : 84/500, loss = 77077.08427002\n",
      "Avg Grad Norm: 9239.8795, Avg Grad Mean: 23.4317, Avg Grad Std: 1368.1881\n",
      "--- 28.01565194129944 seconds ---\n",
      "epoch : 85/500, loss = 22319.12720215\n",
      "Avg Grad Norm: 308.5796, Avg Grad Mean: 0.8079, Avg Grad Std: 40.1612\n",
      "--- 28.342350482940674 seconds ---\n",
      "epoch : 86/500, loss = 19059.64137451\n",
      "Avg Grad Norm: 113.2642, Avg Grad Mean: 0.2184, Avg Grad Std: 18.0142\n",
      "--- 28.019338369369507 seconds ---\n",
      "epoch : 87/500, loss = 45049.71979248\n",
      "Avg Grad Norm: 6366.0937, Avg Grad Mean: 12.0180, Avg Grad Std: 1095.2771\n",
      "--- 28.325580596923828 seconds ---\n",
      "epoch : 88/500, loss = 58201.30696777\n",
      "Avg Grad Norm: 10537.8047, Avg Grad Mean: -18.3487, Avg Grad Std: 1786.9664\n",
      "--- 28.04205632209778 seconds ---\n",
      "epoch : 89/500, loss = 6167826.87692139\n",
      "Avg Grad Norm: 4234076.7097, Avg Grad Mean: 575304.1538, Avg Grad Std: 1163756.5760\n",
      "--- 28.349193334579468 seconds ---\n",
      "epoch : 90/500, loss = 272970.37863281\n",
      "Avg Grad Norm: 87766.6225, Avg Grad Mean: 89.2702, Avg Grad Std: 15962.3138\n",
      "--- 28.020289182662964 seconds ---\n",
      "epoch : 91/500, loss = 193163.21255859\n",
      "Avg Grad Norm: 71969.1929, Avg Grad Mean: 95.9156, Avg Grad Std: 13119.7405\n",
      "--- 28.02118182182312 seconds ---\n",
      "epoch : 92/500, loss = 71775.59598633\n",
      "Avg Grad Norm: 25659.5385, Avg Grad Mean: 38.2511, Avg Grad Std: 4681.2206\n",
      "--- 28.016629457473755 seconds ---\n",
      "epoch : 93/500, loss = 45526.50470215\n",
      "Avg Grad Norm: 22722.9750, Avg Grad Mean: -3.2240, Avg Grad Std: 4227.0891\n",
      "--- 28.020708799362183 seconds ---\n",
      "epoch : 94/500, loss = 882103.20212402\n",
      "Avg Grad Norm: 82265.7705, Avg Grad Mean: 411.8248, Avg Grad Std: 15085.4702\n",
      "--- 28.34505534172058 seconds ---\n",
      "epoch : 95/500, loss = 32063.60727539\n",
      "Avg Grad Norm: 1682.3172, Avg Grad Mean: 16.3380, Avg Grad Std: 259.4371\n",
      "--- 28.02047562599182 seconds ---\n",
      "epoch : 96/500, loss = 124650.74968262\n",
      "Avg Grad Norm: 22079.9854, Avg Grad Mean: 43.7214, Avg Grad Std: 4023.5573\n",
      "--- 28.008344173431396 seconds ---\n",
      "epoch : 97/500, loss = 37294.92542725\n",
      "Avg Grad Norm: 13911.5678, Avg Grad Mean: 10.8403, Avg Grad Std: 2580.3062\n",
      "--- 28.310790061950684 seconds ---\n",
      "epoch : 98/500, loss = 109562.19868896\n",
      "Avg Grad Norm: 32091.6478, Avg Grad Mean: 91.6004, Avg Grad Std: 6061.3011\n",
      "--- 28.01918363571167 seconds ---\n",
      "epoch : 99/500, loss = 2407782.40790039\n",
      "Avg Grad Norm: 170329.3686, Avg Grad Mean: 973.6008, Avg Grad Std: 22229.6947\n",
      "--- 28.30966544151306 seconds ---\n",
      "epoch : 100/500, loss = 3829047.50023926\n",
      "Avg Grad Norm: 780422.3916, Avg Grad Mean: 559.5671, Avg Grad Std: 94238.4110\n",
      "--- 28.005029916763306 seconds ---\n",
      "epoch : 101/500, loss = 3503424.74720215\n",
      "Avg Grad Norm: 327130.9594, Avg Grad Mean: 1742.2238, Avg Grad Std: 62674.7156\n",
      "--- 28.00906205177307 seconds ---\n",
      "epoch : 102/500, loss = 675756048.62450194\n",
      "Avg Grad Norm: 271479483.1279, Avg Grad Mean: 2068405.9270, Avg Grad Std: 46565434.2099\n",
      "--- 28.016825675964355 seconds ---\n",
      "epoch : 103/500, loss = 1575095.94054687\n",
      "Avg Grad Norm: 96561.0239, Avg Grad Mean: 1054.8421, Avg Grad Std: 11554.9317\n",
      "--- 28.025569200515747 seconds ---\n",
      "epoch : 104/500, loss = 2055377.30031250\n",
      "Avg Grad Norm: 71689.2582, Avg Grad Mean: 593.8006, Avg Grad Std: 8492.1450\n",
      "--- 28.342920541763306 seconds ---\n",
      "epoch : 105/500, loss = 613833.45703125\n",
      "Avg Grad Norm: 18002.3952, Avg Grad Mean: 647.2245, Avg Grad Std: 1893.4751\n",
      "--- 28.043213844299316 seconds ---\n",
      "epoch : 106/500, loss = 389415.86656250\n",
      "Avg Grad Norm: 11508.3490, Avg Grad Mean: 416.7108, Avg Grad Std: 1246.4083\n",
      "--- 28.021482467651367 seconds ---\n",
      "epoch : 107/500, loss = 1783148.19281250\n",
      "Avg Grad Norm: 153243.5118, Avg Grad Mean: 183.9078, Avg Grad Std: 20623.9986\n",
      "--- 28.34402298927307 seconds ---\n",
      "epoch : 108/500, loss = 996147.93796875\n",
      "Avg Grad Norm: 46899.8084, Avg Grad Mean: 49.3956, Avg Grad Std: 6154.0663\n",
      "--- 28.463165521621704 seconds ---\n",
      "epoch : 109/500, loss = 303865.64671875\n",
      "Avg Grad Norm: 36659.3488, Avg Grad Mean: 181.4107, Avg Grad Std: 4110.4115\n",
      "--- 28.73134207725525 seconds ---\n",
      "epoch : 110/500, loss = 216951.37687500\n",
      "Avg Grad Norm: 20614.7014, Avg Grad Mean: 58.3764, Avg Grad Std: 2414.8528\n",
      "--- 28.366063117980957 seconds ---\n",
      "epoch : 111/500, loss = 1347199.27898438\n",
      "Avg Grad Norm: 144330.2312, Avg Grad Mean: -711.1913, Avg Grad Std: 15902.1132\n",
      "--- 28.37641954421997 seconds ---\n",
      "epoch : 112/500, loss = 568265.25648437\n",
      "Avg Grad Norm: 29235.8358, Avg Grad Mean: 782.6001, Avg Grad Std: 3605.9246\n",
      "--- 28.395348072052002 seconds ---\n",
      "epoch : 113/500, loss = 150115.19859375\n",
      "Avg Grad Norm: 7441.6922, Avg Grad Mean: 280.3265, Avg Grad Std: 877.3203\n",
      "--- 28.387691974639893 seconds ---\n",
      "epoch : 114/500, loss = 83168.54050781\n",
      "Avg Grad Norm: 4317.0939, Avg Grad Mean: 151.4792, Avg Grad Std: 536.6551\n",
      "--- 28.882630109786987 seconds ---\n",
      "epoch : 115/500, loss = 57399.57425781\n",
      "Avg Grad Norm: 2915.0755, Avg Grad Mean: 73.0564, Avg Grad Std: 410.8168\n",
      "--- 28.343819856643677 seconds ---\n",
      "epoch : 116/500, loss = 49750.40546875\n",
      "Avg Grad Norm: 2030.9907, Avg Grad Mean: 17.1803, Avg Grad Std: 322.0191\n",
      "--- 28.211816787719727 seconds ---\n",
      "epoch : 117/500, loss = 45673.31175781\n",
      "Avg Grad Norm: 1943.4462, Avg Grad Mean: 4.3657, Avg Grad Std: 317.4275\n",
      "--- 28.74116849899292 seconds ---\n",
      "epoch : 118/500, loss = 18823116.94019531\n",
      "Avg Grad Norm: 7361369.2485, Avg Grad Mean: -126407.4675, Avg Grad Std: 1115823.0487\n",
      "--- 28.758708715438843 seconds ---\n",
      "epoch : 119/500, loss = 4919634.24875000\n",
      "Avg Grad Norm: 71656.1214, Avg Grad Mean: 1203.4858, Avg Grad Std: 10406.4813\n",
      "--- 28.38749933242798 seconds ---\n",
      "epoch : 120/500, loss = 1149867.92546875\n",
      "Avg Grad Norm: 25932.9255, Avg Grad Mean: 243.0055, Avg Grad Std: 3782.1271\n",
      "--- 28.3803608417511 seconds ---\n",
      "epoch : 121/500, loss = 289007.65117188\n",
      "Avg Grad Norm: 10192.4074, Avg Grad Mean: -30.1070, Avg Grad Std: 1619.2982\n",
      "--- 28.389626026153564 seconds ---\n",
      "epoch : 122/500, loss = 160118.39156250\n",
      "Avg Grad Norm: 7919.4199, Avg Grad Mean: -17.2841, Avg Grad Std: 1232.8757\n",
      "--- 28.36691379547119 seconds ---\n",
      "epoch : 123/500, loss = 114233.40945313\n",
      "Avg Grad Norm: 3902.7642, Avg Grad Mean: -1.8279, Avg Grad Std: 649.6791\n",
      "--- 28.361116886138916 seconds ---\n",
      "epoch : 124/500, loss = 97389.29285156\n",
      "Avg Grad Norm: 3764.5698, Avg Grad Mean: 3.2204, Avg Grad Std: 561.5985\n",
      "--- 28.739648580551147 seconds ---\n",
      "epoch : 125/500, loss = 90083.20126953\n",
      "Avg Grad Norm: 9163.4833, Avg Grad Mean: -1.8785, Avg Grad Std: 1225.0438\n",
      "--- 28.389071702957153 seconds ---\n",
      "epoch : 126/500, loss = 353052.74214844\n",
      "Avg Grad Norm: 100969.3439, Avg Grad Mean: 775.4962, Avg Grad Std: 14489.4929\n",
      "--- 28.390070915222168 seconds ---\n",
      "epoch : 127/500, loss = 586523.23953125\n",
      "Avg Grad Norm: 14097.4623, Avg Grad Mean: 50.5143, Avg Grad Std: 1863.3574\n",
      "--- 28.729719877243042 seconds ---\n",
      "epoch : 128/500, loss = 248098.27406250\n",
      "Avg Grad Norm: 9326.3393, Avg Grad Mean: 63.0492, Avg Grad Std: 1480.2548\n",
      "--- 28.75378918647766 seconds ---\n",
      "epoch : 129/500, loss = 168480.07195313\n",
      "Avg Grad Norm: 9106.4743, Avg Grad Mean: 47.7879, Avg Grad Std: 1518.4544\n",
      "--- 28.413458347320557 seconds ---\n",
      "epoch : 130/500, loss = 125691.86437500\n",
      "Avg Grad Norm: 13170.8326, Avg Grad Mean: 33.4924, Avg Grad Std: 2255.3564\n",
      "--- 28.37525963783264 seconds ---\n",
      "epoch : 131/500, loss = 99693.41796875\n",
      "Avg Grad Norm: 13166.2813, Avg Grad Mean: 31.4718, Avg Grad Std: 2264.5288\n",
      "--- 28.40036177635193 seconds ---\n",
      "epoch : 132/500, loss = 80098.94419922\n",
      "Avg Grad Norm: 12437.7843, Avg Grad Mean: 31.5275, Avg Grad Std: 2143.5981\n",
      "--- 28.41887331008911 seconds ---\n",
      "epoch : 133/500, loss = 86868.07710937\n",
      "Avg Grad Norm: 22995.4271, Avg Grad Mean: 59.3902, Avg Grad Std: 3936.3422\n",
      "--- 28.419158697128296 seconds ---\n",
      "epoch : 134/500, loss = 243982.86367188\n",
      "Avg Grad Norm: 45115.7829, Avg Grad Mean: 165.8097, Avg Grad Std: 7352.2830\n",
      "--- 28.743717432022095 seconds ---\n",
      "epoch : 135/500, loss = 122090.57679688\n",
      "Avg Grad Norm: 4209.7537, Avg Grad Mean: 8.6505, Avg Grad Std: 536.3992\n",
      "--- 28.41579246520996 seconds ---\n",
      "epoch : 136/500, loss = 48376.26513672\n",
      "Avg Grad Norm: 1308.5380, Avg Grad Mean: 14.0596, Avg Grad Std: 168.2918\n",
      "--- 29.937748908996582 seconds ---\n",
      "epoch : 137/500, loss = 34122.17701172\n",
      "Avg Grad Norm: 2084.5002, Avg Grad Mean: 18.7622, Avg Grad Std: 301.4930\n",
      "--- 28.870663166046143 seconds ---\n",
      "epoch : 138/500, loss = 52174.57609375\n",
      "Avg Grad Norm: 7187.9701, Avg Grad Mean: 16.9867, Avg Grad Std: 965.8595\n",
      "--- 29.535398960113525 seconds ---\n",
      "epoch : 139/500, loss = 28543.94455566\n",
      "Avg Grad Norm: 2743.2902, Avg Grad Mean: 11.2024, Avg Grad Std: 396.5300\n",
      "--- 28.85161280632019 seconds ---\n",
      "epoch : 140/500, loss = 25564.01003418\n",
      "Avg Grad Norm: 2222.2412, Avg Grad Mean: 5.4837, Avg Grad Std: 322.4570\n",
      "--- 28.834720849990845 seconds ---\n",
      "epoch : 141/500, loss = 24303.54390625\n",
      "Avg Grad Norm: 2327.6794, Avg Grad Mean: 7.1765, Avg Grad Std: 336.7870\n",
      "--- 28.450282335281372 seconds ---\n",
      "epoch : 142/500, loss = 23105.04084473\n",
      "Avg Grad Norm: 2219.7396, Avg Grad Mean: 5.1304, Avg Grad Std: 331.7661\n",
      "--- 28.23241376876831 seconds ---\n",
      "epoch : 143/500, loss = 23371.19608887\n",
      "Avg Grad Norm: 2590.7079, Avg Grad Mean: 6.5489, Avg Grad Std: 387.5410\n",
      "--- 28.26088047027588 seconds ---\n",
      "epoch : 144/500, loss = 142591.05195312\n",
      "Avg Grad Norm: 25827.5025, Avg Grad Mean: -37.4683, Avg Grad Std: 3988.4783\n",
      "--- 28.598405838012695 seconds ---\n",
      "epoch : 145/500, loss = 72302.83154297\n",
      "Avg Grad Norm: 3737.1676, Avg Grad Mean: 43.6183, Avg Grad Std: 542.6983\n",
      "--- 28.235223531723022 seconds ---\n",
      "epoch : 146/500, loss = 43626.27077148\n",
      "Avg Grad Norm: 3062.6120, Avg Grad Mean: 8.3610, Avg Grad Std: 437.4447\n",
      "--- 28.56807541847229 seconds ---\n",
      "epoch : 147/500, loss = 70674.48444336\n",
      "Avg Grad Norm: 19760.0190, Avg Grad Mean: -31.0431, Avg Grad Std: 3296.2883\n",
      "--- 28.292499542236328 seconds ---\n",
      "epoch : 148/500, loss = 33244.78620117\n",
      "Avg Grad Norm: 1493.7447, Avg Grad Mean: 18.8012, Avg Grad Std: 230.1442\n",
      "--- 28.611364364624023 seconds ---\n",
      "epoch : 149/500, loss = 28348.31776367\n",
      "Avg Grad Norm: 2364.0778, Avg Grad Mean: 8.0944, Avg Grad Std: 386.2154\n",
      "--- 28.256391286849976 seconds ---\n",
      "epoch : 150/500, loss = 25898.49199219\n",
      "Avg Grad Norm: 2773.6851, Avg Grad Mean: 9.8402, Avg Grad Std: 466.1150\n",
      "--- 28.224379777908325 seconds ---\n",
      "epoch : 151/500, loss = 24383.93240723\n",
      "Avg Grad Norm: 2294.5884, Avg Grad Mean: 8.5408, Avg Grad Std: 382.6684\n",
      "--- 28.23931384086609 seconds ---\n",
      "epoch : 152/500, loss = 23440.38732910\n",
      "Avg Grad Norm: 2536.5858, Avg Grad Mean: 5.5477, Avg Grad Std: 423.9152\n",
      "--- 28.23284101486206 seconds ---\n",
      "epoch : 153/500, loss = 22589.74581543\n",
      "Avg Grad Norm: 2581.6508, Avg Grad Mean: 5.7375, Avg Grad Std: 435.6923\n",
      "--- 28.233876943588257 seconds ---\n",
      "epoch : 154/500, loss = 21951.96914551\n",
      "Avg Grad Norm: 2517.7696, Avg Grad Mean: 5.0441, Avg Grad Std: 428.7217\n",
      "--- 28.569162368774414 seconds ---\n",
      "epoch : 155/500, loss = 21367.22679932\n",
      "Avg Grad Norm: 2302.6635, Avg Grad Mean: 5.1221, Avg Grad Std: 394.6287\n",
      "--- 28.245994329452515 seconds ---\n",
      "epoch : 156/500, loss = 21054.35085938\n",
      "Avg Grad Norm: 2623.1561, Avg Grad Mean: 3.6278, Avg Grad Std: 450.6431\n",
      "--- 28.558436393737793 seconds ---\n",
      "epoch : 157/500, loss = 20395.84222168\n",
      "Avg Grad Norm: 2205.9007, Avg Grad Mean: 4.6119, Avg Grad Std: 381.2968\n",
      "--- 28.23496127128601 seconds ---\n",
      "epoch : 158/500, loss = 20119.42473877\n",
      "Avg Grad Norm: 2321.0101, Avg Grad Mean: 3.0205, Avg Grad Std: 403.2267\n",
      "--- 28.561750888824463 seconds ---\n",
      "epoch : 159/500, loss = 19715.19644775\n",
      "Avg Grad Norm: 2290.5197, Avg Grad Mean: 2.3982, Avg Grad Std: 399.9596\n",
      "--- 28.262985229492188 seconds ---\n",
      "epoch : 160/500, loss = 19586.13868164\n",
      "Avg Grad Norm: 2138.4338, Avg Grad Mean: 2.9109, Avg Grad Std: 375.2430\n",
      "--- 28.240692377090454 seconds ---\n",
      "epoch : 161/500, loss = 19261.97583984\n",
      "Avg Grad Norm: 2193.4064, Avg Grad Mean: 2.2056, Avg Grad Std: 387.8400\n",
      "--- 28.243286848068237 seconds ---\n",
      "epoch : 162/500, loss = 3815587.37990967\n",
      "Avg Grad Norm: 537282.5803, Avg Grad Mean: 6352.1727, Avg Grad Std: 115418.4814\n",
      "--- 28.270986557006836 seconds ---\n",
      "epoch : 163/500, loss = 291480.78726563\n",
      "Avg Grad Norm: 7044.6759, Avg Grad Mean: 49.1895, Avg Grad Std: 1469.4752\n",
      "--- 28.61345624923706 seconds ---\n",
      "epoch : 164/500, loss = 171788.38207031\n",
      "Avg Grad Norm: 4481.9274, Avg Grad Mean: 23.1876, Avg Grad Std: 915.7709\n",
      "--- 28.293263912200928 seconds ---\n",
      "epoch : 165/500, loss = 112188.10847656\n",
      "Avg Grad Norm: 3101.5698, Avg Grad Mean: 10.7775, Avg Grad Std: 625.4844\n",
      "--- 28.305261850357056 seconds ---\n",
      "epoch : 166/500, loss = 78160.91824219\n",
      "Avg Grad Norm: 2355.8454, Avg Grad Mean: 5.9074, Avg Grad Std: 474.2350\n",
      "--- 28.63737416267395 seconds ---\n",
      "epoch : 167/500, loss = 55740.27449219\n",
      "Avg Grad Norm: 1560.8399, Avg Grad Mean: -0.1149, Avg Grad Std: 309.3754\n",
      "--- 28.164175033569336 seconds ---\n",
      "epoch : 168/500, loss = 39869.64266602\n",
      "Avg Grad Norm: 925.9476, Avg Grad Mean: -3.4653, Avg Grad Std: 177.9612\n",
      "--- 28.21756911277771 seconds ---\n",
      "epoch : 169/500, loss = 31156.74800781\n",
      "Avg Grad Norm: 708.5618, Avg Grad Mean: -2.4288, Avg Grad Std: 130.2609\n",
      "--- 27.86042833328247 seconds ---\n",
      "epoch : 170/500, loss = 27225.22913086\n",
      "Avg Grad Norm: 622.3466, Avg Grad Mean: -1.8916, Avg Grad Std: 114.1449\n",
      "--- 27.87162709236145 seconds ---\n",
      "epoch : 171/500, loss = 24594.54395020\n",
      "Avg Grad Norm: 580.6405, Avg Grad Mean: -1.4936, Avg Grad Std: 107.1022\n",
      "--- 27.86951518058777 seconds ---\n",
      "epoch : 172/500, loss = 27294.34873535\n",
      "Avg Grad Norm: 7676.7831, Avg Grad Mean: 35.2026, Avg Grad Std: 1490.6148\n",
      "--- 27.86751103401184 seconds ---\n",
      "epoch : 173/500, loss = 32213.90799805\n",
      "Avg Grad Norm: 1310.6170, Avg Grad Mean: 4.0530, Avg Grad Std: 271.7186\n",
      "--- 28.208871841430664 seconds ---\n",
      "epoch : 174/500, loss = 22775.47298340\n",
      "Avg Grad Norm: 562.5439, Avg Grad Mean: 0.1364, Avg Grad Std: 111.5161\n",
      "--- 27.87876605987549 seconds ---\n",
      "epoch : 175/500, loss = 19978.06499756\n",
      "Avg Grad Norm: 283.4803, Avg Grad Mean: -0.5224, Avg Grad Std: 55.1755\n",
      "--- 27.869414806365967 seconds ---\n",
      "epoch : 176/500, loss = 122477.69519043\n",
      "Avg Grad Norm: 25803.3862, Avg Grad Mean: 49.2565, Avg Grad Std: 5371.7054\n",
      "--- 28.19382619857788 seconds ---\n",
      "epoch : 177/500, loss = 30807.28064941\n",
      "Avg Grad Norm: 792.7521, Avg Grad Mean: 6.3835, Avg Grad Std: 157.2426\n",
      "--- 27.869588136672974 seconds ---\n",
      "epoch : 178/500, loss = 23745.88489746\n",
      "Avg Grad Norm: 478.2133, Avg Grad Mean: 3.1519, Avg Grad Std: 91.1984\n",
      "--- 28.18516731262207 seconds ---\n",
      "epoch : 179/500, loss = 20187.51381104\n",
      "Avg Grad Norm: 299.8865, Avg Grad Mean: 1.7886, Avg Grad Std: 56.2057\n",
      "--- 27.870418310165405 seconds ---\n",
      "epoch : 180/500, loss = 18313.60614258\n",
      "Avg Grad Norm: 180.5231, Avg Grad Mean: 0.9959, Avg Grad Std: 33.5630\n",
      "--- 27.865308046340942 seconds ---\n",
      "epoch : 181/500, loss = 17214.09635498\n",
      "Avg Grad Norm: 109.6057, Avg Grad Mean: 0.5142, Avg Grad Std: 20.0505\n",
      "--- 27.869932174682617 seconds ---\n",
      "epoch : 182/500, loss = 104331.82625000\n",
      "Avg Grad Norm: 12384.0570, Avg Grad Mean: -3.0716, Avg Grad Std: 2331.5009\n",
      "--- 27.856980562210083 seconds ---\n",
      "epoch : 183/500, loss = 30199.82452148\n",
      "Avg Grad Norm: 6153.3441, Avg Grad Mean: 0.0217, Avg Grad Std: 1201.4774\n",
      "--- 28.17695951461792 seconds ---\n",
      "epoch : 184/500, loss = 19917.59211182\n",
      "Avg Grad Norm: 223.9852, Avg Grad Mean: -0.9230, Avg Grad Std: 28.8815\n",
      "--- 27.86518168449402 seconds ---\n",
      "epoch : 185/500, loss = 17044.87750488\n",
      "Avg Grad Norm: 144.5509, Avg Grad Mean: -0.4216, Avg Grad Std: 22.6673\n",
      "--- 27.86179804801941 seconds ---\n",
      "epoch : 186/500, loss = 34441.37244629\n",
      "Avg Grad Norm: 3912.4444, Avg Grad Mean: -2.1077, Avg Grad Std: 649.1102\n",
      "--- 28.182984828948975 seconds ---\n",
      "epoch : 187/500, loss = 19071.59971924\n",
      "Avg Grad Norm: 139.4435, Avg Grad Mean: -0.3015, Avg Grad Std: 16.7790\n",
      "--- 27.865180253982544 seconds ---\n",
      "epoch : 188/500, loss = 17266.46570557\n",
      "Avg Grad Norm: 217.0961, Avg Grad Mean: -0.0880, Avg Grad Std: 35.0970\n",
      "--- 28.179231882095337 seconds ---\n",
      "epoch : 189/500, loss = 16995.55455566\n",
      "Avg Grad Norm: 726.8401, Avg Grad Mean: 0.0280, Avg Grad Std: 121.5051\n",
      "--- 27.857433795928955 seconds ---\n",
      "epoch : 190/500, loss = 29756.09049072\n",
      "Avg Grad Norm: 3960.4568, Avg Grad Mean: 13.0964, Avg Grad Std: 739.1890\n",
      "--- 27.859555959701538 seconds ---\n",
      "epoch : 191/500, loss = 18369.30403564\n",
      "Avg Grad Norm: 139.8238, Avg Grad Mean: -0.7310, Avg Grad Std: 27.2498\n",
      "--- 27.85294532775879 seconds ---\n",
      "epoch : 192/500, loss = 17571.14522461\n",
      "Avg Grad Norm: 47.1795, Avg Grad Mean: -0.4174, Avg Grad Std: 9.1007\n",
      "--- 27.86514687538147 seconds ---\n",
      "epoch : 193/500, loss = 17230.77618896\n",
      "Avg Grad Norm: 44.2212, Avg Grad Mean: -0.2451, Avg Grad Std: 8.5229\n",
      "--- 28.189902544021606 seconds ---\n",
      "epoch : 194/500, loss = 17096.60854492\n",
      "Avg Grad Norm: 114.3470, Avg Grad Mean: -0.0857, Avg Grad Std: 22.5296\n",
      "--- 27.869980335235596 seconds ---\n",
      "epoch : 195/500, loss = 17040.38017090\n",
      "Avg Grad Norm: 179.7500, Avg Grad Mean: -0.0040, Avg Grad Std: 35.5761\n",
      "--- 27.871617078781128 seconds ---\n",
      "epoch : 196/500, loss = 16994.71802490\n",
      "Avg Grad Norm: 212.4237, Avg Grad Mean: -0.0595, Avg Grad Std: 42.1936\n",
      "--- 28.198259592056274 seconds ---\n",
      "epoch : 197/500, loss = 16930.63370605\n",
      "Avg Grad Norm: 179.2277, Avg Grad Mean: -0.0445, Avg Grad Std: 35.7795\n",
      "--- 28.17061471939087 seconds ---\n",
      "epoch : 198/500, loss = 17266.00935303\n",
      "Avg Grad Norm: 545.1135, Avg Grad Mean: 0.1801, Avg Grad Std: 107.8105\n",
      "--- 27.859036922454834 seconds ---\n",
      "epoch : 199/500, loss = 17164.17847412\n",
      "Avg Grad Norm: 20.3158, Avg Grad Mean: -0.1366, Avg Grad Std: 3.6698\n",
      "--- 27.859328985214233 seconds ---\n",
      "epoch : 200/500, loss = 16914.83430786\n",
      "Avg Grad Norm: 120.1741, Avg Grad Mean: -0.0682, Avg Grad Std: 25.2599\n",
      "--- 27.867638111114502 seconds ---\n",
      "epoch : 201/500, loss = 16817.99224121\n",
      "Avg Grad Norm: 189.5739, Avg Grad Mean: -0.0645, Avg Grad Std: 39.5174\n",
      "--- 27.860579013824463 seconds ---\n",
      "epoch : 202/500, loss = 16748.05444824\n",
      "Avg Grad Norm: 114.3714, Avg Grad Mean: 0.0185, Avg Grad Std: 23.8671\n",
      "--- 27.86539125442505 seconds ---\n",
      "epoch : 203/500, loss = 66592.79024170\n",
      "Avg Grad Norm: 618423.0899, Avg Grad Mean: 2625.3668, Avg Grad Std: 104528.3430\n",
      "--- 28.192349672317505 seconds ---\n",
      "epoch : 204/500, loss = 996453.15087891\n",
      "Avg Grad Norm: 103855.3634, Avg Grad Mean: 6441.1986, Avg Grad Std: 31671.8682\n",
      "--- 27.87755060195923 seconds ---\n",
      "epoch : 205/500, loss = 1082215.55728516\n",
      "Avg Grad Norm: 75097.0489, Avg Grad Mean: 1093.1485, Avg Grad Std: 10315.4776\n",
      "--- 27.865166187286377 seconds ---\n",
      "epoch : 206/500, loss = 326248.49386719\n",
      "Avg Grad Norm: 17690.7770, Avg Grad Mean: 215.8949, Avg Grad Std: 2701.6251\n",
      "--- 28.165507316589355 seconds ---\n",
      "epoch : 207/500, loss = 151062.73982422\n",
      "Avg Grad Norm: 11565.4835, Avg Grad Mean: 88.3536, Avg Grad Std: 1527.3744\n",
      "--- 28.149047374725342 seconds ---\n",
      "epoch : 208/500, loss = 104113647.61453125\n",
      "Avg Grad Norm: 63911196.4874, Avg Grad Mean: 192647.6674, Avg Grad Std: 6869008.8888\n",
      "--- 27.84113383293152 seconds ---\n",
      "epoch : 209/500, loss = 66682.88917480\n",
      "Avg Grad Norm: 9340.2758, Avg Grad Mean: 336.3818, Avg Grad Std: 1192.7063\n",
      "--- 27.931509017944336 seconds ---\n",
      "epoch : 210/500, loss = 16286375.06920899\n",
      "Avg Grad Norm: 2724770.2712, Avg Grad Mean: 119291.5660, Avg Grad Std: 519595.8208\n",
      "--- 27.84897255897522 seconds ---\n",
      "epoch : 211/500, loss = 6684434.15125000\n",
      "Avg Grad Norm: 141845.7983, Avg Grad Mean: 1798.9236, Avg Grad Std: 18579.2571\n",
      "--- 27.84014368057251 seconds ---\n",
      "epoch : 212/500, loss = 2340856.23687500\n",
      "Avg Grad Norm: 58454.0129, Avg Grad Mean: 551.8745, Avg Grad Std: 7482.0579\n",
      "--- 27.8498055934906 seconds ---\n",
      "epoch : 213/500, loss = 1211052.93062500\n",
      "Avg Grad Norm: 34982.6613, Avg Grad Mean: 329.7993, Avg Grad Std: 3896.7845\n",
      "--- 28.195112228393555 seconds ---\n",
      "epoch : 214/500, loss = 693498.71921875\n",
      "Avg Grad Norm: 20646.0784, Avg Grad Mean: 233.4912, Avg Grad Std: 2121.2424\n",
      "--- 27.86801266670227 seconds ---\n",
      "epoch : 215/500, loss = 404829.93093750\n",
      "Avg Grad Norm: 13126.2542, Avg Grad Mean: 164.8465, Avg Grad Std: 1192.0787\n",
      "--- 28.1818528175354 seconds ---\n",
      "epoch : 216/500, loss = 238553.65414063\n",
      "Avg Grad Norm: 8398.7044, Avg Grad Mean: 112.6667, Avg Grad Std: 669.7760\n",
      "--- 27.871331453323364 seconds ---\n",
      "epoch : 217/500, loss = 143984.11117188\n",
      "Avg Grad Norm: 5880.6362, Avg Grad Mean: 76.2872, Avg Grad Std: 446.4150\n",
      "--- 28.137009859085083 seconds ---\n",
      "epoch : 218/500, loss = 91086.48750000\n",
      "Avg Grad Norm: 4423.6075, Avg Grad Mean: 50.8374, Avg Grad Std: 353.6092\n",
      "--- 27.84862208366394 seconds ---\n",
      "epoch : 219/500, loss = 61496.75830078\n",
      "Avg Grad Norm: 3263.1437, Avg Grad Mean: 33.3537, Avg Grad Std: 265.9059\n",
      "--- 27.856987953186035 seconds ---\n",
      "epoch : 220/500, loss = 44949.99263672\n",
      "Avg Grad Norm: 2756.1744, Avg Grad Mean: 22.7428, Avg Grad Std: 212.0268\n",
      "--- 27.861372232437134 seconds ---\n",
      "epoch : 221/500, loss = 35350.06430664\n",
      "Avg Grad Norm: 5508.7950, Avg Grad Mean: 4.8914, Avg Grad Std: 521.1332\n",
      "--- 27.85685682296753 seconds ---\n",
      "epoch : 222/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 27.8728244304657 seconds ---\n",
      "epoch : 223/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 28.17935299873352 seconds ---\n",
      "epoch : 224/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 27.84479260444641 seconds ---\n",
      "epoch : 225/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 28.17250895500183 seconds ---\n",
      "epoch : 226/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 27.866450548171997 seconds ---\n",
      "epoch : 227/500, loss = nan\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 28.140861749649048 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m grad_norm, grad_mean, grad_std \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grad_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m epoch_grad_norms\u001b[38;5;241m.\u001b[39mappend(grad_norm)\n\u001b[1;32m     63\u001b[0m epoch_grad_means\u001b[38;5;241m.\u001b[39mappend(grad_mean)\n",
      "Cell \u001b[0;32mIn[143], line 31\u001b[0m, in \u001b[0;36mcompute_grad_stats\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         grad_norms\u001b[38;5;241m.\u001b[39mappend(\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m         grad_means\u001b[38;5;241m.\u001b[39mappend(param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     33\u001b[0m         grad_stds\u001b[38;5;241m.\u001b[39mappend(param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions, num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "total_epochs = 500\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    loss_class = nn.BCEWithLogitsLoss()\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = loss_class(output_function.swapaxes(0,1), target_function)\n",
    "    return params_loss*lam + y_loss*(1-lam)\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "\n",
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "grad_norms = []\n",
    "grad_means = []\n",
    "grad_stds = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "        '''for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: grad norm = {param.grad.norm()}, grad std = {param.grad.std()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")'''\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    grad_norms.append(avg_grad_norm)\n",
    "    grad_means.append(avg_grad_mean)\n",
    "    grad_stds.append(avg_grad_std)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, total_epochs + 1), train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot gradient norm\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, total_epochs + 1), grad_norms)\n",
    "plt.title('Average Gradient Norm')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Gradient Norm')\n",
    "\n",
    "# Plot gradient mean and std\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, total_epochs + 1), grad_means, label='Mean')\n",
    "plt.plot(range(1, total_epochs + 1), grad_stds, label='Std')\n",
    "plt.title('Gradient Mean and Std')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: tensor([0.4318, 0.0094, 0.5542, 0.0000, 0.0046], device='cuda:3',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "true_func: tensor([1., 0., 0., 0., 0.], device='cuda:3')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8881840550>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values)\n",
    "ran = np.random.randint(best_func.shape[1])\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"true_func: {target_funcs[ran, 0:5]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
