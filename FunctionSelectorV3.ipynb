{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.selu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device).requires_grad_(True)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def sympy_to_torch(self, expr, symbols):\n",
    "        torch_funcs = {\n",
    "            sp.Add: lambda *args: reduce(torch.add, args),\n",
    "            sp.Mul: lambda *args: reduce(torch.mul, args),\n",
    "            sp.Pow: torch.pow,\n",
    "            sp.sin: torch.sin,\n",
    "            sp.cos: torch.cos,\n",
    "        }\n",
    "\n",
    "        def torch_func(*args):\n",
    "            def _eval(ex):\n",
    "                if isinstance(ex, sp.Symbol):\n",
    "                    return args[symbols.index(ex)]\n",
    "                elif isinstance(ex, sp.Number):\n",
    "                    return torch.full_like(args[0], float(ex))\n",
    "                elif isinstance(ex, sp.Expr):\n",
    "                    op = type(ex)\n",
    "                    if op in torch_funcs:\n",
    "                        return torch_funcs[op](*[_eval(arg) for arg in ex.args])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported operation: {op}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type: {type(ex)}\")\n",
    "            \n",
    "            return _eval(expr)\n",
    "\n",
    "        return torch_func\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        torch_func = self.sympy_to_torch(formula, symbols)\n",
    "        var_values = [params[:, j] for j in range(len(symbols)-1)] + [x.unsqueeze(1)]\n",
    "        results = torch_func(*var_values)\n",
    "        return results.swapaxes(0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.requires_grad_(True)\n",
    "        outs = inputs.unsqueeze(1).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        weights = F.softmax(-stacked_losses, dim=0)\n",
    "        best_out = torch.sum(weights.unsqueeze(2) * stacked_preds, dim=0)\n",
    "        best_loss = torch.sum(weights * stacked_losses, dim=0)        \n",
    "        best_func = weights.t()\n",
    "        best_params = torch.sum(weights.unsqueeze(2) * torch.stack(all_params), dim=0)\n",
    "        return best_out, best_loss, best_func, weights, best_params, outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1642122/2683534046.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_1642122/2683534046.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_1642122/2683534046.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)\n",
    "target_funcs = hold1['target_funcs'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n",
      "target_funcs: torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")\n",
    "print(f\"target_funcs: {target_funcs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values[0:2000, :], params[0:2000, :], target_funcs[:, 0:2])\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.71855370\n",
      "Avg Grad Norm: 0.0250, Avg Grad Mean: 0.0002, Avg Grad Std: 0.0025\n",
      "--- 0.8021547794342041 seconds ---\n",
      "epoch : 1/100, loss = 0.71435108\n",
      "Avg Grad Norm: 0.0221, Avg Grad Mean: 0.0002, Avg Grad Std: 0.0021\n",
      "--- 0.6967723369598389 seconds ---\n",
      "epoch : 2/100, loss = 0.71059450\n",
      "Avg Grad Norm: 0.0204, Avg Grad Mean: 0.0002, Avg Grad Std: 0.0019\n",
      "--- 0.8167510032653809 seconds ---\n",
      "epoch : 3/100, loss = 0.70701568\n",
      "Avg Grad Norm: 0.0197, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0018\n",
      "--- 0.7464878559112549 seconds ---\n",
      "epoch : 4/100, loss = 0.70340447\n",
      "Avg Grad Norm: 0.0196, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0017\n",
      "--- 0.6845340728759766 seconds ---\n",
      "epoch : 5/100, loss = 0.69983290\n",
      "Avg Grad Norm: 0.0192, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0017\n",
      "--- 0.7653887271881104 seconds ---\n",
      "epoch : 6/100, loss = 0.69629221\n",
      "Avg Grad Norm: 0.0186, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0016\n",
      "--- 0.8038280010223389 seconds ---\n",
      "epoch : 7/100, loss = 0.69297749\n",
      "Avg Grad Norm: 0.0177, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0015\n",
      "--- 0.8364949226379395 seconds ---\n",
      "epoch : 8/100, loss = 0.68982132\n",
      "Avg Grad Norm: 0.0167, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0015\n",
      "--- 0.6110231876373291 seconds ---\n",
      "epoch : 9/100, loss = 0.68702416\n",
      "Avg Grad Norm: 0.0153, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0013\n",
      "--- 0.8750503063201904 seconds ---\n",
      "epoch : 10/100, loss = 0.68552841\n",
      "Avg Grad Norm: 0.0146, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0013\n",
      "--- 0.8594577312469482 seconds ---\n",
      "epoch : 11/100, loss = 0.68528948\n",
      "Avg Grad Norm: 0.0144, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0013\n",
      "--- 0.8048410415649414 seconds ---\n",
      "epoch : 12/100, loss = 0.68505482\n",
      "Avg Grad Norm: 0.0143, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0013\n",
      "--- 0.6094679832458496 seconds ---\n",
      "epoch : 13/100, loss = 0.68482133\n",
      "Avg Grad Norm: 0.0142, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.651430606842041 seconds ---\n",
      "epoch : 14/100, loss = 0.68459170\n",
      "Avg Grad Norm: 0.0140, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6825649738311768 seconds ---\n",
      "epoch : 15/100, loss = 0.68436271\n",
      "Avg Grad Norm: 0.0139, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6421999931335449 seconds ---\n",
      "epoch : 16/100, loss = 0.68413972\n",
      "Avg Grad Norm: 0.0138, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6968405246734619 seconds ---\n",
      "epoch : 17/100, loss = 0.68391609\n",
      "Avg Grad Norm: 0.0135, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.5785598754882812 seconds ---\n",
      "epoch : 18/100, loss = 0.68369500\n",
      "Avg Grad Norm: 0.0134, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.681394100189209 seconds ---\n",
      "epoch : 19/100, loss = 0.68347845\n",
      "Avg Grad Norm: 0.0133, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7414252758026123 seconds ---\n",
      "epoch : 20/100, loss = 0.68335283\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6598465442657471 seconds ---\n",
      "epoch : 21/100, loss = 0.68333124\n",
      "Avg Grad Norm: 0.0133, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6471095085144043 seconds ---\n",
      "epoch : 22/100, loss = 0.68330967\n",
      "Avg Grad Norm: 0.0133, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6857123374938965 seconds ---\n",
      "epoch : 23/100, loss = 0.68328800\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.716925859451294 seconds ---\n",
      "epoch : 24/100, loss = 0.68326634\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7283587455749512 seconds ---\n",
      "epoch : 25/100, loss = 0.68324454\n",
      "Avg Grad Norm: 0.0133, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6129631996154785 seconds ---\n",
      "epoch : 26/100, loss = 0.68322282\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7095456123352051 seconds ---\n",
      "epoch : 27/100, loss = 0.68320090\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.8352608680725098 seconds ---\n",
      "epoch : 28/100, loss = 0.68317910\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7620887756347656 seconds ---\n",
      "epoch : 29/100, loss = 0.68315721\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6620299816131592 seconds ---\n",
      "epoch : 30/100, loss = 0.68314463\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7791769504547119 seconds ---\n",
      "epoch : 31/100, loss = 0.68314246\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7751176357269287 seconds ---\n",
      "epoch : 32/100, loss = 0.68314032\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.82328200340271 seconds ---\n",
      "epoch : 33/100, loss = 0.68313815\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7984011173248291 seconds ---\n",
      "epoch : 34/100, loss = 0.68313599\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6939518451690674 seconds ---\n",
      "epoch : 35/100, loss = 0.68313383\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6135320663452148 seconds ---\n",
      "epoch : 36/100, loss = 0.68313164\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7400579452514648 seconds ---\n",
      "epoch : 37/100, loss = 0.68312945\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6632626056671143 seconds ---\n",
      "epoch : 38/100, loss = 0.68312726\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7807209491729736 seconds ---\n",
      "epoch : 39/100, loss = 0.68312507\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7206413745880127 seconds ---\n",
      "epoch : 40/100, loss = 0.68312386\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.5846972465515137 seconds ---\n",
      "epoch : 41/100, loss = 0.68312380\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6658823490142822 seconds ---\n",
      "epoch : 42/100, loss = 0.68312372\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7100424766540527 seconds ---\n",
      "epoch : 43/100, loss = 0.68312364\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6119585037231445 seconds ---\n",
      "epoch : 44/100, loss = 0.68312359\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6702368259429932 seconds ---\n",
      "epoch : 45/100, loss = 0.68312351\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6343531608581543 seconds ---\n",
      "epoch : 46/100, loss = 0.68312343\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7537362575531006 seconds ---\n",
      "epoch : 47/100, loss = 0.68312335\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6886377334594727 seconds ---\n",
      "epoch : 48/100, loss = 0.68312328\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.8388857841491699 seconds ---\n",
      "epoch : 49/100, loss = 0.68312319\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.8202333450317383 seconds ---\n",
      "epoch : 50/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6774008274078369 seconds ---\n",
      "epoch : 51/100, loss = 0.68312314\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6474850177764893 seconds ---\n",
      "epoch : 52/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6594858169555664 seconds ---\n",
      "epoch : 53/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7380654811859131 seconds ---\n",
      "epoch : 54/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7309095859527588 seconds ---\n",
      "epoch : 55/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7880184650421143 seconds ---\n",
      "epoch : 56/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6677007675170898 seconds ---\n",
      "epoch : 57/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.5907409191131592 seconds ---\n",
      "epoch : 58/100, loss = 0.68312314\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6613833904266357 seconds ---\n",
      "epoch : 59/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7275645732879639 seconds ---\n",
      "epoch : 60/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6939594745635986 seconds ---\n",
      "epoch : 61/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6775975227355957 seconds ---\n",
      "epoch : 62/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7034564018249512 seconds ---\n",
      "epoch : 63/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6727299690246582 seconds ---\n",
      "epoch : 64/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7200031280517578 seconds ---\n",
      "epoch : 65/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6186928749084473 seconds ---\n",
      "epoch : 66/100, loss = 0.68312314\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6469037532806396 seconds ---\n",
      "epoch : 67/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6810188293457031 seconds ---\n",
      "epoch : 68/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6745831966400146 seconds ---\n",
      "epoch : 69/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6329488754272461 seconds ---\n",
      "epoch : 70/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6947057247161865 seconds ---\n",
      "epoch : 71/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7205548286437988 seconds ---\n",
      "epoch : 72/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7878003120422363 seconds ---\n",
      "epoch : 73/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7067499160766602 seconds ---\n",
      "epoch : 74/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7882206439971924 seconds ---\n",
      "epoch : 75/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7843618392944336 seconds ---\n",
      "epoch : 76/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7987487316131592 seconds ---\n",
      "epoch : 77/100, loss = 0.68312313\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.8525917530059814 seconds ---\n",
      "epoch : 78/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7226295471191406 seconds ---\n",
      "epoch : 79/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.574350118637085 seconds ---\n",
      "epoch : 80/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6866500377655029 seconds ---\n",
      "epoch : 81/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.6819124221801758 seconds ---\n",
      "epoch : 82/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7821147441864014 seconds ---\n",
      "epoch : 83/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7449696063995361 seconds ---\n",
      "epoch : 84/100, loss = 0.68312314\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6977086067199707 seconds ---\n",
      "epoch : 85/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.8123366832733154 seconds ---\n",
      "epoch : 86/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0132, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.6070854663848877 seconds ---\n",
      "epoch : 87/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.71868896484375 seconds ---\n",
      "epoch : 88/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.8401212692260742 seconds ---\n",
      "epoch : 89/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.631596565246582 seconds ---\n",
      "epoch : 90/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.8027551174163818 seconds ---\n",
      "epoch : 91/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7385096549987793 seconds ---\n",
      "epoch : 92/100, loss = 0.68312317\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.5880091190338135 seconds ---\n",
      "epoch : 93/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7278404235839844 seconds ---\n",
      "epoch : 94/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7161715030670166 seconds ---\n",
      "epoch : 95/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.8264410495758057 seconds ---\n",
      "epoch : 96/100, loss = 0.68312316\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7610964775085449 seconds ---\n",
      "epoch : 97/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7984006404876709 seconds ---\n",
      "epoch : 98/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0130, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0011\n",
      "--- 0.7832608222961426 seconds ---\n",
      "epoch : 99/100, loss = 0.68312315\n",
      "Avg Grad Norm: 0.0131, Avg Grad Mean: 0.0003, Avg Grad Std: 0.0012\n",
      "--- 0.7470507621765137 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    loss_class = nn.BCEWithLogitsLoss()\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = loss_class(output_function.swapaxes(0,1), target_function)#torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss #params_loss*lam + y_loss*(1-lam)\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    " \n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0891, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8527281284332275 seconds ---\n",
      "epoch : 1/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0898, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7678751945495605 seconds ---\n",
      "epoch : 2/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.6781072616577148 seconds ---\n",
      "epoch : 3/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0904, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0081\n",
      "--- 0.7393097877502441 seconds ---\n",
      "epoch : 4/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0900, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.656632661819458 seconds ---\n",
      "epoch : 5/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8025989532470703 seconds ---\n",
      "epoch : 6/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7750968933105469 seconds ---\n",
      "epoch : 7/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.8149797916412354 seconds ---\n",
      "epoch : 8/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.7998840808868408 seconds ---\n",
      "epoch : 9/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.6425008773803711 seconds ---\n",
      "epoch : 10/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8432254791259766 seconds ---\n",
      "epoch : 11/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0903, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0081\n",
      "--- 0.7511966228485107 seconds ---\n",
      "epoch : 12/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6750190258026123 seconds ---\n",
      "epoch : 13/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.7250690460205078 seconds ---\n",
      "epoch : 14/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.82135009765625 seconds ---\n",
      "epoch : 15/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7674059867858887 seconds ---\n",
      "epoch : 16/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.7165887355804443 seconds ---\n",
      "epoch : 17/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0900, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.7500405311584473 seconds ---\n",
      "epoch : 18/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.9048664569854736 seconds ---\n",
      "epoch : 19/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.8668262958526611 seconds ---\n",
      "epoch : 20/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.9465410709381104 seconds ---\n",
      "epoch : 21/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0904, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0081\n",
      "--- 0.8145573139190674 seconds ---\n",
      "epoch : 22/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.875791072845459 seconds ---\n",
      "epoch : 23/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6808927059173584 seconds ---\n",
      "epoch : 24/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.6712548732757568 seconds ---\n",
      "epoch : 25/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8392527103424072 seconds ---\n",
      "epoch : 26/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0903, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0081\n",
      "--- 0.7222790718078613 seconds ---\n",
      "epoch : 27/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7526659965515137 seconds ---\n",
      "epoch : 28/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7207632064819336 seconds ---\n",
      "epoch : 29/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7042171955108643 seconds ---\n",
      "epoch : 30/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.7229530811309814 seconds ---\n",
      "epoch : 31/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7205016613006592 seconds ---\n",
      "epoch : 32/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.9261350631713867 seconds ---\n",
      "epoch : 33/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0901, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.7260475158691406 seconds ---\n",
      "epoch : 34/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.8854866027832031 seconds ---\n",
      "epoch : 35/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7898416519165039 seconds ---\n",
      "epoch : 36/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.7981846332550049 seconds ---\n",
      "epoch : 37/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0906, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0082\n",
      "--- 0.8185808658599854 seconds ---\n",
      "epoch : 38/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.6420915126800537 seconds ---\n",
      "epoch : 39/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.9011998176574707 seconds ---\n",
      "epoch : 40/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0900, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.9042026996612549 seconds ---\n",
      "epoch : 41/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.847883939743042 seconds ---\n",
      "epoch : 42/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.9028744697570801 seconds ---\n",
      "epoch : 43/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0891, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.8203485012054443 seconds ---\n",
      "epoch : 44/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.8303623199462891 seconds ---\n",
      "epoch : 45/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.831831693649292 seconds ---\n",
      "epoch : 46/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8504281044006348 seconds ---\n",
      "epoch : 47/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0898, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7005889415740967 seconds ---\n",
      "epoch : 48/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8423688411712646 seconds ---\n",
      "epoch : 49/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.764336109161377 seconds ---\n",
      "epoch : 50/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7321634292602539 seconds ---\n",
      "epoch : 51/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.6476616859436035 seconds ---\n",
      "epoch : 52/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7623875141143799 seconds ---\n",
      "epoch : 53/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.8298461437225342 seconds ---\n",
      "epoch : 54/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.6897492408752441 seconds ---\n",
      "epoch : 55/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0901, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.6518480777740479 seconds ---\n",
      "epoch : 56/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0902, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.9345812797546387 seconds ---\n",
      "epoch : 57/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0900, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7872540950775146 seconds ---\n",
      "epoch : 58/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7874636650085449 seconds ---\n",
      "epoch : 59/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0900, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.8570492267608643 seconds ---\n",
      "epoch : 60/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0898, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7850394248962402 seconds ---\n",
      "epoch : 61/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.6571671962738037 seconds ---\n",
      "epoch : 62/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0901, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.8600773811340332 seconds ---\n",
      "epoch : 63/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.8862910270690918 seconds ---\n",
      "epoch : 64/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8109831809997559 seconds ---\n",
      "epoch : 65/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0903, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0081\n",
      "--- 0.7867605686187744 seconds ---\n",
      "epoch : 66/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 1.3182942867279053 seconds ---\n",
      "epoch : 67/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6804137229919434 seconds ---\n",
      "epoch : 68/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.9509372711181641 seconds ---\n",
      "epoch : 69/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0901, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.8973238468170166 seconds ---\n",
      "epoch : 70/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8614821434020996 seconds ---\n",
      "epoch : 71/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7949204444885254 seconds ---\n",
      "epoch : 72/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.7799952030181885 seconds ---\n",
      "epoch : 73/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0898, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7967042922973633 seconds ---\n",
      "epoch : 74/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.9480023384094238 seconds ---\n",
      "epoch : 75/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8547239303588867 seconds ---\n",
      "epoch : 76/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.829038143157959 seconds ---\n",
      "epoch : 77/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.961946964263916 seconds ---\n",
      "epoch : 78/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0897, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.6980054378509521 seconds ---\n",
      "epoch : 79/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6286141872406006 seconds ---\n",
      "epoch : 80/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0892, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.6674678325653076 seconds ---\n",
      "epoch : 81/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.6799979209899902 seconds ---\n",
      "epoch : 82/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6637611389160156 seconds ---\n",
      "epoch : 83/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7193248271942139 seconds ---\n",
      "epoch : 84/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0899, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7190523147583008 seconds ---\n",
      "epoch : 85/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0893, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0077\n",
      "--- 0.6635909080505371 seconds ---\n",
      "epoch : 86/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0891, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7244095802307129 seconds ---\n",
      "epoch : 87/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0894, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7597253322601318 seconds ---\n",
      "epoch : 88/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8073291778564453 seconds ---\n",
      "epoch : 89/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.762589693069458 seconds ---\n",
      "epoch : 90/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7776796817779541 seconds ---\n",
      "epoch : 91/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0896, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0079\n",
      "--- 0.7125575542449951 seconds ---\n",
      "epoch : 92/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6478219032287598 seconds ---\n",
      "epoch : 93/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.6457898616790771 seconds ---\n",
      "epoch : 94/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0901, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.7523515224456787 seconds ---\n",
      "epoch : 95/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.7327373027801514 seconds ---\n",
      "epoch : 96/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0902, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0081\n",
      "--- 0.6982254981994629 seconds ---\n",
      "epoch : 97/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0895, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0078\n",
      "--- 0.8456485271453857 seconds ---\n",
      "epoch : 98/100, loss = 0.28419583\n",
      "Avg Grad Norm: 0.0901, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0080\n",
      "--- 0.8097591400146484 seconds ---\n",
      "epoch : 99/100, loss = 0.28419582\n",
      "Avg Grad Norm: 0.0906, Avg Grad Mean: 0.0007, Avg Grad Std: 0.0082\n",
      "--- 0.8272275924682617 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        if check_nan(inputs, \"inputs\") or check_nan(true_params, \"true_params\") or check_nan(true_func, \"true_func\"):\n",
    "            continue\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        if check_nan(best_out, \"best_out\") or check_nan(best_index, \"best_index\") or check_nan(best_params, \"best_params\"):\n",
    "            continue\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        if check_nan(loss, \"loss\"):\n",
    "            continue\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if(param.grad is None):\n",
    "                print(f\"None detected in {name}\")\n",
    "            if check_nan(param.grad, f\"gradient of {name}\"):\n",
    "                continue\n",
    "        \n",
    "        '''for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: grad norm = {param.grad.norm()}, grad std = {param.grad.std()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")'''\n",
    "\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    " \n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: tensor([0.5043, 0.4957], device='cuda:3', grad_fn=<SelectBackward0>)\n",
      "true_func: tensor([1., 0.], device='cuda:3')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb9b8210490>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "ran = np.random.randint(0, 10)\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values[0:2000])\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"true_func: {target_funcs[ran, 0:2]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
