{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        var_values = {str(symbols[j]): params[:, j] for j in range(len(symbols)-1)}\n",
    "        eval_func = sp.lambdify(symbols, formula, modules=\"numpy\")\n",
    "        var_values[str(symbols[-1])] = x.unsqueeze(1)\n",
    "        results = eval_func(**var_values)\n",
    "        results = torch.nan_to_num(results, 0)\n",
    "        return results.swapaxes(0,1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outs = inputs.unsqueeze(1).to(self.device)  # Add channel dimension\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0)\n",
    "        best_out = stacked_preds[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "        best_params = []\n",
    "        for index, value in enumerate(best_indexes):\n",
    "            best_params.append(all_params[value][index])\n",
    "        best_indexes = torch.tensor(best_indexes, dtype=torch.float32, requires_grad=True)\n",
    "        return best_out, best_loss, best_func, best_indexes, torch.stack(best_params), outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137582/764187205.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_1137582/764187205.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_1137582/764187205.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values[0:2000, :], params[0:2000, :], function_labels[0:2000])\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137582/1572267915.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  best_indexes = torch.tensor(best_indexes, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.13188457489013672 seconds ---\n",
      "epoch : 1/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12717580795288086 seconds ---\n",
      "epoch : 2/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12565016746520996 seconds ---\n",
      "epoch : 3/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12527060508728027 seconds ---\n",
      "epoch : 4/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12601518630981445 seconds ---\n",
      "epoch : 5/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12572550773620605 seconds ---\n",
      "epoch : 6/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12636184692382812 seconds ---\n",
      "epoch : 7/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1259596347808838 seconds ---\n",
      "epoch : 8/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12607765197753906 seconds ---\n",
      "epoch : 9/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1257016658782959 seconds ---\n",
      "epoch : 10/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12597012519836426 seconds ---\n",
      "epoch : 11/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12651562690734863 seconds ---\n",
      "epoch : 12/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.13021564483642578 seconds ---\n",
      "epoch : 13/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12623214721679688 seconds ---\n",
      "epoch : 14/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12596702575683594 seconds ---\n",
      "epoch : 15/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12646102905273438 seconds ---\n",
      "epoch : 16/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12680697441101074 seconds ---\n",
      "epoch : 17/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12545251846313477 seconds ---\n",
      "epoch : 18/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12556695938110352 seconds ---\n",
      "epoch : 19/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12573599815368652 seconds ---\n",
      "epoch : 20/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1262218952178955 seconds ---\n",
      "epoch : 21/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12596940994262695 seconds ---\n",
      "epoch : 22/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12699651718139648 seconds ---\n",
      "epoch : 23/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1270895004272461 seconds ---\n",
      "epoch : 24/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12699580192565918 seconds ---\n",
      "epoch : 25/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1263427734375 seconds ---\n",
      "epoch : 26/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1270754337310791 seconds ---\n",
      "epoch : 27/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12716364860534668 seconds ---\n",
      "epoch : 28/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12532615661621094 seconds ---\n",
      "epoch : 29/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12587428092956543 seconds ---\n",
      "epoch : 30/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12614703178405762 seconds ---\n",
      "epoch : 31/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12467002868652344 seconds ---\n",
      "epoch : 32/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12562870979309082 seconds ---\n",
      "epoch : 33/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12589120864868164 seconds ---\n",
      "epoch : 34/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12641453742980957 seconds ---\n",
      "epoch : 35/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12575316429138184 seconds ---\n",
      "epoch : 36/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12852716445922852 seconds ---\n",
      "epoch : 37/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12636613845825195 seconds ---\n",
      "epoch : 38/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12666702270507812 seconds ---\n",
      "epoch : 39/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1260695457458496 seconds ---\n",
      "epoch : 40/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1263563632965088 seconds ---\n",
      "epoch : 41/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1253664493560791 seconds ---\n",
      "epoch : 42/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1262514591217041 seconds ---\n",
      "epoch : 43/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12604260444641113 seconds ---\n",
      "epoch : 44/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12595486640930176 seconds ---\n",
      "epoch : 45/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12623834609985352 seconds ---\n",
      "epoch : 46/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12708806991577148 seconds ---\n",
      "epoch : 47/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12665414810180664 seconds ---\n",
      "epoch : 48/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12672829627990723 seconds ---\n",
      "epoch : 49/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1286334991455078 seconds ---\n",
      "epoch : 50/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12709617614746094 seconds ---\n",
      "epoch : 51/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12630438804626465 seconds ---\n",
      "epoch : 52/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.126875638961792 seconds ---\n",
      "epoch : 53/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12645363807678223 seconds ---\n",
      "epoch : 54/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12696051597595215 seconds ---\n",
      "epoch : 55/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.12667441368103027 seconds ---\n",
      "epoch : 56/100, loss = 0.40749999\n",
      "Avg Grad Norm: nan, Avg Grad Mean: nan, Avg Grad Std: nan\n",
      "--- 0.1277914047241211 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[251], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m true_params \u001b[38;5;241m=\u001b[39m true_params\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m true_func \u001b[38;5;241m=\u001b[39m true_func\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 52\u001b[0m best_out,_,best_func,best_index,best_params,_,_,_\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m lam_val \u001b[38;5;241m=\u001b[39m lambda_scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(best_out, inputs, best_params, true_params, best_index\u001b[38;5;241m.\u001b[39mfloat(), true_func\u001b[38;5;241m.\u001b[39mfloat(), lam_val)\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[247], line 79\u001b[0m, in \u001b[0;36mMulti_Func.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     78\u001b[0m     outs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_x1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     xfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(outs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     81\u001b[0m     xfc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_xfc(xfc)\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1556\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1556\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Function to compute gradient statistics\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute and log gradient statistics\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        # Log gradients to TensorBoard every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    writer.add_histogram(f'gradients/{name}', param.grad, epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    " \n",
    "    # Compute average gradient statistics for the epoch\n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Gradients/norm', avg_grad_norm, epoch)\n",
    "    writer.add_scalar('Gradients/mean', avg_grad_mean, epoch)\n",
    "    writer.add_scalar('Gradients/std', avg_grad_std, epoch)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137582/182172253.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  best_indexes = torch.tensor(best_indexes, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.31649999\n",
      "--- 0.1766524314880371 seconds ---\n",
      "epoch : 1/100, loss = 0.31649999\n",
      "--- 0.1309223175048828 seconds ---\n",
      "epoch : 2/100, loss = 0.31649999\n",
      "--- 0.13064241409301758 seconds ---\n",
      "epoch : 3/100, loss = 0.31649999\n",
      "--- 0.130615234375 seconds ---\n",
      "epoch : 4/100, loss = 0.31649999\n",
      "--- 0.13306736946105957 seconds ---\n",
      "epoch : 5/100, loss = 0.31649999\n",
      "--- 0.13086271286010742 seconds ---\n",
      "epoch : 6/100, loss = 0.31649999\n",
      "--- 0.1311173439025879 seconds ---\n",
      "epoch : 7/100, loss = 0.31649999\n",
      "--- 0.13053512573242188 seconds ---\n",
      "epoch : 8/100, loss = 0.31649999\n",
      "--- 0.13073253631591797 seconds ---\n",
      "epoch : 9/100, loss = 0.31649999\n",
      "--- 0.1305248737335205 seconds ---\n",
      "epoch : 10/100, loss = 0.31649999\n",
      "--- 0.1305999755859375 seconds ---\n",
      "epoch : 11/100, loss = 0.31649999\n",
      "--- 0.13042736053466797 seconds ---\n",
      "epoch : 12/100, loss = 0.31649999\n",
      "--- 0.1331171989440918 seconds ---\n",
      "epoch : 13/100, loss = 0.31649999\n",
      "--- 0.13069987297058105 seconds ---\n",
      "epoch : 14/100, loss = 0.31649999\n",
      "--- 0.13083815574645996 seconds ---\n",
      "epoch : 15/100, loss = 0.31649999\n",
      "--- 0.13071084022521973 seconds ---\n",
      "epoch : 16/100, loss = 0.31649999\n",
      "--- 0.13096094131469727 seconds ---\n",
      "epoch : 17/100, loss = 0.31649999\n",
      "--- 0.13074302673339844 seconds ---\n",
      "epoch : 18/100, loss = 0.31649999\n",
      "--- 0.13081097602844238 seconds ---\n",
      "epoch : 19/100, loss = 0.31650000\n",
      "--- 0.13070344924926758 seconds ---\n",
      "epoch : 20/100, loss = 0.31649999\n",
      "--- 0.1305985450744629 seconds ---\n",
      "epoch : 21/100, loss = 0.31649999\n",
      "--- 0.1327042579650879 seconds ---\n",
      "epoch : 22/100, loss = 0.31649999\n",
      "--- 0.13103437423706055 seconds ---\n",
      "epoch : 23/100, loss = 0.31649999\n",
      "--- 0.13060665130615234 seconds ---\n",
      "epoch : 24/100, loss = 0.31649999\n",
      "--- 0.13081717491149902 seconds ---\n",
      "epoch : 25/100, loss = 0.31649999\n",
      "--- 0.13053345680236816 seconds ---\n",
      "epoch : 26/100, loss = 0.31649999\n",
      "--- 0.13057780265808105 seconds ---\n",
      "epoch : 27/100, loss = 0.31649999\n",
      "--- 0.1304335594177246 seconds ---\n",
      "epoch : 28/100, loss = 0.31649999\n",
      "--- 0.1307222843170166 seconds ---\n",
      "epoch : 29/100, loss = 0.31649999\n",
      "--- 0.1316075325012207 seconds ---\n",
      "epoch : 30/100, loss = 0.31649999\n",
      "--- 0.130659818649292 seconds ---\n",
      "epoch : 31/100, loss = 0.31649999\n",
      "--- 0.13101935386657715 seconds ---\n",
      "epoch : 32/100, loss = 0.31649999\n",
      "--- 0.1308891773223877 seconds ---\n",
      "epoch : 33/100, loss = 0.31649999\n",
      "--- 0.13057327270507812 seconds ---\n",
      "epoch : 34/100, loss = 0.31649999\n",
      "--- 0.13056111335754395 seconds ---\n",
      "epoch : 35/100, loss = 0.31649999\n",
      "--- 0.1309494972229004 seconds ---\n",
      "epoch : 36/100, loss = 0.31649999\n",
      "--- 0.1306157112121582 seconds ---\n",
      "epoch : 37/100, loss = 0.31649999\n",
      "--- 0.13081860542297363 seconds ---\n",
      "epoch : 38/100, loss = 0.31649999\n",
      "--- 0.13215184211730957 seconds ---\n",
      "epoch : 39/100, loss = 0.31649999\n",
      "--- 0.13072443008422852 seconds ---\n",
      "epoch : 40/100, loss = 0.31649999\n",
      "--- 0.1312863826751709 seconds ---\n",
      "epoch : 41/100, loss = 0.31649999\n",
      "--- 0.1303246021270752 seconds ---\n",
      "epoch : 42/100, loss = 0.31649999\n",
      "--- 0.13068008422851562 seconds ---\n",
      "epoch : 43/100, loss = 0.31649999\n",
      "--- 0.13045144081115723 seconds ---\n",
      "epoch : 44/100, loss = 0.31649999\n",
      "--- 0.1310417652130127 seconds ---\n",
      "epoch : 45/100, loss = 0.31649999\n",
      "--- 0.1306288242340088 seconds ---\n",
      "epoch : 46/100, loss = 0.31649999\n",
      "--- 0.13291430473327637 seconds ---\n",
      "epoch : 47/100, loss = 0.31650000\n",
      "--- 0.13050341606140137 seconds ---\n",
      "epoch : 48/100, loss = 0.31649999\n",
      "--- 0.1305253505706787 seconds ---\n",
      "epoch : 49/100, loss = 0.31650000\n",
      "--- 0.13025403022766113 seconds ---\n",
      "epoch : 50/100, loss = 0.31649999\n",
      "--- 0.1305522918701172 seconds ---\n",
      "epoch : 51/100, loss = 0.31649999\n",
      "--- 0.13028430938720703 seconds ---\n",
      "epoch : 52/100, loss = 0.31649999\n",
      "--- 0.1310412883758545 seconds ---\n",
      "epoch : 53/100, loss = 0.31649999\n",
      "--- 0.13139724731445312 seconds ---\n",
      "epoch : 54/100, loss = 0.31649999\n",
      "--- 0.1305103302001953 seconds ---\n",
      "epoch : 55/100, loss = 0.31649999\n",
      "--- 0.13022804260253906 seconds ---\n",
      "epoch : 56/100, loss = 0.31649999\n",
      "--- 0.13213729858398438 seconds ---\n",
      "epoch : 57/100, loss = 0.31649999\n",
      "--- 0.1303858757019043 seconds ---\n",
      "epoch : 58/100, loss = 0.31649999\n",
      "--- 0.13059186935424805 seconds ---\n",
      "epoch : 59/100, loss = 0.31650000\n",
      "--- 0.1305701732635498 seconds ---\n",
      "epoch : 60/100, loss = 0.31649999\n",
      "--- 0.13086700439453125 seconds ---\n",
      "epoch : 61/100, loss = 0.31649999\n",
      "--- 0.13031864166259766 seconds ---\n",
      "epoch : 62/100, loss = 0.31649999\n",
      "--- 0.13055706024169922 seconds ---\n",
      "epoch : 63/100, loss = 0.31649999\n",
      "--- 0.13066554069519043 seconds ---\n",
      "epoch : 64/100, loss = 0.31649999\n",
      "--- 0.13054728507995605 seconds ---\n",
      "epoch : 65/100, loss = 0.31649999\n",
      "--- 0.13042259216308594 seconds ---\n",
      "epoch : 66/100, loss = 0.31649999\n",
      "--- 0.13277578353881836 seconds ---\n",
      "epoch : 67/100, loss = 0.31649999\n",
      "--- 0.13048028945922852 seconds ---\n",
      "epoch : 68/100, loss = 0.31649999\n",
      "--- 0.13069677352905273 seconds ---\n",
      "epoch : 69/100, loss = 0.31649999\n",
      "--- 0.13023710250854492 seconds ---\n",
      "epoch : 70/100, loss = 0.31649999\n",
      "--- 0.13062143325805664 seconds ---\n",
      "epoch : 71/100, loss = 0.31649999\n",
      "--- 0.13083457946777344 seconds ---\n",
      "epoch : 72/100, loss = 0.31649999\n",
      "--- 0.13061952590942383 seconds ---\n",
      "epoch : 73/100, loss = 0.31649999\n",
      "--- 0.13042807579040527 seconds ---\n",
      "epoch : 74/100, loss = 0.31649999\n",
      "--- 0.13066816329956055 seconds ---\n",
      "epoch : 75/100, loss = 0.31649999\n",
      "--- 0.1304323673248291 seconds ---\n",
      "epoch : 76/100, loss = 0.31649999\n",
      "--- 0.1314527988433838 seconds ---\n",
      "epoch : 77/100, loss = 0.31649999\n",
      "--- 0.1304159164428711 seconds ---\n",
      "epoch : 78/100, loss = 0.31649999\n",
      "--- 0.13069844245910645 seconds ---\n",
      "epoch : 79/100, loss = 0.31649999\n",
      "--- 0.13054132461547852 seconds ---\n",
      "epoch : 80/100, loss = 0.31649999\n",
      "--- 0.13072991371154785 seconds ---\n",
      "epoch : 81/100, loss = 0.31650000\n",
      "--- 0.13035297393798828 seconds ---\n",
      "epoch : 82/100, loss = 0.31649999\n",
      "--- 0.13052630424499512 seconds ---\n",
      "epoch : 83/100, loss = 0.31649999\n",
      "--- 0.13027524948120117 seconds ---\n",
      "epoch : 84/100, loss = 0.31649999\n",
      "--- 0.13066339492797852 seconds ---\n",
      "epoch : 85/100, loss = 0.31649999\n",
      "--- 0.13039779663085938 seconds ---\n",
      "epoch : 86/100, loss = 0.31649999\n",
      "--- 0.13201189041137695 seconds ---\n",
      "epoch : 87/100, loss = 0.31649999\n",
      "--- 0.13043832778930664 seconds ---\n",
      "epoch : 88/100, loss = 0.31649999\n",
      "--- 0.13071775436401367 seconds ---\n",
      "epoch : 89/100, loss = 0.31649999\n",
      "--- 0.1305100917816162 seconds ---\n",
      "epoch : 90/100, loss = 0.31649999\n",
      "--- 0.13068771362304688 seconds ---\n",
      "epoch : 91/100, loss = 0.31649999\n",
      "--- 0.13059544563293457 seconds ---\n",
      "epoch : 92/100, loss = 0.31649999\n",
      "--- 0.13066577911376953 seconds ---\n",
      "epoch : 93/100, loss = 0.31649999\n",
      "--- 0.1304018497467041 seconds ---\n",
      "epoch : 94/100, loss = 0.31649999\n",
      "--- 0.13051533699035645 seconds ---\n",
      "epoch : 95/100, loss = 0.31649999\n",
      "--- 0.1302812099456787 seconds ---\n",
      "epoch : 96/100, loss = 0.31649999\n",
      "--- 0.13255715370178223 seconds ---\n",
      "epoch : 97/100, loss = 0.31649999\n",
      "--- 0.13053345680236816 seconds ---\n",
      "epoch : 98/100, loss = 0.31649999\n",
      "--- 0.13080811500549316 seconds ---\n",
      "epoch : 99/100, loss = 0.31649999\n",
      "--- 0.1306912899017334 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    optimizer.zero_grad()\n",
    "    for inputs, true_params, true_func in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: k*x\n",
      "best_loss: 0.6276634335517883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137582/182172253.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  best_indexes = torch.tensor(best_indexes, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fadb8fa3ed0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "ran = np.random.randint(0, 100)\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values[0:2000])\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"best_loss: {best_loss[ran]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
