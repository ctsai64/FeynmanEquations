{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.selu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device).requires_grad_(True)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([8, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(8, 8),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([6, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(6, 6),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.LayerNorm(20),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(4, 4),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(4, 4),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),           \n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(4, 4),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(4, 4),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(4, 4),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock1D(4, 4),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def sympy_to_torch(self, expr, symbols):\n",
    "        torch_funcs = {\n",
    "            sp.Add: lambda *args: reduce(torch.add, args),\n",
    "            sp.Mul: lambda *args: reduce(torch.mul, args),\n",
    "            sp.Pow: torch.pow,\n",
    "            sp.sin: torch.sin,\n",
    "            sp.cos: torch.cos,\n",
    "        }\n",
    "\n",
    "        def torch_func(*args):\n",
    "            def _eval(ex):\n",
    "                if isinstance(ex, sp.Symbol):\n",
    "                    return args[symbols.index(ex)]\n",
    "                elif isinstance(ex, sp.Number):\n",
    "                    return torch.full_like(args[0], float(ex))\n",
    "                elif isinstance(ex, sp.Expr):\n",
    "                    op = type(ex)\n",
    "                    if op in torch_funcs:\n",
    "                        return torch_funcs[op](*[_eval(arg) for arg in ex.args])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported operation: {op}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type: {type(ex)}\")\n",
    "            \n",
    "            return _eval(expr)\n",
    "\n",
    "        return torch_func\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        torch_func = self.sympy_to_torch(formula, symbols)\n",
    "        var_values = [params[:, j] for j in range(len(symbols)-1)] + [x.unsqueeze(1)]\n",
    "        results = torch_func(*var_values)\n",
    "        return results.swapaxes(0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.requires_grad_(True)\n",
    "        outs = inputs.unsqueeze(1).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        \n",
    "        weights = F.softmax(-stacked_losses, dim=0)\n",
    "        best_out = torch.sum(weights.unsqueeze(2) * stacked_preds, dim=0)\n",
    "        best_loss = torch.sum(weights * stacked_losses, dim=0)        \n",
    "        best_func = list(zip(self.functions, weights.t()))\n",
    "        best_params = torch.sum(weights.unsqueeze(2) * torch.stack(all_params), dim=0)\n",
    "        \n",
    "        return best_out, best_loss, best_func, weights, best_params, outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1642122/2683534046.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_1642122/2683534046.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_1642122/2683534046.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)\n",
    "target_funcs = hold1['target_funcs'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n",
      "target_funcs: torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")\n",
    "print(f\"target_funcs: {target_funcs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values[0:2000, :], params[0:2000, :], target_funcs[:, 0:2])\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.71591435\n",
      "Avg Grad Norm: 0.0053, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.9816007614135742 seconds ---\n",
      "epoch : 1/100, loss = 0.71398858\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6431503295898438 seconds ---\n",
      "epoch : 2/100, loss = 0.71229111\n",
      "Avg Grad Norm: 0.0049, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.3554573059082031 seconds ---\n",
      "epoch : 3/100, loss = 0.71038183\n",
      "Avg Grad Norm: 0.0050, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.6355667114257812 seconds ---\n",
      "epoch : 4/100, loss = 0.70845017\n",
      "Avg Grad Norm: 0.0050, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.8868188858032227 seconds ---\n",
      "epoch : 5/100, loss = 0.70651675\n",
      "Avg Grad Norm: 0.0048, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.7577786445617676 seconds ---\n",
      "epoch : 6/100, loss = 0.70466190\n",
      "Avg Grad Norm: 0.0048, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.3598556518554688 seconds ---\n",
      "epoch : 7/100, loss = 0.70278258\n",
      "Avg Grad Norm: 0.0047, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.4375653266906738 seconds ---\n",
      "epoch : 8/100, loss = 0.70087712\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.795217514038086 seconds ---\n",
      "epoch : 9/100, loss = 0.69920036\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.675894021987915 seconds ---\n",
      "epoch : 10/100, loss = 0.69818915\n",
      "Avg Grad Norm: 0.0045, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.8903462886810303 seconds ---\n",
      "epoch : 11/100, loss = 0.69794215\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 2.033090114593506 seconds ---\n",
      "epoch : 12/100, loss = 0.69783347\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.9152944087982178 seconds ---\n",
      "epoch : 13/100, loss = 0.69761850\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.945568561553955 seconds ---\n",
      "epoch : 14/100, loss = 0.69751486\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6855506896972656 seconds ---\n",
      "epoch : 15/100, loss = 0.69739377\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.835573673248291 seconds ---\n",
      "epoch : 16/100, loss = 0.69719609\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.880054235458374 seconds ---\n",
      "epoch : 17/100, loss = 0.69710243\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6667132377624512 seconds ---\n",
      "epoch : 18/100, loss = 0.69689671\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8991730213165283 seconds ---\n",
      "epoch : 19/100, loss = 0.69678214\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6054503917694092 seconds ---\n",
      "epoch : 20/100, loss = 0.69665193\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7364840507507324 seconds ---\n",
      "epoch : 21/100, loss = 0.69662918\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8177473545074463 seconds ---\n",
      "epoch : 22/100, loss = 0.69663436\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6741223335266113 seconds ---\n",
      "epoch : 23/100, loss = 0.69661241\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.472379446029663 seconds ---\n",
      "epoch : 24/100, loss = 0.69661745\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.711789608001709 seconds ---\n",
      "epoch : 25/100, loss = 0.69656416\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.838752269744873 seconds ---\n",
      "epoch : 26/100, loss = 0.69656456\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6510226726531982 seconds ---\n",
      "epoch : 27/100, loss = 0.69653384\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.960465431213379 seconds ---\n",
      "epoch : 28/100, loss = 0.69651773\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6735124588012695 seconds ---\n",
      "epoch : 29/100, loss = 0.69648942\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8932268619537354 seconds ---\n",
      "epoch : 30/100, loss = 0.69657291\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 2.018463134765625 seconds ---\n",
      "epoch : 31/100, loss = 0.69653391\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 2.036924362182617 seconds ---\n",
      "epoch : 32/100, loss = 0.69649261\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7188193798065186 seconds ---\n",
      "epoch : 33/100, loss = 0.69649655\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.913128137588501 seconds ---\n",
      "epoch : 34/100, loss = 0.69649572\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6973087787628174 seconds ---\n",
      "epoch : 35/100, loss = 0.69649166\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.696241855621338 seconds ---\n",
      "epoch : 36/100, loss = 0.69656430\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8591153621673584 seconds ---\n",
      "epoch : 37/100, loss = 0.69657375\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7606983184814453 seconds ---\n",
      "epoch : 38/100, loss = 0.69647369\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5777039527893066 seconds ---\n",
      "epoch : 39/100, loss = 0.69651347\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3778009414672852 seconds ---\n",
      "epoch : 40/100, loss = 0.69652589\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4182491302490234 seconds ---\n",
      "epoch : 41/100, loss = 0.69643802\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2654778957366943 seconds ---\n",
      "epoch : 42/100, loss = 0.69650517\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2562892436981201 seconds ---\n",
      "epoch : 43/100, loss = 0.69654840\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6823420524597168 seconds ---\n",
      "epoch : 44/100, loss = 0.69647625\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5515666007995605 seconds ---\n",
      "epoch : 45/100, loss = 0.69656579\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4923365116119385 seconds ---\n",
      "epoch : 46/100, loss = 0.69650358\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.476475715637207 seconds ---\n",
      "epoch : 47/100, loss = 0.69650360\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6844263076782227 seconds ---\n",
      "epoch : 48/100, loss = 0.69649301\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6909148693084717 seconds ---\n",
      "epoch : 49/100, loss = 0.69649293\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5256857872009277 seconds ---\n",
      "epoch : 50/100, loss = 0.69652913\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4490764141082764 seconds ---\n",
      "epoch : 51/100, loss = 0.69659028\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2598645687103271 seconds ---\n",
      "epoch : 52/100, loss = 0.69648310\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5295765399932861 seconds ---\n",
      "epoch : 53/100, loss = 0.69649727\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.278841257095337 seconds ---\n",
      "epoch : 54/100, loss = 0.69650789\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5415003299713135 seconds ---\n",
      "epoch : 55/100, loss = 0.69646662\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5031931400299072 seconds ---\n",
      "epoch : 56/100, loss = 0.69644714\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5248379707336426 seconds ---\n",
      "epoch : 57/100, loss = 0.69652351\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.6155328750610352 seconds ---\n",
      "epoch : 58/100, loss = 0.69651688\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.487905740737915 seconds ---\n",
      "epoch : 59/100, loss = 0.69645311\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3757414817810059 seconds ---\n",
      "epoch : 60/100, loss = 0.69649407\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3027384281158447 seconds ---\n",
      "epoch : 61/100, loss = 0.69648551\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3944628238677979 seconds ---\n",
      "epoch : 62/100, loss = 0.69648736\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3206818103790283 seconds ---\n",
      "epoch : 63/100, loss = 0.69655954\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5063445568084717 seconds ---\n",
      "epoch : 64/100, loss = 0.69657391\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.8325002193450928 seconds ---\n",
      "epoch : 65/100, loss = 0.69654406\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 2.0090832710266113 seconds ---\n",
      "epoch : 66/100, loss = 0.69648631\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4503705501556396 seconds ---\n",
      "epoch : 67/100, loss = 0.69649113\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2366015911102295 seconds ---\n",
      "epoch : 68/100, loss = 0.69662389\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.246882677078247 seconds ---\n",
      "epoch : 69/100, loss = 0.69652211\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3182380199432373 seconds ---\n",
      "epoch : 70/100, loss = 0.69648699\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.9558544158935547 seconds ---\n",
      "epoch : 71/100, loss = 0.69650928\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2577672004699707 seconds ---\n",
      "epoch : 72/100, loss = 0.69645442\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.337172269821167 seconds ---\n",
      "epoch : 73/100, loss = 0.69653460\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.301318883895874 seconds ---\n",
      "epoch : 74/100, loss = 0.69647480\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5165138244628906 seconds ---\n",
      "epoch : 75/100, loss = 0.69646190\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.48258376121521 seconds ---\n",
      "epoch : 76/100, loss = 0.69653832\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.649498462677002 seconds ---\n",
      "epoch : 77/100, loss = 0.69654799\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3741307258605957 seconds ---\n",
      "epoch : 78/100, loss = 0.69658546\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.4233167171478271 seconds ---\n",
      "epoch : 79/100, loss = 0.69644811\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2652363777160645 seconds ---\n",
      "epoch : 80/100, loss = 0.69650693\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5170397758483887 seconds ---\n",
      "epoch : 81/100, loss = 0.69653253\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.3225719928741455 seconds ---\n",
      "epoch : 82/100, loss = 0.69651183\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.584892988204956 seconds ---\n",
      "epoch : 83/100, loss = 0.69662381\n",
      "Avg Grad Norm: 0.0045, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 1.3906116485595703 seconds ---\n",
      "epoch : 84/100, loss = 0.69650443\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7503166198730469 seconds ---\n",
      "epoch : 85/100, loss = 0.69663491\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.562878131866455 seconds ---\n",
      "epoch : 86/100, loss = 0.69648457\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5824973583221436 seconds ---\n",
      "epoch : 87/100, loss = 0.69647450\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.82411527633667 seconds ---\n",
      "epoch : 88/100, loss = 0.69654149\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7973251342773438 seconds ---\n",
      "epoch : 89/100, loss = 0.69646564\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.88004732131958 seconds ---\n",
      "epoch : 90/100, loss = 0.69652548\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.5275602340698242 seconds ---\n",
      "epoch : 91/100, loss = 0.69647067\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7002079486846924 seconds ---\n",
      "epoch : 92/100, loss = 0.69645203\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.7309739589691162 seconds ---\n",
      "epoch : 93/100, loss = 0.69646772\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.367478370666504 seconds ---\n",
      "epoch : 94/100, loss = 0.69650871\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.238755464553833 seconds ---\n",
      "epoch : 95/100, loss = 0.69648070\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 2.034677743911743 seconds ---\n",
      "epoch : 96/100, loss = 0.69644457\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2506308555603027 seconds ---\n",
      "epoch : 97/100, loss = 0.69645797\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.239722490310669 seconds ---\n",
      "epoch : 98/100, loss = 0.69647517\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2471387386322021 seconds ---\n",
      "epoch : 99/100, loss = 0.69652040\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 1.2412161827087402 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "total_epochs = 100\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    loss_class = nn.BCEWithLogitsLoss()\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = loss_class(output_function.swapaxes(0,1), target_function)#torch.mean((output_function - target_function) ** 2)\n",
    "    return function_loss\n",
    "    #return y_loss*(1-lam) + params_loss*lam\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        if check_nan(inputs, \"inputs\") or check_nan(true_params, \"true_params\") or check_nan(true_func, \"true_func\"):\n",
    "            continue\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        if check_nan(best_out, \"best_out\") or check_nan(best_index, \"best_index\") or check_nan(best_params, \"best_params\"):\n",
    "            continue\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        if check_nan(loss, \"loss\"):\n",
    "            continue\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if(param.grad is None):\n",
    "                print(f\"None detected in {name}\")\n",
    "            if check_nan(param.grad, f\"gradient of {name}\"):\n",
    "                continue\n",
    "        \n",
    "        '''for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: grad norm = {param.grad.norm()}, grad std = {param.grad.std()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")'''\n",
    "\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    " \n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1137582/182172253.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  best_indexes = torch.tensor(best_indexes, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/100, loss = 0.31649999\n",
      "--- 0.1766524314880371 seconds ---\n",
      "epoch : 1/100, loss = 0.31649999\n",
      "--- 0.1309223175048828 seconds ---\n",
      "epoch : 2/100, loss = 0.31649999\n",
      "--- 0.13064241409301758 seconds ---\n",
      "epoch : 3/100, loss = 0.31649999\n",
      "--- 0.130615234375 seconds ---\n",
      "epoch : 4/100, loss = 0.31649999\n",
      "--- 0.13306736946105957 seconds ---\n",
      "epoch : 5/100, loss = 0.31649999\n",
      "--- 0.13086271286010742 seconds ---\n",
      "epoch : 6/100, loss = 0.31649999\n",
      "--- 0.1311173439025879 seconds ---\n",
      "epoch : 7/100, loss = 0.31649999\n",
      "--- 0.13053512573242188 seconds ---\n",
      "epoch : 8/100, loss = 0.31649999\n",
      "--- 0.13073253631591797 seconds ---\n",
      "epoch : 9/100, loss = 0.31649999\n",
      "--- 0.1305248737335205 seconds ---\n",
      "epoch : 10/100, loss = 0.31649999\n",
      "--- 0.1305999755859375 seconds ---\n",
      "epoch : 11/100, loss = 0.31649999\n",
      "--- 0.13042736053466797 seconds ---\n",
      "epoch : 12/100, loss = 0.31649999\n",
      "--- 0.1331171989440918 seconds ---\n",
      "epoch : 13/100, loss = 0.31649999\n",
      "--- 0.13069987297058105 seconds ---\n",
      "epoch : 14/100, loss = 0.31649999\n",
      "--- 0.13083815574645996 seconds ---\n",
      "epoch : 15/100, loss = 0.31649999\n",
      "--- 0.13071084022521973 seconds ---\n",
      "epoch : 16/100, loss = 0.31649999\n",
      "--- 0.13096094131469727 seconds ---\n",
      "epoch : 17/100, loss = 0.31649999\n",
      "--- 0.13074302673339844 seconds ---\n",
      "epoch : 18/100, loss = 0.31649999\n",
      "--- 0.13081097602844238 seconds ---\n",
      "epoch : 19/100, loss = 0.31650000\n",
      "--- 0.13070344924926758 seconds ---\n",
      "epoch : 20/100, loss = 0.31649999\n",
      "--- 0.1305985450744629 seconds ---\n",
      "epoch : 21/100, loss = 0.31649999\n",
      "--- 0.1327042579650879 seconds ---\n",
      "epoch : 22/100, loss = 0.31649999\n",
      "--- 0.13103437423706055 seconds ---\n",
      "epoch : 23/100, loss = 0.31649999\n",
      "--- 0.13060665130615234 seconds ---\n",
      "epoch : 24/100, loss = 0.31649999\n",
      "--- 0.13081717491149902 seconds ---\n",
      "epoch : 25/100, loss = 0.31649999\n",
      "--- 0.13053345680236816 seconds ---\n",
      "epoch : 26/100, loss = 0.31649999\n",
      "--- 0.13057780265808105 seconds ---\n",
      "epoch : 27/100, loss = 0.31649999\n",
      "--- 0.1304335594177246 seconds ---\n",
      "epoch : 28/100, loss = 0.31649999\n",
      "--- 0.1307222843170166 seconds ---\n",
      "epoch : 29/100, loss = 0.31649999\n",
      "--- 0.1316075325012207 seconds ---\n",
      "epoch : 30/100, loss = 0.31649999\n",
      "--- 0.130659818649292 seconds ---\n",
      "epoch : 31/100, loss = 0.31649999\n",
      "--- 0.13101935386657715 seconds ---\n",
      "epoch : 32/100, loss = 0.31649999\n",
      "--- 0.1308891773223877 seconds ---\n",
      "epoch : 33/100, loss = 0.31649999\n",
      "--- 0.13057327270507812 seconds ---\n",
      "epoch : 34/100, loss = 0.31649999\n",
      "--- 0.13056111335754395 seconds ---\n",
      "epoch : 35/100, loss = 0.31649999\n",
      "--- 0.1309494972229004 seconds ---\n",
      "epoch : 36/100, loss = 0.31649999\n",
      "--- 0.1306157112121582 seconds ---\n",
      "epoch : 37/100, loss = 0.31649999\n",
      "--- 0.13081860542297363 seconds ---\n",
      "epoch : 38/100, loss = 0.31649999\n",
      "--- 0.13215184211730957 seconds ---\n",
      "epoch : 39/100, loss = 0.31649999\n",
      "--- 0.13072443008422852 seconds ---\n",
      "epoch : 40/100, loss = 0.31649999\n",
      "--- 0.1312863826751709 seconds ---\n",
      "epoch : 41/100, loss = 0.31649999\n",
      "--- 0.1303246021270752 seconds ---\n",
      "epoch : 42/100, loss = 0.31649999\n",
      "--- 0.13068008422851562 seconds ---\n",
      "epoch : 43/100, loss = 0.31649999\n",
      "--- 0.13045144081115723 seconds ---\n",
      "epoch : 44/100, loss = 0.31649999\n",
      "--- 0.1310417652130127 seconds ---\n",
      "epoch : 45/100, loss = 0.31649999\n",
      "--- 0.1306288242340088 seconds ---\n",
      "epoch : 46/100, loss = 0.31649999\n",
      "--- 0.13291430473327637 seconds ---\n",
      "epoch : 47/100, loss = 0.31650000\n",
      "--- 0.13050341606140137 seconds ---\n",
      "epoch : 48/100, loss = 0.31649999\n",
      "--- 0.1305253505706787 seconds ---\n",
      "epoch : 49/100, loss = 0.31650000\n",
      "--- 0.13025403022766113 seconds ---\n",
      "epoch : 50/100, loss = 0.31649999\n",
      "--- 0.1305522918701172 seconds ---\n",
      "epoch : 51/100, loss = 0.31649999\n",
      "--- 0.13028430938720703 seconds ---\n",
      "epoch : 52/100, loss = 0.31649999\n",
      "--- 0.1310412883758545 seconds ---\n",
      "epoch : 53/100, loss = 0.31649999\n",
      "--- 0.13139724731445312 seconds ---\n",
      "epoch : 54/100, loss = 0.31649999\n",
      "--- 0.1305103302001953 seconds ---\n",
      "epoch : 55/100, loss = 0.31649999\n",
      "--- 0.13022804260253906 seconds ---\n",
      "epoch : 56/100, loss = 0.31649999\n",
      "--- 0.13213729858398438 seconds ---\n",
      "epoch : 57/100, loss = 0.31649999\n",
      "--- 0.1303858757019043 seconds ---\n",
      "epoch : 58/100, loss = 0.31649999\n",
      "--- 0.13059186935424805 seconds ---\n",
      "epoch : 59/100, loss = 0.31650000\n",
      "--- 0.1305701732635498 seconds ---\n",
      "epoch : 60/100, loss = 0.31649999\n",
      "--- 0.13086700439453125 seconds ---\n",
      "epoch : 61/100, loss = 0.31649999\n",
      "--- 0.13031864166259766 seconds ---\n",
      "epoch : 62/100, loss = 0.31649999\n",
      "--- 0.13055706024169922 seconds ---\n",
      "epoch : 63/100, loss = 0.31649999\n",
      "--- 0.13066554069519043 seconds ---\n",
      "epoch : 64/100, loss = 0.31649999\n",
      "--- 0.13054728507995605 seconds ---\n",
      "epoch : 65/100, loss = 0.31649999\n",
      "--- 0.13042259216308594 seconds ---\n",
      "epoch : 66/100, loss = 0.31649999\n",
      "--- 0.13277578353881836 seconds ---\n",
      "epoch : 67/100, loss = 0.31649999\n",
      "--- 0.13048028945922852 seconds ---\n",
      "epoch : 68/100, loss = 0.31649999\n",
      "--- 0.13069677352905273 seconds ---\n",
      "epoch : 69/100, loss = 0.31649999\n",
      "--- 0.13023710250854492 seconds ---\n",
      "epoch : 70/100, loss = 0.31649999\n",
      "--- 0.13062143325805664 seconds ---\n",
      "epoch : 71/100, loss = 0.31649999\n",
      "--- 0.13083457946777344 seconds ---\n",
      "epoch : 72/100, loss = 0.31649999\n",
      "--- 0.13061952590942383 seconds ---\n",
      "epoch : 73/100, loss = 0.31649999\n",
      "--- 0.13042807579040527 seconds ---\n",
      "epoch : 74/100, loss = 0.31649999\n",
      "--- 0.13066816329956055 seconds ---\n",
      "epoch : 75/100, loss = 0.31649999\n",
      "--- 0.1304323673248291 seconds ---\n",
      "epoch : 76/100, loss = 0.31649999\n",
      "--- 0.1314527988433838 seconds ---\n",
      "epoch : 77/100, loss = 0.31649999\n",
      "--- 0.1304159164428711 seconds ---\n",
      "epoch : 78/100, loss = 0.31649999\n",
      "--- 0.13069844245910645 seconds ---\n",
      "epoch : 79/100, loss = 0.31649999\n",
      "--- 0.13054132461547852 seconds ---\n",
      "epoch : 80/100, loss = 0.31649999\n",
      "--- 0.13072991371154785 seconds ---\n",
      "epoch : 81/100, loss = 0.31650000\n",
      "--- 0.13035297393798828 seconds ---\n",
      "epoch : 82/100, loss = 0.31649999\n",
      "--- 0.13052630424499512 seconds ---\n",
      "epoch : 83/100, loss = 0.31649999\n",
      "--- 0.13027524948120117 seconds ---\n",
      "epoch : 84/100, loss = 0.31649999\n",
      "--- 0.13066339492797852 seconds ---\n",
      "epoch : 85/100, loss = 0.31649999\n",
      "--- 0.13039779663085938 seconds ---\n",
      "epoch : 86/100, loss = 0.31649999\n",
      "--- 0.13201189041137695 seconds ---\n",
      "epoch : 87/100, loss = 0.31649999\n",
      "--- 0.13043832778930664 seconds ---\n",
      "epoch : 88/100, loss = 0.31649999\n",
      "--- 0.13071775436401367 seconds ---\n",
      "epoch : 89/100, loss = 0.31649999\n",
      "--- 0.1305100917816162 seconds ---\n",
      "epoch : 90/100, loss = 0.31649999\n",
      "--- 0.13068771362304688 seconds ---\n",
      "epoch : 91/100, loss = 0.31649999\n",
      "--- 0.13059544563293457 seconds ---\n",
      "epoch : 92/100, loss = 0.31649999\n",
      "--- 0.13066577911376953 seconds ---\n",
      "epoch : 93/100, loss = 0.31649999\n",
      "--- 0.1304018497467041 seconds ---\n",
      "epoch : 94/100, loss = 0.31649999\n",
      "--- 0.13051533699035645 seconds ---\n",
      "epoch : 95/100, loss = 0.31649999\n",
      "--- 0.1302812099456787 seconds ---\n",
      "epoch : 96/100, loss = 0.31649999\n",
      "--- 0.13255715370178223 seconds ---\n",
      "epoch : 97/100, loss = 0.31649999\n",
      "--- 0.13053345680236816 seconds ---\n",
      "epoch : 98/100, loss = 0.31649999\n",
      "--- 0.13080811500549316 seconds ---\n",
      "epoch : 99/100, loss = 0.31649999\n",
      "--- 0.1306912899017334 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    optimizer.zero_grad()\n",
    "    for inputs, true_params, true_func in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 100])\n",
      "best_func: (k*x, tensor([0.5162, 0.4838], device='cuda:3', grad_fn=<UnbindBackward0>))\n",
      "best_loss: 0.3484625816345215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fba6afb3450>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "ran = np.random.randint(0, 10)\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values[0:2000])\n",
    "print(best_out.shape)\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"best_loss: {best_loss[ran]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
