{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.selu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func(nn.Module):\n",
    "    def __init__(self, functions, num_params, symbols, x_data, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device).requires_grad_(True)\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([8, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7, padding=3),\n",
    "            nn.LayerNorm([6, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 100]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.LayerNorm(20),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),           \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5, padding=2),\n",
    "            nn.LayerNorm([4, 32]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def sympy_to_torch(self, expr, symbols):\n",
    "        torch_funcs = {\n",
    "            sp.Add: lambda *args: reduce(torch.add, args),\n",
    "            sp.Mul: lambda *args: reduce(torch.mul, args),\n",
    "            sp.Pow: torch.pow,\n",
    "            sp.sin: torch.sin,\n",
    "            sp.cos: torch.cos,\n",
    "        }\n",
    "\n",
    "        def torch_func(*args):\n",
    "            def _eval(ex):\n",
    "                if isinstance(ex, sp.Symbol):\n",
    "                    return args[symbols.index(ex)]\n",
    "                elif isinstance(ex, sp.Number):\n",
    "                    return torch.full_like(args[0], float(ex))\n",
    "                elif isinstance(ex, sp.Expr):\n",
    "                    op = type(ex)\n",
    "                    if op in torch_funcs:\n",
    "                        return torch_funcs[op](*[_eval(arg) for arg in ex.args])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported operation: {op}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type: {type(ex)}\")\n",
    "            \n",
    "            return _eval(expr)\n",
    "\n",
    "        return torch_func\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        torch_func = self.sympy_to_torch(formula, symbols)\n",
    "        var_values = [params[:, j] for j in range(len(symbols)-1)] + [x.unsqueeze(1)]\n",
    "        results = torch_func(*var_values)\n",
    "        return results.swapaxes(0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.requires_grad_(True)\n",
    "        outs = inputs.unsqueeze(1).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        all_params = []\n",
    "        \n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            all_params.append(F.pad(params, (0, self.max_params-self.num_params[f])))\n",
    "            output = self.evaluate(params, f).to(self.device)\n",
    "            outputs.append(output)\n",
    "            loss = torch.mean(((inputs - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]        \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(outputs).to(self.device)\n",
    "        weights = F.softmax(-stacked_losses, dim=0)\n",
    "        best_out = torch.sum(weights.unsqueeze(2) * stacked_preds, dim=0)\n",
    "        best_loss = torch.sum(weights * stacked_losses, dim=0)        \n",
    "        best_func = weights.t()\n",
    "        best_params = torch.sum(weights.unsqueeze(2) * torch.stack(all_params), dim=0)\n",
    "        return best_out, best_loss, best_func, weights, best_params, outputs, losses, all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2247633/2683534046.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold1 = torch.load('hold_data1.pth')\n",
      "/tmp/ipykernel_2247633/2683534046.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold2 = torch.load('hold_data2.pth')\n",
      "/tmp/ipykernel_2247633/2683534046.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hold3 = torch.load('hold_data3.pth')\n"
     ]
    }
   ],
   "source": [
    "hold1 = torch.load('hold_data1.pth')\n",
    "hold2 = torch.load('hold_data2.pth')\n",
    "hold3 = torch.load('hold_data3.pth')\n",
    "#hold4 = torch.load('hold_data4.pth')\n",
    "#hold5 = torch.load('hold_data5.pth')\n",
    "\n",
    "x_values = hold1['x_values'].to(device)\n",
    "y_values = hold1['y_values'].to(device)\n",
    "#derivatives = torch.cat((hold4['derivatives1'],hold4['derivatives2'])).to(device)\n",
    "functions = hold2['formulas']\n",
    "symbols = hold2['symbols']\n",
    "function_labels = hold2['function_labels'].to(device)\n",
    "params = hold3['param_values'].to(device)\n",
    "num_params = hold3['num_params'].to(device)\n",
    "full_params = hold3['full_params'].to(device)\n",
    "target_funcs = hold1['target_funcs'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([100000, 100])\n",
      "param_values: torch.Size([100000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([100000])\n",
      "full_params: torch.Size([100000, 50])\n",
      "target_funcs: torch.Size([100000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "#print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")\n",
    "print(f\"full_params: {full_params.shape}\")\n",
    "print(f\"target_funcs: {target_funcs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    def __init__(self, data1, data2, data3):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data1[index], self.data2[index], self.data3[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripleDataset(y_values[0:2000], params[0:2000], target_funcs[:, 0:2])\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0/500, loss = 0.26476470\n",
      "Avg Grad Norm: 0.0341, Avg Grad Mean: -0.0004, Avg Grad Std: 0.0029\n",
      "--- 0.688868522644043 seconds ---\n",
      "epoch : 1/500, loss = 0.04637934\n",
      "Avg Grad Norm: 0.0208, Avg Grad Mean: -0.0001, Avg Grad Std: 0.0019\n",
      "--- 0.35109496116638184 seconds ---\n",
      "epoch : 2/500, loss = 0.01443847\n",
      "Avg Grad Norm: 0.0176, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.35982370376586914 seconds ---\n",
      "epoch : 3/500, loss = 0.00741651\n",
      "Avg Grad Norm: 0.0101, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0009\n",
      "--- 0.35796141624450684 seconds ---\n",
      "epoch : 4/500, loss = 0.00538392\n",
      "Avg Grad Norm: 0.0065, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.38460206985473633 seconds ---\n",
      "epoch : 5/500, loss = 0.00470538\n",
      "Avg Grad Norm: 0.0082, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0009\n",
      "--- 0.3805358409881592 seconds ---\n",
      "epoch : 6/500, loss = 0.00417563\n",
      "Avg Grad Norm: 0.0062, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.37621545791625977 seconds ---\n",
      "epoch : 7/500, loss = 0.00356177\n",
      "Avg Grad Norm: 0.0062, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.38002824783325195 seconds ---\n",
      "epoch : 8/500, loss = 0.00327130\n",
      "Avg Grad Norm: 0.0188, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0020\n",
      "--- 0.38095712661743164 seconds ---\n",
      "epoch : 9/500, loss = 0.00271111\n",
      "Avg Grad Norm: 0.0216, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0022\n",
      "--- 0.37896108627319336 seconds ---\n",
      "epoch : 10/500, loss = 0.00165665\n",
      "Avg Grad Norm: 0.0156, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0016\n",
      "--- 0.38715672492980957 seconds ---\n",
      "epoch : 11/500, loss = 0.00101835\n",
      "Avg Grad Norm: 0.0103, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0011\n",
      "--- 0.3646883964538574 seconds ---\n",
      "epoch : 12/500, loss = 0.00075031\n",
      "Avg Grad Norm: 0.0086, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0009\n",
      "--- 0.36827516555786133 seconds ---\n",
      "epoch : 13/500, loss = 0.00058400\n",
      "Avg Grad Norm: 0.0079, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0008\n",
      "--- 0.3685119152069092 seconds ---\n",
      "epoch : 14/500, loss = 0.00044251\n",
      "Avg Grad Norm: 0.0065, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.372499942779541 seconds ---\n",
      "epoch : 15/500, loss = 0.00039172\n",
      "Avg Grad Norm: 0.0072, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3695530891418457 seconds ---\n",
      "epoch : 16/500, loss = 0.00031944\n",
      "Avg Grad Norm: 0.0053, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36604833602905273 seconds ---\n",
      "epoch : 17/500, loss = 0.00035334\n",
      "Avg Grad Norm: 0.0081, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0008\n",
      "--- 0.3795442581176758 seconds ---\n",
      "epoch : 18/500, loss = 0.00030951\n",
      "Avg Grad Norm: 0.0074, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0008\n",
      "--- 0.36772894859313965 seconds ---\n",
      "epoch : 19/500, loss = 0.00027365\n",
      "Avg Grad Norm: 0.0067, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3649156093597412 seconds ---\n",
      "epoch : 20/500, loss = 0.00026339\n",
      "Avg Grad Norm: 0.0069, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3622007369995117 seconds ---\n",
      "epoch : 21/500, loss = 0.00033212\n",
      "Avg Grad Norm: 0.0100, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0010\n",
      "--- 0.36945080757141113 seconds ---\n",
      "epoch : 22/500, loss = 0.00036281\n",
      "Avg Grad Norm: 0.0101, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0010\n",
      "--- 0.3726232051849365 seconds ---\n",
      "epoch : 23/500, loss = 0.00019902\n",
      "Avg Grad Norm: 0.0055, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.36870861053466797 seconds ---\n",
      "epoch : 24/500, loss = 0.00016734\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.371126651763916 seconds ---\n",
      "epoch : 25/500, loss = 0.00017876\n",
      "Avg Grad Norm: 0.0052, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3728616237640381 seconds ---\n",
      "epoch : 26/500, loss = 0.00013851\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3674321174621582 seconds ---\n",
      "epoch : 27/500, loss = 0.00023014\n",
      "Avg Grad Norm: 0.0082, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0009\n",
      "--- 0.37068724632263184 seconds ---\n",
      "epoch : 28/500, loss = 0.00018617\n",
      "Avg Grad Norm: 0.0067, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3838040828704834 seconds ---\n",
      "epoch : 29/500, loss = 0.00014530\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.37026095390319824 seconds ---\n",
      "epoch : 30/500, loss = 0.00017183\n",
      "Avg Grad Norm: 0.0065, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3699805736541748 seconds ---\n",
      "epoch : 31/500, loss = 0.00014045\n",
      "Avg Grad Norm: 0.0047, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36179256439208984 seconds ---\n",
      "epoch : 32/500, loss = 0.00017167\n",
      "Avg Grad Norm: 0.0065, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3730504512786865 seconds ---\n",
      "epoch : 33/500, loss = 0.00016981\n",
      "Avg Grad Norm: 0.0063, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.373917818069458 seconds ---\n",
      "epoch : 34/500, loss = 0.00012460\n",
      "Avg Grad Norm: 0.0045, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3639335632324219 seconds ---\n",
      "epoch : 35/500, loss = 0.00014565\n",
      "Avg Grad Norm: 0.0056, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.36610984802246094 seconds ---\n",
      "epoch : 36/500, loss = 0.00015626\n",
      "Avg Grad Norm: 0.0058, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.36747241020202637 seconds ---\n",
      "epoch : 37/500, loss = 0.00042534\n",
      "Avg Grad Norm: 0.0119, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0012\n",
      "--- 0.36553215980529785 seconds ---\n",
      "epoch : 38/500, loss = 0.00040330\n",
      "Avg Grad Norm: 0.0110, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0011\n",
      "--- 0.36687135696411133 seconds ---\n",
      "epoch : 39/500, loss = 0.00019612\n",
      "Avg Grad Norm: 0.0067, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3430933952331543 seconds ---\n",
      "epoch : 40/500, loss = 0.00015645\n",
      "Avg Grad Norm: 0.0060, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.368910551071167 seconds ---\n",
      "epoch : 41/500, loss = 0.00007828\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.368070125579834 seconds ---\n",
      "epoch : 42/500, loss = 0.00007170\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36818718910217285 seconds ---\n",
      "epoch : 43/500, loss = 0.00014495\n",
      "Avg Grad Norm: 0.0064, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.37085628509521484 seconds ---\n",
      "epoch : 44/500, loss = 0.00013600\n",
      "Avg Grad Norm: 0.0054, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3659207820892334 seconds ---\n",
      "epoch : 45/500, loss = 0.00007483\n",
      "Avg Grad Norm: 0.0025, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.33592653274536133 seconds ---\n",
      "epoch : 46/500, loss = 0.00010310\n",
      "Avg Grad Norm: 0.0050, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36686277389526367 seconds ---\n",
      "epoch : 47/500, loss = 0.00014835\n",
      "Avg Grad Norm: 0.0070, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0008\n",
      "--- 0.36898350715637207 seconds ---\n",
      "epoch : 48/500, loss = 0.00007786\n",
      "Avg Grad Norm: 0.0032, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3704414367675781 seconds ---\n",
      "epoch : 49/500, loss = 0.00009191\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.37167859077453613 seconds ---\n",
      "epoch : 50/500, loss = 0.00012390\n",
      "Avg Grad Norm: 0.0058, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.320664644241333 seconds ---\n",
      "epoch : 51/500, loss = 0.00010343\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3074510097503662 seconds ---\n",
      "epoch : 52/500, loss = 0.00007251\n",
      "Avg Grad Norm: 0.0032, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.31557202339172363 seconds ---\n",
      "epoch : 53/500, loss = 0.00007259\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.31619834899902344 seconds ---\n",
      "epoch : 54/500, loss = 0.00008915\n",
      "Avg Grad Norm: 0.0045, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3122985363006592 seconds ---\n",
      "epoch : 55/500, loss = 0.00010616\n",
      "Avg Grad Norm: 0.0053, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.31531620025634766 seconds ---\n",
      "epoch : 56/500, loss = 0.00007780\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3120918273925781 seconds ---\n",
      "epoch : 57/500, loss = 0.00007784\n",
      "Avg Grad Norm: 0.0035, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3169229030609131 seconds ---\n",
      "epoch : 58/500, loss = 0.00011177\n",
      "Avg Grad Norm: 0.0054, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.309037446975708 seconds ---\n",
      "epoch : 59/500, loss = 0.00008440\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3170015811920166 seconds ---\n",
      "epoch : 60/500, loss = 0.00008783\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3126561641693115 seconds ---\n",
      "epoch : 61/500, loss = 0.00009643\n",
      "Avg Grad Norm: 0.0049, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.31328773498535156 seconds ---\n",
      "epoch : 62/500, loss = 0.00006578\n",
      "Avg Grad Norm: 0.0031, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3086986541748047 seconds ---\n",
      "epoch : 63/500, loss = 0.00006449\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.31325626373291016 seconds ---\n",
      "epoch : 64/500, loss = 0.00004917\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.30994486808776855 seconds ---\n",
      "epoch : 65/500, loss = 0.00009149\n",
      "Avg Grad Norm: 0.0047, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.31005263328552246 seconds ---\n",
      "epoch : 66/500, loss = 0.00020532\n",
      "Avg Grad Norm: 0.0089, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0010\n",
      "--- 0.3071260452270508 seconds ---\n",
      "epoch : 67/500, loss = 0.00010502\n",
      "Avg Grad Norm: 0.0047, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.31044864654541016 seconds ---\n",
      "epoch : 68/500, loss = 0.00005667\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.30737972259521484 seconds ---\n",
      "epoch : 69/500, loss = 0.00008588\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3206312656402588 seconds ---\n",
      "epoch : 70/500, loss = 0.00009370\n",
      "Avg Grad Norm: 0.0048, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.35859227180480957 seconds ---\n",
      "epoch : 71/500, loss = 0.00008107\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36315274238586426 seconds ---\n",
      "epoch : 72/500, loss = 0.00009754\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36505722999572754 seconds ---\n",
      "epoch : 73/500, loss = 0.00006049\n",
      "Avg Grad Norm: 0.0030, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36685800552368164 seconds ---\n",
      "epoch : 74/500, loss = 0.00007430\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3681528568267822 seconds ---\n",
      "epoch : 75/500, loss = 0.00006461\n",
      "Avg Grad Norm: 0.0035, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36909914016723633 seconds ---\n",
      "epoch : 76/500, loss = 0.00009228\n",
      "Avg Grad Norm: 0.0051, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3678255081176758 seconds ---\n",
      "epoch : 77/500, loss = 0.00007136\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36324572563171387 seconds ---\n",
      "epoch : 78/500, loss = 0.00011053\n",
      "Avg Grad Norm: 0.0059, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3650059700012207 seconds ---\n",
      "epoch : 79/500, loss = 0.00004873\n",
      "Avg Grad Norm: 0.0021, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.363675594329834 seconds ---\n",
      "epoch : 80/500, loss = 0.00009289\n",
      "Avg Grad Norm: 0.0051, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3641185760498047 seconds ---\n",
      "epoch : 81/500, loss = 0.00008305\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36507320404052734 seconds ---\n",
      "epoch : 82/500, loss = 0.00007505\n",
      "Avg Grad Norm: 0.0035, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36289453506469727 seconds ---\n",
      "epoch : 83/500, loss = 0.00007779\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3538076877593994 seconds ---\n",
      "epoch : 84/500, loss = 0.00007124\n",
      "Avg Grad Norm: 0.0036, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36144042015075684 seconds ---\n",
      "epoch : 85/500, loss = 0.00008761\n",
      "Avg Grad Norm: 0.0045, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36914825439453125 seconds ---\n",
      "epoch : 86/500, loss = 0.00009200\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.37864232063293457 seconds ---\n",
      "epoch : 87/500, loss = 0.00009293\n",
      "Avg Grad Norm: 0.0049, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3701663017272949 seconds ---\n",
      "epoch : 88/500, loss = 0.00005660\n",
      "Avg Grad Norm: 0.0025, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3709115982055664 seconds ---\n",
      "epoch : 89/500, loss = 0.00004499\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.3679537773132324 seconds ---\n",
      "epoch : 90/500, loss = 0.00008117\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.368180513381958 seconds ---\n",
      "epoch : 91/500, loss = 0.00011996\n",
      "Avg Grad Norm: 0.0059, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3708508014678955 seconds ---\n",
      "epoch : 92/500, loss = 0.00014172\n",
      "Avg Grad Norm: 0.0060, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3688204288482666 seconds ---\n",
      "epoch : 93/500, loss = 0.00008072\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36615443229675293 seconds ---\n",
      "epoch : 94/500, loss = 0.00005049\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.37093257904052734 seconds ---\n",
      "epoch : 95/500, loss = 0.00004918\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.35800695419311523 seconds ---\n",
      "epoch : 96/500, loss = 0.00007166\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.37392640113830566 seconds ---\n",
      "epoch : 97/500, loss = 0.00004787\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.368025541305542 seconds ---\n",
      "epoch : 98/500, loss = 0.00005436\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36508727073669434 seconds ---\n",
      "epoch : 99/500, loss = 0.00003827\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.3717670440673828 seconds ---\n",
      "epoch : 100/500, loss = 0.00004776\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3689699172973633 seconds ---\n",
      "epoch : 101/500, loss = 0.00004634\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3652842044830322 seconds ---\n",
      "epoch : 102/500, loss = 0.00017469\n",
      "Avg Grad Norm: 0.0081, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0009\n",
      "--- 0.36487698554992676 seconds ---\n",
      "epoch : 103/500, loss = 0.00023044\n",
      "Avg Grad Norm: 0.0076, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3538351058959961 seconds ---\n",
      "epoch : 104/500, loss = 0.00014763\n",
      "Avg Grad Norm: 0.0067, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0007\n",
      "--- 0.3659634590148926 seconds ---\n",
      "epoch : 105/500, loss = 0.00025258\n",
      "Avg Grad Norm: 0.0095, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0011\n",
      "--- 0.36527371406555176 seconds ---\n",
      "epoch : 106/500, loss = 0.00012683\n",
      "Avg Grad Norm: 0.0047, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3644552230834961 seconds ---\n",
      "epoch : 107/500, loss = 0.00007827\n",
      "Avg Grad Norm: 0.0033, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36455655097961426 seconds ---\n",
      "epoch : 108/500, loss = 0.00005227\n",
      "Avg Grad Norm: 0.0032, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3668556213378906 seconds ---\n",
      "epoch : 109/500, loss = 0.00004383\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3611307144165039 seconds ---\n",
      "epoch : 110/500, loss = 0.00004421\n",
      "Avg Grad Norm: 0.0028, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3607816696166992 seconds ---\n",
      "epoch : 111/500, loss = 0.00007565\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.368152379989624 seconds ---\n",
      "epoch : 112/500, loss = 0.00006427\n",
      "Avg Grad Norm: 0.0041, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3652184009552002 seconds ---\n",
      "epoch : 113/500, loss = 0.00004443\n",
      "Avg Grad Norm: 0.0023, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.3598036766052246 seconds ---\n",
      "epoch : 114/500, loss = 0.00003356\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.376680850982666 seconds ---\n",
      "epoch : 115/500, loss = 0.00004023\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3659837245941162 seconds ---\n",
      "epoch : 116/500, loss = 0.00007151\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36707639694213867 seconds ---\n",
      "epoch : 117/500, loss = 0.00008493\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3651716709136963 seconds ---\n",
      "epoch : 118/500, loss = 0.00006369\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.366976261138916 seconds ---\n",
      "epoch : 119/500, loss = 0.00006388\n",
      "Avg Grad Norm: 0.0032, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36978793144226074 seconds ---\n",
      "epoch : 120/500, loss = 0.00006668\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3621025085449219 seconds ---\n",
      "epoch : 121/500, loss = 0.00007027\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.37488651275634766 seconds ---\n",
      "epoch : 122/500, loss = 0.00005439\n",
      "Avg Grad Norm: 0.0031, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36188507080078125 seconds ---\n",
      "epoch : 123/500, loss = 0.00004800\n",
      "Avg Grad Norm: 0.0024, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36994481086730957 seconds ---\n",
      "epoch : 124/500, loss = 0.00005638\n",
      "Avg Grad Norm: 0.0037, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3719911575317383 seconds ---\n",
      "epoch : 125/500, loss = 0.00005786\n",
      "Avg Grad Norm: 0.0033, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36606574058532715 seconds ---\n",
      "epoch : 126/500, loss = 0.00005542\n",
      "Avg Grad Norm: 0.0032, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3689534664154053 seconds ---\n",
      "epoch : 127/500, loss = 0.00005850\n",
      "Avg Grad Norm: 0.0035, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3682718276977539 seconds ---\n",
      "epoch : 128/500, loss = 0.00005836\n",
      "Avg Grad Norm: 0.0036, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3481636047363281 seconds ---\n",
      "epoch : 129/500, loss = 0.00006435\n",
      "Avg Grad Norm: 0.0037, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3658010959625244 seconds ---\n",
      "epoch : 130/500, loss = 0.00007534\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36657190322875977 seconds ---\n",
      "epoch : 131/500, loss = 0.00006418\n",
      "Avg Grad Norm: 0.0036, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3643226623535156 seconds ---\n",
      "epoch : 132/500, loss = 0.00010377\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3651580810546875 seconds ---\n",
      "epoch : 133/500, loss = 0.00017147\n",
      "Avg Grad Norm: 0.0072, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0008\n",
      "--- 0.3650228977203369 seconds ---\n",
      "epoch : 134/500, loss = 0.00010377\n",
      "Avg Grad Norm: 0.0043, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3697211742401123 seconds ---\n",
      "epoch : 135/500, loss = 0.00006093\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3630492687225342 seconds ---\n",
      "epoch : 136/500, loss = 0.00011025\n",
      "Avg Grad Norm: 0.0056, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3592872619628906 seconds ---\n",
      "epoch : 137/500, loss = 0.00008686\n",
      "Avg Grad Norm: 0.0048, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36634016036987305 seconds ---\n",
      "epoch : 138/500, loss = 0.00006827\n",
      "Avg Grad Norm: 0.0037, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3643479347229004 seconds ---\n",
      "epoch : 139/500, loss = 0.00008051\n",
      "Avg Grad Norm: 0.0046, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3649876117706299 seconds ---\n",
      "epoch : 140/500, loss = 0.00004286\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3651421070098877 seconds ---\n",
      "epoch : 141/500, loss = 0.00004176\n",
      "Avg Grad Norm: 0.0023, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.3678438663482666 seconds ---\n",
      "epoch : 142/500, loss = 0.00004699\n",
      "Avg Grad Norm: 0.0031, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36411523818969727 seconds ---\n",
      "epoch : 143/500, loss = 0.00005721\n",
      "Avg Grad Norm: 0.0037, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3657114505767822 seconds ---\n",
      "epoch : 144/500, loss = 0.00005401\n",
      "Avg Grad Norm: 0.0036, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.35777711868286133 seconds ---\n",
      "epoch : 145/500, loss = 0.00003170\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36247944831848145 seconds ---\n",
      "epoch : 146/500, loss = 0.00004164\n",
      "Avg Grad Norm: 0.0028, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3666267395019531 seconds ---\n",
      "epoch : 147/500, loss = 0.00003703\n",
      "Avg Grad Norm: 0.0025, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3620162010192871 seconds ---\n",
      "epoch : 148/500, loss = 0.00002699\n",
      "Avg Grad Norm: 0.0015, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0001\n",
      "--- 0.3663358688354492 seconds ---\n",
      "epoch : 149/500, loss = 0.00003359\n",
      "Avg Grad Norm: 0.0023, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.3670222759246826 seconds ---\n",
      "epoch : 150/500, loss = 0.00002958\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36646580696105957 seconds ---\n",
      "epoch : 151/500, loss = 0.00004227\n",
      "Avg Grad Norm: 0.0029, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.35161709785461426 seconds ---\n",
      "epoch : 152/500, loss = 0.00016302\n",
      "Avg Grad Norm: 0.0057, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.36343884468078613 seconds ---\n",
      "epoch : 153/500, loss = 0.00013065\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.34226250648498535 seconds ---\n",
      "epoch : 154/500, loss = 0.00005738\n",
      "Avg Grad Norm: 0.0031, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36709117889404297 seconds ---\n",
      "epoch : 155/500, loss = 0.00004573\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3649177551269531 seconds ---\n",
      "epoch : 156/500, loss = 0.00006942\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36698174476623535 seconds ---\n",
      "epoch : 157/500, loss = 0.00009610\n",
      "Avg Grad Norm: 0.0055, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3608896732330322 seconds ---\n",
      "epoch : 158/500, loss = 0.00004324\n",
      "Avg Grad Norm: 0.0028, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3598289489746094 seconds ---\n",
      "epoch : 159/500, loss = 0.00003978\n",
      "Avg Grad Norm: 0.0023, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36504554748535156 seconds ---\n",
      "epoch : 160/500, loss = 0.00003499\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36005282402038574 seconds ---\n",
      "epoch : 161/500, loss = 0.00002667\n",
      "Avg Grad Norm: 0.0016, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0001\n",
      "--- 0.35812854766845703 seconds ---\n",
      "epoch : 162/500, loss = 0.00003661\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36184072494506836 seconds ---\n",
      "epoch : 163/500, loss = 0.00002973\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.3620946407318115 seconds ---\n",
      "epoch : 164/500, loss = 0.00003103\n",
      "Avg Grad Norm: 0.0022, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36605024337768555 seconds ---\n",
      "epoch : 165/500, loss = 0.00006104\n",
      "Avg Grad Norm: 0.0039, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.33704352378845215 seconds ---\n",
      "epoch : 166/500, loss = 0.00004170\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36276888847351074 seconds ---\n",
      "epoch : 167/500, loss = 0.00008670\n",
      "Avg Grad Norm: 0.0049, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3601384162902832 seconds ---\n",
      "epoch : 168/500, loss = 0.00007565\n",
      "Avg Grad Norm: 0.0044, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.3642852306365967 seconds ---\n",
      "epoch : 169/500, loss = 0.00003321\n",
      "Avg Grad Norm: 0.0020, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.35053229331970215 seconds ---\n",
      "epoch : 170/500, loss = 0.00005172\n",
      "Avg Grad Norm: 0.0035, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.35941529273986816 seconds ---\n",
      "epoch : 171/500, loss = 0.00006077\n",
      "Avg Grad Norm: 0.0038, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3650028705596924 seconds ---\n",
      "epoch : 172/500, loss = 0.00005891\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3589663505554199 seconds ---\n",
      "epoch : 173/500, loss = 0.00004338\n",
      "Avg Grad Norm: 0.0026, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.36598730087280273 seconds ---\n",
      "epoch : 174/500, loss = 0.00002894\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36381077766418457 seconds ---\n",
      "epoch : 175/500, loss = 0.00002389\n",
      "Avg Grad Norm: 0.0017, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.361926794052124 seconds ---\n",
      "epoch : 176/500, loss = 0.00002945\n",
      "Avg Grad Norm: 0.0019, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0002\n",
      "--- 0.36512303352355957 seconds ---\n",
      "epoch : 177/500, loss = 0.00004807\n",
      "Avg Grad Norm: 0.0031, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3596336841583252 seconds ---\n",
      "epoch : 178/500, loss = 0.00005502\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.3503262996673584 seconds ---\n",
      "epoch : 179/500, loss = 0.00007381\n",
      "Avg Grad Norm: 0.0040, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.35792016983032227 seconds ---\n",
      "epoch : 180/500, loss = 0.00009525\n",
      "Avg Grad Norm: 0.0051, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.3619265556335449 seconds ---\n",
      "epoch : 181/500, loss = 0.00009724\n",
      "Avg Grad Norm: 0.0052, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.35840868949890137 seconds ---\n",
      "epoch : 182/500, loss = 0.00008642\n",
      "Avg Grad Norm: 0.0050, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0006\n",
      "--- 0.36367106437683105 seconds ---\n",
      "epoch : 183/500, loss = 0.00007450\n",
      "Avg Grad Norm: 0.0042, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0005\n",
      "--- 0.35991787910461426 seconds ---\n",
      "epoch : 184/500, loss = 0.00004835\n",
      "Avg Grad Norm: 0.0033, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36411213874816895 seconds ---\n",
      "epoch : 185/500, loss = 0.00005260\n",
      "Avg Grad Norm: 0.0034, Avg Grad Mean: 0.0000, Avg Grad Std: 0.0004\n",
      "--- 0.36202454566955566 seconds ---\n",
      "epoch : 186/500, loss = 0.00004318\n",
      "Avg Grad Norm: 0.0027, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.35184335708618164 seconds ---\n",
      "epoch : 187/500, loss = 0.00003606\n",
      "Avg Grad Norm: 0.0025, Avg Grad Mean: -0.0000, Avg Grad Std: 0.0003\n",
      "--- 0.3633451461791992 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m lam_val \u001b[38;5;241m=\u001b[39m lambda_scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(best_out, inputs, best_params, true_params, best_index\u001b[38;5;241m.\u001b[39mfloat(), true_func\u001b[38;5;241m.\u001b[39mfloat(), lam_val)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     61\u001b[0m grad_norm, grad_mean, grad_std \u001b[38;5;241m=\u001b[39m compute_grad_stats(model)\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FeynmanEquations/.conda/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Multi_Func(functions[0:2], num_params, symbols, x_values, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "total_epochs = 500\n",
    "\n",
    "def loss_func(output_y, target_y, output_params, target_params, output_function, target_function, lam):\n",
    "    loss_class = nn.BCEWithLogitsLoss()\n",
    "    y_loss = torch.mean((output_y - target_y) ** 2)\n",
    "    params_loss = torch.mean((output_params - target_params) ** 2)\n",
    "    function_loss = loss_class(output_function.swapaxes(0,1), target_function)\n",
    "    return params_loss*lam + y_loss*(1-lam)\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 1*100\n",
    "    end_lambda = 0\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "lambda_scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "\n",
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_grad_stats(model):\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    grad_stds = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            grad_means.append(param.grad.mean().item())\n",
    "            grad_stds.append(param.grad.std().item())\n",
    "    return np.mean(grad_norms), np.mean(grad_means), np.mean(grad_stds)\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "grad_norms = []\n",
    "grad_means = []\n",
    "grad_stds = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "    epoch_grad_norms = []\n",
    "    epoch_grad_means = []\n",
    "    epoch_grad_stds = []\n",
    "\n",
    "    for batch_idx, (inputs, true_params, true_func) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        true_params = true_params.to(device)\n",
    "        true_func = true_func.to(device)\n",
    "        best_out,_,best_func,best_index,best_params,_,_,_= model(inputs)\n",
    "        lam_val = lambda_scheduler.get_last_lr()[0]\n",
    "        loss = loss_func(best_out, inputs, best_params, true_params, best_index.float(), true_func.float(), lam_val)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        grad_norm, grad_mean, grad_std = compute_grad_stats(model)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        epoch_grad_means.append(grad_mean)\n",
    "        epoch_grad_stds.append(grad_std)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "\n",
    "        '''for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: grad norm = {param.grad.norm()}, grad std = {param.grad.std()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")'''\n",
    "\n",
    "    scheduler.step()\n",
    "    lambda_scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    avg_grad_norm = np.mean(epoch_grad_norms)\n",
    "    avg_grad_mean = np.mean(epoch_grad_means)\n",
    "    avg_grad_std = np.mean(epoch_grad_stds)\n",
    "\n",
    "    grad_norms.append(avg_grad_norm)\n",
    "    grad_means.append(avg_grad_mean)\n",
    "    grad_stds.append(avg_grad_std)\n",
    "\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"Avg Grad Norm: {avg_grad_norm:.4f}, Avg Grad Mean: {avg_grad_mean:.4f}, Avg Grad Std: {avg_grad_std:.4f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, total_epochs + 1), train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot gradient norm\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, total_epochs + 1), grad_norms)\n",
    "plt.title('Average Gradient Norm')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Gradient Norm')\n",
    "\n",
    "# Plot gradient mean and std\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, total_epochs + 1), grad_means, label='Mean')\n",
    "plt.plot(range(1, total_epochs + 1), grad_stds, label='Std')\n",
    "plt.title('Gradient Mean and Std')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: tensor([0.4318, 0.0094, 0.5542, 0.0000, 0.0046], device='cuda:3',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "true_func: tensor([1., 0., 0., 0., 0.], device='cuda:3')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8881840550>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, all_params = model(y_values)\n",
    "ran = np.random.randint(best_func.shape[1])\n",
    "print(f\"best_func: {best_func[ran]}\")\n",
    "print(f\"true_func: {target_funcs[ran, 0:5]}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out[ran].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
