{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243124/343381410.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load('hold_data.pth')\n"
     ]
    }
   ],
   "source": [
    "loaded_data = torch.load('hold_data.pth')\n",
    "\n",
    "x_values = loaded_data['x_values'].to(device)\n",
    "y_values = loaded_data['y_values'].to(device)\n",
    "derivatives = loaded_data['derivatives'].to(device)\n",
    "params = loaded_data['param_values'].to(device)\n",
    "functions = loaded_data['formulas']\n",
    "symbols = loaded_data['symbols']\n",
    "num_params = loaded_data['num_params'].to(device)\n",
    "function_labels = loaded_data['function_labels'].to(device)\n",
    "#hessians = torch.load('hold_other.pth')['hessians'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([10000, 100])\n",
      "derivatives: torch.Size([10000, 100, 5])\n",
      "param_values: torch.Size([10000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n",
      "function_labels: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "print(f\"derivatives: {derivatives.shape}\")\n",
    "#print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")\n",
    "print(f\"function_labels: {function_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''h = hessians.flatten(1,2)\n",
    "d = derivatives.flatten(1,2)\n",
    "d = F.pad(d, (0,h.shape[1]-d.shape[1]))\n",
    "y = F.pad(y_values, (0,h.shape[1]-y_values.shape[1]))'''\n",
    "d = derivatives.flatten(1,2)\n",
    "y = F.pad(y_values, (0,d.shape[1]-y_values.shape[1]))\n",
    "input_data = torch.stack([d,y], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func_Channels(nn.Module):\n",
    "    def __init__(self, functions, num_params, x_data, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device)\n",
    "        self.input_channels = input_channels\n",
    "        self.num_params = num_params\n",
    "        self.max_params = max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "        self.epsilon = 1e-4\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        var_values = {str(symbols[j]): params[:, j] for j in range(len(symbols)-1)}\n",
    "        eval_func = sp.lambdify(symbols, formula, modules=\"numpy\")\n",
    "        #results = []\n",
    "        #for xi in x:\n",
    "        var_values[str(symbols[-1])] = x.unsqueeze(1)\n",
    "            #np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "        results = eval_func(**var_values)\n",
    "        results = torch.nan_to_num(results, 0)\n",
    "        #results.append(eval_func(**var_values))\n",
    "        #tensor_results = [torch.tensor(r, device=device) for r in results]\n",
    "        return results.swapaxes(0,1)\n",
    "    \n",
    "    def derivative(self, params, index):\n",
    "        derivatives = torch.zeros((params.shape[0], self.x_data.shape[0], self.max_params))\n",
    "        params_n = params.clone().detach().requires_grad_(True)\n",
    "        for p in range(len(symbols[index])-1):\n",
    "            plus = params_n.clone()\n",
    "            minus = params_n.clone()\n",
    "            plus[:,p] += self.epsilon\n",
    "            forward_values = self.evaluate(plus, index)\n",
    "            minus[:, p] -= self.epsilon\n",
    "            backward_values = self.evaluate(minus, index)\n",
    "            derivatives[:, :, p] = (forward_values - backward_values) / (2 * self.epsilon)\n",
    "        return derivatives.flatten(1,2)\n",
    "\n",
    "\n",
    "    '''def hessian(self, params, index):\n",
    "        params.requires_grad_(True)\n",
    "        y = self.evaluate(params, index)\n",
    "        grad = torch.autograd.grad(y.sum(), params, create_graph=True)[0]\n",
    "        return torch.stack([torch.autograd.grad(g, params, retain_graph=True)[0] for g in grad.flatten()]).reshape(params.shape + params.shape)'''\n",
    "\n",
    "\n",
    "    def hessian(self, params, index):\n",
    "        hessians = torch.zeros((params.shape[0], self.x_data.shape[0], self.max_params, self.max_params))\n",
    "        params_f = params.clone().detach().requires_grad_(True)\n",
    "        for j in range(len(symbols[index])-1):\n",
    "            for k in range(len(symbols[index])-1):\n",
    "                plus_plus = params_f.clone()\n",
    "                plus_minus = params_f.clone()\n",
    "                minus_plus = params_f.clone()\n",
    "                minus_minus = params_f.clone()\n",
    "\n",
    "                plus_plus[:, j] += self.epsilon\n",
    "                plus_plus[:, k] += self.epsilon\n",
    "\n",
    "                plus_minus[:, j] += self.epsilon\n",
    "                plus_minus[:, k] -= self.epsilon\n",
    "\n",
    "                minus_plus[:, j] -= self.epsilon\n",
    "                minus_plus[:, k] += self.epsilon\n",
    "\n",
    "                minus_minus[:, j] -= self.epsilon\n",
    "                minus_minus[:, k] -= self.epsilon\n",
    "\n",
    "                forward_forward = self.evaluate(plus_plus, index)\n",
    "                forward_backward = self.evaluate(plus_minus, index)\n",
    "                backward_forward = self.evaluate(minus_plus, index)\n",
    "                backward_backward = self.evaluate(minus_minus,index)\n",
    "                hessians[:, :, j, k] = (forward_forward - forward_backward - backward_forward + backward_backward) / (4 * self.epsilon **2)\n",
    "        return hessians.flatten(1,3)\n",
    "    \n",
    "    def loss_func(self, outputs, targets):\n",
    "        losses = []\n",
    "        outputs = outputs.permute(2,0,1)\n",
    "        targets = targets.permute(2,0,1)\n",
    "        for output, target in zip(outputs, targets):\n",
    "            loss = torch.mean(((target - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "        total_losses = torch.mean(torch.stack(losses), dim=0)\n",
    "        #l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        #l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        #total_losses += 0.01 * l2_reg + 0.01 * l1_reg\n",
    "        return total_losses\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        target = inputs.squeeze(dim=2)\n",
    "        outs = torch.swapaxes(inputs, 1, 2).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = torch.reshape(outs, (-1, 2, 128))\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        preds = []\n",
    "        pred_params = []\n",
    "        #hessians =[]\n",
    "\n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            pred_params.append(params)\n",
    "            y_vals = self.evaluate(params, f).to(self.device)\n",
    "            d_vals = self.derivative(params, f).to(self.device)\n",
    "            #h_vals = self.hessian(params, f).to(self.device)\n",
    "            #hessians.append(h_vals)\n",
    "            #d_vals = F.pad(d_vals, (0,h_vals.shape[1]-d_vals.shape[1]))\n",
    "            #y_vals = F.pad(y_vals, (0,h_vals.shape[1]-y_vals.shape[1]))\n",
    "            #d_vals = F.pad(d_vals, (0,2500-d_vals.shape[1]))\n",
    "            y_vals = F.pad(y_vals, (0,d_vals.shape[1]-y_vals.shape[1]))\n",
    "            #y_vals = F.pad(y_vals, (0,d_vals.shape[1]-y_vals.shape[1]))\n",
    "            # output = torch.stack([h_vals,d_vals,y_vals], dim=2).to(self.device)\n",
    "            output = torch.stack([d_vals,y_vals], dim=2).to(self.device)\n",
    "            outputs.append(output)\n",
    "            preds.append(y_vals)\n",
    "            loss = self.loss_func(output, target)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]  \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(preds).to(self.device)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0)\n",
    "        best_out = stacked_preds[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "        best_params = []\n",
    "        for index, value in enumerate(best_indexes):\n",
    "            best_params.append(pred_params[value][index])\n",
    "        return best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, pred_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss_func(model, l_val, output_y, target_y, output_func, target_func):\n",
    "    #target_max = torch.max(target_y, dim=-1, keepdim=True)[0]\n",
    "    #target_min = torch.min(target_y, dim=-1, keepdim=True)[0]\n",
    "    #target_range = torch.clamp(target_max - target_min, min=1e-6).squeeze(-1)\n",
    "\n",
    "    mse_loss_y = torch.mean((output_y - target_y) ** 2)\n",
    "    mse_loss_func = (output_func == target_func).float().mean()\n",
    "    #print(f\"output_y: {output_y[0][0:100]}\")\n",
    "    #print(f\"target_y: {target_y[0][0]}\")\n",
    "    #print(f\"output_func: {output_func[0]}\")\n",
    "    #print(f\"target_func: {target_func[0]}\")\n",
    "    print(f\"\\nmse_loss_y: {mse_loss_y}\")\n",
    "    print(f\"mse_loss_func: {mse_loss_func}\")\n",
    "    total_loss = (mse_loss_y*(1-l_val)) + (mse_loss_func*l_val)\n",
    "    print(f\"l_val: {l_val}\")\n",
    "    print(f\"total loss: {total_loss}\")\n",
    "    #normalized_loss = mse_loss / target_range\n",
    "    \n",
    "    #l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    #l1_reg = sum(p.abs().sum() for p in model.parameters())    \n",
    "    #total_loss = torch.mean(normalized_loss) + 0.01 * l2_reg + 0.01 * l1_reg\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mse_loss_y: 650451392.0\n",
      "mse_loss_func: 0.5680000185966492\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.5680006742477417\n",
      "\n",
      "mse_loss_y: 80465984.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0920000895857811\n",
      "\n",
      "mse_loss_y: 5188622.0\n",
      "mse_loss_func: 0.0820000022649765\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0820000097155571\n",
      "\n",
      "mse_loss_y: 22115378.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000002592802048\n",
      "\n",
      "mse_loss_y: 28151472.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600003063678741\n",
      "\n",
      "mse_loss_y: 31082970.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200003534555435\n",
      "\n",
      "mse_loss_y: 1250839.25\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100000351667404\n",
      "\n",
      "mse_loss_y: 4189521.25\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000900030136\n",
      "\n",
      "mse_loss_y: 206206960.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800021141767502\n",
      "\n",
      "mse_loss_y: 286549952.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400028735399246\n",
      "epoch : 0/50, loss = 0.14218592\n",
      "--- 10.063415765762329 seconds ---\n",
      "\n",
      "mse_loss_y: 226572992.0\n",
      "mse_loss_func: 0.11800000816583633\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11800023168325424\n",
      "\n",
      "mse_loss_y: 22039072.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600002318620682\n",
      "\n",
      "mse_loss_y: 5172347.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000894069672\n",
      "\n",
      "mse_loss_y: 2344491.75\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700000822544098\n",
      "\n",
      "mse_loss_y: 74661192.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1050000786781311\n",
      "\n",
      "mse_loss_y: 48690576.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100005775690079\n",
      "\n",
      "mse_loss_y: 324958880.0\n",
      "mse_loss_func: 0.08800000697374344\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0880003347992897\n",
      "\n",
      "mse_loss_y: 5543876.5\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400001376867294\n",
      "\n",
      "mse_loss_y: 299828576.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600030422210693\n",
      "\n",
      "mse_loss_y: 26687486.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500003397464752\n",
      "epoch : 1/50, loss = 0.09999011\n",
      "--- 10.32418942451477 seconds ---\n",
      "\n",
      "mse_loss_y: 283196224.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500028729438782\n",
      "\n",
      "mse_loss_y: 1718148.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000083446503\n",
      "\n",
      "mse_loss_y: 3944269.25\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400001376867294\n",
      "\n",
      "mse_loss_y: 6139100.5\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600001364946365\n",
      "\n",
      "mse_loss_y: 400436352.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300040990114212\n",
      "\n",
      "mse_loss_y: 63150044.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300006181001663\n",
      "\n",
      "mse_loss_y: 198860400.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600020736455917\n",
      "\n",
      "mse_loss_y: 6984938.5\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 71635408.0\n",
      "mse_loss_func: 0.08500000089406967\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08500007539987564\n",
      "\n",
      "mse_loss_y: 434727.3125\n",
      "mse_loss_func: 0.12000000476837158\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.12000000476837158\n",
      "epoch : 2/50, loss = 0.09999011\n",
      "--- 10.446610927581787 seconds ---\n",
      "\n",
      "mse_loss_y: 36822800.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700004011392593\n",
      "\n",
      "mse_loss_y: 9979632.0\n",
      "mse_loss_func: 0.11600000411272049\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11600001156330109\n",
      "\n",
      "mse_loss_y: 328010976.0\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400033414363861\n",
      "\n",
      "mse_loss_y: 159206912.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000015795230865\n",
      "\n",
      "mse_loss_y: 97693216.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300010442733765\n",
      "\n",
      "mse_loss_y: 72576912.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800007939338684\n",
      "\n",
      "mse_loss_y: 322285888.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500032663345337\n",
      "\n",
      "mse_loss_y: 5891930.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800001233816147\n",
      "\n",
      "mse_loss_y: 99725.6171875\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000028014183\n",
      "\n",
      "mse_loss_y: 3931550.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100001096725464\n",
      "epoch : 3/50, loss = 0.09999011\n",
      "--- 10.14356780052185 seconds ---\n",
      "\n",
      "mse_loss_y: 26154746.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300003200769424\n",
      "\n",
      "mse_loss_y: 7895701.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100001096725464\n",
      "\n",
      "mse_loss_y: 372620992.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800037741661072\n",
      "\n",
      "mse_loss_y: 8487428.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400000959634781\n",
      "\n",
      "mse_loss_y: 284428416.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600028395652771\n",
      "\n",
      "mse_loss_y: 52885.3515625\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500000417232513\n",
      "\n",
      "mse_loss_y: 211156288.0\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300021409988403\n",
      "\n",
      "mse_loss_y: 42230552.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500004887580872\n",
      "\n",
      "mse_loss_y: 77751512.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0920000821352005\n",
      "\n",
      "mse_loss_y: 5720987.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300000965595245\n",
      "epoch : 4/50, loss = 0.09999011\n",
      "--- 10.08614730834961 seconds ---\n",
      "\n",
      "mse_loss_y: 109191688.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600011259317398\n",
      "\n",
      "mse_loss_y: 3994855.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 3157271.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400000423192978\n",
      "\n",
      "mse_loss_y: 283543616.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600028932094574\n",
      "\n",
      "mse_loss_y: 231768416.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600023716688156\n",
      "\n",
      "mse_loss_y: 9319802.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000894069672\n",
      "\n",
      "mse_loss_y: 1689588.125\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000149011612\n",
      "\n",
      "mse_loss_y: 365290336.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400036931037903\n",
      "\n",
      "mse_loss_y: 23941038.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600002318620682\n",
      "\n",
      "mse_loss_y: 4602888.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900001227855682\n",
      "epoch : 5/50, loss = 0.09999011\n",
      "--- 10.25791883468628 seconds ---\n",
      "\n",
      "mse_loss_y: 21295410.0\n",
      "mse_loss_func: 0.08300000429153442\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08300002664327621\n",
      "\n",
      "mse_loss_y: 1505636.5\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600000619888306\n",
      "\n",
      "mse_loss_y: 234607104.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600023716688156\n",
      "\n",
      "mse_loss_y: 34108972.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100004076957703\n",
      "\n",
      "mse_loss_y: 311034752.0\n",
      "mse_loss_func: 0.11400000751018524\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11400032043457031\n",
      "\n",
      "mse_loss_y: 4242631.5\n",
      "mse_loss_func: 0.08300000429153442\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08300001174211502\n",
      "\n",
      "mse_loss_y: 4097129.75\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400000959634781\n",
      "\n",
      "mse_loss_y: 23974550.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.104000024497509\n",
      "\n",
      "mse_loss_y: 50196320.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200005978345871\n",
      "\n",
      "mse_loss_y: 351436992.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700035840272903\n",
      "epoch : 6/50, loss = 0.09999011\n",
      "--- 10.666842699050903 seconds ---\n",
      "\n",
      "mse_loss_y: 34679140.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500004351139069\n",
      "\n",
      "mse_loss_y: 303855168.0\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11200030893087387\n",
      "\n",
      "mse_loss_y: 5078753.5\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 91638960.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1050000935792923\n",
      "\n",
      "mse_loss_y: 73087864.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.096000075340271\n",
      "\n",
      "mse_loss_y: 1629352.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100000351667404\n",
      "\n",
      "mse_loss_y: 5044150.5\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300001293420792\n",
      "\n",
      "mse_loss_y: 36648696.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000004082918167\n",
      "\n",
      "mse_loss_y: 43050848.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600004553794861\n",
      "\n",
      "mse_loss_y: 441786560.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300044178962708\n",
      "epoch : 7/50, loss = 0.09999011\n",
      "--- 9.833747386932373 seconds ---\n",
      "\n",
      "mse_loss_y: 291156576.0\n",
      "mse_loss_func: 0.08300000429153442\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0830002948641777\n",
      "\n",
      "mse_loss_y: 29778954.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100003331899643\n",
      "\n",
      "mse_loss_y: 4240317.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000102519989\n",
      "\n",
      "mse_loss_y: 299995296.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900030493736267\n",
      "\n",
      "mse_loss_y: 1091103.75\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200000554323196\n",
      "\n",
      "mse_loss_y: 99673408.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600009769201279\n",
      "\n",
      "mse_loss_y: 2448442.25\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400000423192978\n",
      "\n",
      "mse_loss_y: 278602752.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000028252601624\n",
      "\n",
      "mse_loss_y: 6037125.5\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000001430511475\n",
      "\n",
      "mse_loss_y: 23475602.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700002521276474\n",
      "epoch : 8/50, loss = 0.09999011\n",
      "--- 9.751325368881226 seconds ---\n",
      "\n",
      "mse_loss_y: 5286135.5\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300000965595245\n",
      "\n",
      "mse_loss_y: 67835704.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0960000678896904\n",
      "\n",
      "mse_loss_y: 77994056.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300008207559586\n",
      "\n",
      "mse_loss_y: 23633638.0\n",
      "mse_loss_func: 0.11400000751018524\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11400002986192703\n",
      "\n",
      "mse_loss_y: 43819744.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400004893541336\n",
      "\n",
      "mse_loss_y: 918442.8125\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800000488758087\n",
      "\n",
      "mse_loss_y: 161818.171875\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200000762939453\n",
      "\n",
      "mse_loss_y: 47314624.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600005090236664\n",
      "\n",
      "mse_loss_y: 609526400.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200061649084091\n",
      "\n",
      "mse_loss_y: 160008880.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1020001620054245\n",
      "epoch : 9/50, loss = 0.09999011\n",
      "--- 10.195881128311157 seconds ---\n",
      "\n",
      "mse_loss_y: 28798946.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.091000035405159\n",
      "\n",
      "mse_loss_y: 5453120.5\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 174441.5\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700000286102295\n",
      "\n",
      "mse_loss_y: 372795936.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0970003753900528\n",
      "\n",
      "mse_loss_y: 4085198.5\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 285822336.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1170002892613411\n",
      "\n",
      "mse_loss_y: 168809456.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700017958879471\n",
      "\n",
      "mse_loss_y: 126671904.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900013357400894\n",
      "\n",
      "mse_loss_y: 81252.1796875\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000083446503\n",
      "\n",
      "mse_loss_y: 43806916.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800004959106445\n",
      "epoch : 10/50, loss = 0.09999011\n",
      "--- 10.139860153198242 seconds ---\n",
      "\n",
      "mse_loss_y: 328315040.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1050003319978714\n",
      "\n",
      "mse_loss_y: 198795744.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.103000208735466\n",
      "\n",
      "mse_loss_y: 655653.5\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800000488758087\n",
      "\n",
      "mse_loss_y: 5417991.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800001233816147\n",
      "\n",
      "mse_loss_y: 72452728.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700008064508438\n",
      "\n",
      "mse_loss_y: 300226272.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700030088424683\n",
      "\n",
      "mse_loss_y: 95975552.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600010305643082\n",
      "\n",
      "mse_loss_y: 1590400.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400000423192978\n",
      "\n",
      "mse_loss_y: 32863134.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900003671646118\n",
      "\n",
      "mse_loss_y: 207007.421875\n",
      "mse_loss_func: 0.08300000429153442\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08300000429153442\n",
      "epoch : 11/50, loss = 0.09999011\n",
      "--- 10.68127155303955 seconds ---\n",
      "\n",
      "mse_loss_y: 468060448.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200047492980957\n",
      "\n",
      "mse_loss_y: 950816.8125\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500000417232513\n",
      "\n",
      "mse_loss_y: 1440330.125\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000083446503\n",
      "\n",
      "mse_loss_y: 310741920.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200031846761703\n",
      "\n",
      "mse_loss_y: 21222410.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600002855062485\n",
      "\n",
      "mse_loss_y: 151584080.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100015252828598\n",
      "\n",
      "mse_loss_y: 98865.921875\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300000548362732\n",
      "\n",
      "mse_loss_y: 2924531.75\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200000762939453\n",
      "\n",
      "mse_loss_y: 71580352.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100008010864258\n",
      "\n",
      "mse_loss_y: 7895772.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200001507997513\n",
      "epoch : 12/50, loss = 0.09999011\n",
      "--- 10.615950584411621 seconds ---\n",
      "\n",
      "mse_loss_y: 128544536.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800013154745102\n",
      "\n",
      "mse_loss_y: 301405376.0\n",
      "mse_loss_func: 0.12000000476837158\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.12000030279159546\n",
      "\n",
      "mse_loss_y: 24797450.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500002861022949\n",
      "\n",
      "mse_loss_y: 230462384.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200023651123047\n",
      "\n",
      "mse_loss_y: 21263202.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000002384185791\n",
      "\n",
      "mse_loss_y: 4016802.75\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000828504562\n",
      "\n",
      "mse_loss_y: 31288156.0\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400003612041473\n",
      "\n",
      "mse_loss_y: 5852966.0\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100001633167267\n",
      "\n",
      "mse_loss_y: 284129056.0\n",
      "mse_loss_func: 0.07700000703334808\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.07700029015541077\n",
      "\n",
      "mse_loss_y: 4739600.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700001358985901\n",
      "epoch : 13/50, loss = 0.09999011\n",
      "--- 10.368114471435547 seconds ---\n",
      "\n",
      "mse_loss_y: 408001024.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600041061639786\n",
      "\n",
      "mse_loss_y: 76079448.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500008076429367\n",
      "\n",
      "mse_loss_y: 5363177.0\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11200001090765\n",
      "\n",
      "mse_loss_y: 6024238.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000900030136\n",
      "\n",
      "mse_loss_y: 175391.546875\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600000619888306\n",
      "\n",
      "mse_loss_y: 26204732.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000003665685654\n",
      "\n",
      "mse_loss_y: 308797760.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700031369924545\n",
      "\n",
      "mse_loss_y: 56909.77734375\n",
      "mse_loss_func: 0.08500000089406967\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08500000089406967\n",
      "\n",
      "mse_loss_y: 163735424.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100016951560974\n",
      "\n",
      "mse_loss_y: 42061420.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900004953145981\n",
      "epoch : 14/50, loss = 0.09999011\n",
      "--- 9.576603651046753 seconds ---\n",
      "\n",
      "mse_loss_y: 28527048.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300003737211227\n",
      "\n",
      "mse_loss_y: 19207354.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800002723932266\n",
      "\n",
      "mse_loss_y: 166475936.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700017005205154\n",
      "\n",
      "mse_loss_y: 3637268.25\n",
      "mse_loss_func: 0.08000000566244125\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08000000566244125\n",
      "\n",
      "mse_loss_y: 43106680.0\n",
      "mse_loss_func: 0.11500000208616257\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11500004678964615\n",
      "\n",
      "mse_loss_y: 23458362.0\n",
      "mse_loss_func: 0.08500000089406967\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08500002324581146\n",
      "\n",
      "mse_loss_y: 678938176.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.090000681579113\n",
      "\n",
      "mse_loss_y: 4833421.5\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 67995144.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200007259845734\n",
      "\n",
      "mse_loss_y: 320108.78125\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100000888109207\n",
      "epoch : 15/50, loss = 0.09999011\n",
      "--- 9.449630498886108 seconds ---\n",
      "\n",
      "mse_loss_y: 8431210.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500001162290573\n",
      "\n",
      "mse_loss_y: 1405647.625\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000000357627869\n",
      "\n",
      "mse_loss_y: 178635.75\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0990000069141388\n",
      "\n",
      "mse_loss_y: 392532224.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900039970874786\n",
      "\n",
      "mse_loss_y: 28312560.0\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300003528594971\n",
      "\n",
      "mse_loss_y: 87310280.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000009089708328\n",
      "\n",
      "mse_loss_y: 24796818.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600002318620682\n",
      "\n",
      "mse_loss_y: 856412.9375\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700000286102295\n",
      "\n",
      "mse_loss_y: 459017312.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0900004655122757\n",
      "\n",
      "mse_loss_y: 33658456.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100004076957703\n",
      "epoch : 16/50, loss = 0.09999011\n",
      "--- 10.185378551483154 seconds ---\n",
      "\n",
      "mse_loss_y: 4585141.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000102519989\n",
      "\n",
      "mse_loss_y: 67796936.0\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100007593631744\n",
      "\n",
      "mse_loss_y: 7326426.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 7659317.5\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 563129.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000028014183\n",
      "\n",
      "mse_loss_y: 373538912.0\n",
      "mse_loss_func: 0.08500000089406967\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08500037342309952\n",
      "\n",
      "mse_loss_y: 4498767.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600001037120819\n",
      "\n",
      "mse_loss_y: 350831616.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800035506486893\n",
      "\n",
      "mse_loss_y: 164054080.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.098000168800354\n",
      "\n",
      "mse_loss_y: 55645164.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500005632638931\n",
      "epoch : 17/50, loss = 0.09999011\n",
      "--- 9.730586290359497 seconds ---\n",
      "\n",
      "mse_loss_y: 300586272.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900029957294464\n",
      "\n",
      "mse_loss_y: 69748248.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400007128715515\n",
      "\n",
      "mse_loss_y: 156103.875\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0950000062584877\n",
      "\n",
      "mse_loss_y: 1357871.5\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200000554323196\n",
      "\n",
      "mse_loss_y: 7482307.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000001102685928\n",
      "\n",
      "mse_loss_y: 94218864.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600009769201279\n",
      "\n",
      "mse_loss_y: 47075152.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600005090236664\n",
      "\n",
      "mse_loss_y: 194812336.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600019991397858\n",
      "\n",
      "mse_loss_y: 310566848.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900031983852386\n",
      "\n",
      "mse_loss_y: 10495415.0\n",
      "mse_loss_func: 0.12300000339746475\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.12300001084804535\n",
      "epoch : 18/50, loss = 0.09999011\n",
      "--- 9.9270658493042 seconds ---\n",
      "\n",
      "mse_loss_y: 67739288.0\n",
      "mse_loss_func: 0.08700000494718552\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08700007200241089\n",
      "\n",
      "mse_loss_y: 272252.34375\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000000357627869\n",
      "\n",
      "mse_loss_y: 551777.4375\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700000822544098\n",
      "\n",
      "mse_loss_y: 423218848.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600042551755905\n",
      "\n",
      "mse_loss_y: 10341968.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000001430511475\n",
      "\n",
      "mse_loss_y: 182774480.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000019311904907\n",
      "\n",
      "mse_loss_y: 27951234.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100003331899643\n",
      "\n",
      "mse_loss_y: 11127132.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 361815.09375\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300000220537186\n",
      "\n",
      "mse_loss_y: 312160736.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400031507015228\n",
      "epoch : 19/50, loss = 0.09999011\n",
      "--- 10.597522020339966 seconds ---\n",
      "\n",
      "mse_loss_y: 282761280.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500028938055038\n",
      "\n",
      "mse_loss_y: 343092608.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700035095214844\n",
      "\n",
      "mse_loss_y: 80097320.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700008809566498\n",
      "\n",
      "mse_loss_y: 81049624.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800008475780487\n",
      "\n",
      "mse_loss_y: 33855712.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200004488229752\n",
      "\n",
      "mse_loss_y: 4743119.5\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700001031160355\n",
      "\n",
      "mse_loss_y: 180534832.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0980001837015152\n",
      "\n",
      "mse_loss_y: 86893.5546875\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0990000069141388\n",
      "\n",
      "mse_loss_y: 4418877.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0910000130534172\n",
      "\n",
      "mse_loss_y: 25859260.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600002318620682\n",
      "epoch : 20/50, loss = 0.09999011\n",
      "--- 8.388853549957275 seconds ---\n",
      "\n",
      "mse_loss_y: 7414957.5\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900001436471939\n",
      "\n",
      "mse_loss_y: 48041576.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800004750490189\n",
      "\n",
      "mse_loss_y: 283549952.0\n",
      "mse_loss_func: 0.11600000411272049\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11600028723478317\n",
      "\n",
      "mse_loss_y: 234378544.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500023514032364\n",
      "\n",
      "mse_loss_y: 37712752.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300003945827484\n",
      "\n",
      "mse_loss_y: 24426882.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800002723932266\n",
      "\n",
      "mse_loss_y: 2481206.5\n",
      "mse_loss_func: 0.08800000697374344\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08800000697374344\n",
      "\n",
      "mse_loss_y: 96510760.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700009971857071\n",
      "\n",
      "mse_loss_y: 2047631.625\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200000762939453\n",
      "\n",
      "mse_loss_y: 299935264.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400030016899109\n",
      "epoch : 21/50, loss = 0.09999011\n",
      "--- 9.537174463272095 seconds ---\n",
      "\n",
      "mse_loss_y: 345527104.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300035029649734\n",
      "\n",
      "mse_loss_y: 14703778.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000001639127731\n",
      "\n",
      "mse_loss_y: 1051287.25\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100000560283661\n",
      "\n",
      "mse_loss_y: 36591.7265625\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200000762939453\n",
      "\n",
      "mse_loss_y: 226770560.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800022840499878\n",
      "\n",
      "mse_loss_y: 37487768.0\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100004613399506\n",
      "\n",
      "mse_loss_y: 98860136.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300009906291962\n",
      "\n",
      "mse_loss_y: 283027488.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200028866529465\n",
      "\n",
      "mse_loss_y: 1223717.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500000417232513\n",
      "\n",
      "mse_loss_y: 27811092.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500003397464752\n",
      "epoch : 22/50, loss = 0.09999011\n",
      "--- 14.843381404876709 seconds ---\n",
      "\n",
      "mse_loss_y: 42606712.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500005096197128\n",
      "\n",
      "mse_loss_y: 609719744.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600061178207397\n",
      "\n",
      "mse_loss_y: 1363836.375\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000154972076\n",
      "\n",
      "mse_loss_y: 33767484.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900004208087921\n",
      "\n",
      "mse_loss_y: 28507168.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200003743171692\n",
      "\n",
      "mse_loss_y: 9434935.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000828504562\n",
      "\n",
      "mse_loss_y: 3971251.75\n",
      "mse_loss_func: 0.11600000411272049\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11600001156330109\n",
      "\n",
      "mse_loss_y: 67818072.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000007063150406\n",
      "\n",
      "mse_loss_y: 6191050.5\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600001364946365\n",
      "\n",
      "mse_loss_y: 233119312.0\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100023984909058\n",
      "epoch : 23/50, loss = 0.09999011\n",
      "--- 13.837970733642578 seconds ---\n",
      "\n",
      "mse_loss_y: 29662846.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000003337860107\n",
      "\n",
      "mse_loss_y: 4962335.0\n",
      "mse_loss_func: 0.07800000160932541\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.078000009059906\n",
      "\n",
      "mse_loss_y: 27764808.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000003665685654\n",
      "\n",
      "mse_loss_y: 498438.6875\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700000286102295\n",
      "\n",
      "mse_loss_y: 364241280.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200037062168121\n",
      "\n",
      "mse_loss_y: 109744520.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400011390447617\n",
      "\n",
      "mse_loss_y: 5843289.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000001102685928\n",
      "\n",
      "mse_loss_y: 772254.5\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900000482797623\n",
      "\n",
      "mse_loss_y: 301564896.0\n",
      "mse_loss_func: 0.11400000751018524\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11400030553340912\n",
      "\n",
      "mse_loss_y: 191444912.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600019991397858\n",
      "epoch : 24/50, loss = 0.09999011\n",
      "--- 13.73443078994751 seconds ---\n",
      "\n",
      "mse_loss_y: 28324666.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200003534555435\n",
      "\n",
      "mse_loss_y: 3751591.5\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500001162290573\n",
      "\n",
      "mse_loss_y: 41921168.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700004756450653\n",
      "\n",
      "mse_loss_y: 1924893.625\n",
      "mse_loss_func: 0.07700000703334808\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.07700000703334808\n",
      "\n",
      "mse_loss_y: 2391074.5\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700000822544098\n",
      "\n",
      "mse_loss_y: 166216096.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500016808509827\n",
      "\n",
      "mse_loss_y: 685754304.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300068765878677\n",
      "\n",
      "mse_loss_y: 25770918.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900002926588058\n",
      "\n",
      "mse_loss_y: 982370.9375\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900000482797623\n",
      "\n",
      "mse_loss_y: 79462464.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600008815526962\n",
      "epoch : 25/50, loss = 0.09999011\n",
      "--- 13.88330626487732 seconds ---\n",
      "\n",
      "mse_loss_y: 772258.8125\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100000888109207\n",
      "\n",
      "mse_loss_y: 11130011.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000828504562\n",
      "\n",
      "mse_loss_y: 148230.46875\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000149011612\n",
      "\n",
      "mse_loss_y: 283911936.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900028795003891\n",
      "\n",
      "mse_loss_y: 62926668.0\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400006592273712\n",
      "\n",
      "mse_loss_y: 163114048.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300016611814499\n",
      "\n",
      "mse_loss_y: 744438.875\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200000762939453\n",
      "\n",
      "mse_loss_y: 64111712.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0960000678896904\n",
      "\n",
      "mse_loss_y: 71839576.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200008004903793\n",
      "\n",
      "mse_loss_y: 377800640.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700038611888885\n",
      "epoch : 26/50, loss = 0.09999011\n",
      "--- 14.417349100112915 seconds ---\n",
      "\n",
      "mse_loss_y: 34011100.0\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1130000427365303\n",
      "\n",
      "mse_loss_y: 1191582.25\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0950000062584877\n",
      "\n",
      "mse_loss_y: 181048832.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600017964839935\n",
      "\n",
      "mse_loss_y: 480193.90625\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000154972076\n",
      "\n",
      "mse_loss_y: 145992048.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600015193223953\n",
      "\n",
      "mse_loss_y: 329449376.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600032866001129\n",
      "\n",
      "mse_loss_y: 4387133.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000102519989\n",
      "\n",
      "mse_loss_y: 301810592.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400030016899109\n",
      "\n",
      "mse_loss_y: 37873356.0\n",
      "mse_loss_func: 0.11400000751018524\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11400004476308823\n",
      "\n",
      "mse_loss_y: 255342.828125\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0990000069141388\n",
      "epoch : 27/50, loss = 0.09999011\n",
      "--- 10.006277322769165 seconds ---\n",
      "\n",
      "mse_loss_y: 23948484.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700003057718277\n",
      "\n",
      "mse_loss_y: 4637009.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000894069672\n",
      "\n",
      "mse_loss_y: 300128160.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400030016899109\n",
      "\n",
      "mse_loss_y: 13370270.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900001972913742\n",
      "\n",
      "mse_loss_y: 33151204.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300003200769424\n",
      "\n",
      "mse_loss_y: 325966880.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900032937526703\n",
      "\n",
      "mse_loss_y: 4734259.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900001227855682\n",
      "\n",
      "mse_loss_y: 165656144.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300017148256302\n",
      "\n",
      "mse_loss_y: 97253152.0\n",
      "mse_loss_func: 0.0820000022649765\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08200009912252426\n",
      "\n",
      "mse_loss_y: 67653928.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400006920099258\n",
      "epoch : 28/50, loss = 0.09999011\n",
      "--- 9.259985208511353 seconds ---\n",
      "\n",
      "mse_loss_y: 703086.125\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0950000062584877\n",
      "\n",
      "mse_loss_y: 5692377.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000894069672\n",
      "\n",
      "mse_loss_y: 29012704.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800003260374069\n",
      "\n",
      "mse_loss_y: 373260416.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400037467479706\n",
      "\n",
      "mse_loss_y: 57332136.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700006783008575\n",
      "\n",
      "mse_loss_y: 41861172.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700005292892456\n",
      "\n",
      "mse_loss_y: 237872976.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000024199485779\n",
      "\n",
      "mse_loss_y: 286032544.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400028735399246\n",
      "\n",
      "mse_loss_y: 4617218.5\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600001364946365\n",
      "\n",
      "mse_loss_y: 114880.03125\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000154972076\n",
      "epoch : 29/50, loss = 0.09999011\n",
      "--- 9.902065753936768 seconds ---\n",
      "\n",
      "mse_loss_y: 8021153.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000900030136\n",
      "\n",
      "mse_loss_y: 303865536.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000031232833862\n",
      "\n",
      "mse_loss_y: 10336054.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300000965595245\n",
      "\n",
      "mse_loss_y: 5787750.5\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300001293420792\n",
      "\n",
      "mse_loss_y: 73008664.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400007873773575\n",
      "\n",
      "mse_loss_y: 33322944.0\n",
      "mse_loss_func: 0.11900000274181366\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11900003254413605\n",
      "\n",
      "mse_loss_y: 72704.5078125\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100000560283661\n",
      "\n",
      "mse_loss_y: 504403872.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600051283836365\n",
      "\n",
      "mse_loss_y: 92104752.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1010000929236412\n",
      "\n",
      "mse_loss_y: 5576112.0\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400001376867294\n",
      "epoch : 30/50, loss = 0.09999011\n",
      "--- 9.459781646728516 seconds ---\n",
      "\n",
      "mse_loss_y: 32032812.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.091000035405159\n",
      "\n",
      "mse_loss_y: 109551072.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200011730194092\n",
      "\n",
      "mse_loss_y: 167861904.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600017219781876\n",
      "\n",
      "mse_loss_y: 13285517.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600001782178879\n",
      "\n",
      "mse_loss_y: 283290464.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900028795003891\n",
      "\n",
      "mse_loss_y: 1826287.875\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700000286102295\n",
      "\n",
      "mse_loss_y: 118064584.0\n",
      "mse_loss_func: 0.125\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.12500011920928955\n",
      "\n",
      "mse_loss_y: 4826812.5\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500001162290573\n",
      "\n",
      "mse_loss_y: 305553152.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300030767917633\n",
      "\n",
      "mse_loss_y: 206960.9375\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000083446503\n",
      "epoch : 31/50, loss = 0.09999011\n",
      "--- 9.289484024047852 seconds ---\n",
      "\n",
      "mse_loss_y: 283191328.0\n",
      "mse_loss_func: 0.11100000888109207\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11100029200315475\n",
      "\n",
      "mse_loss_y: 169115280.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600017756223679\n",
      "\n",
      "mse_loss_y: 300067552.0\n",
      "mse_loss_func: 0.08100000023841858\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08100029826164246\n",
      "\n",
      "mse_loss_y: 103761952.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200010985136032\n",
      "\n",
      "mse_loss_y: 9878982.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 352021.21875\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000149011612\n",
      "\n",
      "mse_loss_y: 76139712.0\n",
      "mse_loss_func: 0.08300000429153442\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0830000787973404\n",
      "\n",
      "mse_loss_y: 91992800.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700009554624557\n",
      "\n",
      "mse_loss_y: 119344.3203125\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0950000062584877\n",
      "\n",
      "mse_loss_y: 1880528.75\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300000756978989\n",
      "epoch : 32/50, loss = 0.09999011\n",
      "--- 9.632258892059326 seconds ---\n",
      "\n",
      "mse_loss_y: 21609244.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000002384185791\n",
      "\n",
      "mse_loss_y: 30254144.0\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11200003325939178\n",
      "\n",
      "mse_loss_y: 303867648.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600031167268753\n",
      "\n",
      "mse_loss_y: 110764.296875\n",
      "mse_loss_func: 0.08800000697374344\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08800000697374344\n",
      "\n",
      "mse_loss_y: 239742784.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800024330615997\n",
      "\n",
      "mse_loss_y: 46147124.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1000000461935997\n",
      "\n",
      "mse_loss_y: 28477690.0\n",
      "mse_loss_func: 0.08700000494718552\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0870000347495079\n",
      "\n",
      "mse_loss_y: 282779040.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300028532743454\n",
      "\n",
      "mse_loss_y: 77993472.0\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000008136034012\n",
      "\n",
      "mse_loss_y: 5517621.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600001364946365\n",
      "epoch : 33/50, loss = 0.09999011\n",
      "--- 9.976273536682129 seconds ---\n",
      "\n",
      "mse_loss_y: 8018962.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700001031160355\n",
      "\n",
      "mse_loss_y: 6267394.5\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900001227855682\n",
      "\n",
      "mse_loss_y: 106154840.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0970001071691513\n",
      "\n",
      "mse_loss_y: 283043200.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900029003620148\n",
      "\n",
      "mse_loss_y: 313712544.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300032049417496\n",
      "\n",
      "mse_loss_y: 25967310.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700002521276474\n",
      "\n",
      "mse_loss_y: 78245.4609375\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400000631809235\n",
      "\n",
      "mse_loss_y: 109771560.0\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11200011521577835\n",
      "\n",
      "mse_loss_y: 182850448.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700018912553787\n",
      "\n",
      "mse_loss_y: 635008.6875\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500000417232513\n",
      "epoch : 34/50, loss = 0.09999011\n",
      "--- 9.739676475524902 seconds ---\n",
      "\n",
      "mse_loss_y: 71311376.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700007736682892\n",
      "\n",
      "mse_loss_y: 28209110.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900003671646118\n",
      "\n",
      "mse_loss_y: 191616272.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200019925832748\n",
      "\n",
      "mse_loss_y: 6677507.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100001096725464\n",
      "\n",
      "mse_loss_y: 77057952.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000007599592209\n",
      "\n",
      "mse_loss_y: 283305056.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500028729438782\n",
      "\n",
      "mse_loss_y: 48909876.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1000000536441803\n",
      "\n",
      "mse_loss_y: 29179752.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09000003337860107\n",
      "\n",
      "mse_loss_y: 404081.75\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600000619888306\n",
      "\n",
      "mse_loss_y: 299828544.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000029951334\n",
      "epoch : 35/50, loss = 0.09999011\n",
      "--- 9.61471176147461 seconds ---\n",
      "\n",
      "mse_loss_y: 77631536.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100008010864258\n",
      "\n",
      "mse_loss_y: 1163901.625\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200000762939453\n",
      "\n",
      "mse_loss_y: 280930.625\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400000214576721\n",
      "\n",
      "mse_loss_y: 310017728.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300031512975693\n",
      "\n",
      "mse_loss_y: 311128640.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300032049417496\n",
      "\n",
      "mse_loss_y: 158932368.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700015932321548\n",
      "\n",
      "mse_loss_y: 32252834.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600003600120544\n",
      "\n",
      "mse_loss_y: 115235624.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200011938810349\n",
      "\n",
      "mse_loss_y: 8594999.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0910000130534172\n",
      "\n",
      "mse_loss_y: 21260932.0\n",
      "mse_loss_func: 0.13100001215934753\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.13100002706050873\n",
      "epoch : 36/50, loss = 0.09999011\n",
      "--- 9.891312599182129 seconds ---\n",
      "\n",
      "mse_loss_y: 52300832.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100005567073822\n",
      "\n",
      "mse_loss_y: 3336490.75\n",
      "mse_loss_func: 0.12400000542402267\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.12400000542402267\n",
      "\n",
      "mse_loss_y: 94202984.0\n",
      "mse_loss_func: 0.08800000697374344\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0880001038312912\n",
      "\n",
      "mse_loss_y: 8787639.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000828504562\n",
      "\n",
      "mse_loss_y: 274782.65625\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600000619888306\n",
      "\n",
      "mse_loss_y: 283907424.0\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400028944015503\n",
      "\n",
      "mse_loss_y: 5443070.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400001168251038\n",
      "\n",
      "mse_loss_y: 230212848.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10900023579597473\n",
      "\n",
      "mse_loss_y: 304862720.0\n",
      "mse_loss_func: 0.10900000482797623\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1090003103017807\n",
      "\n",
      "mse_loss_y: 53170788.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900005370378494\n",
      "epoch : 37/50, loss = 0.09999011\n",
      "--- 10.041803121566772 seconds ---\n",
      "\n",
      "mse_loss_y: 372132800.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900037944316864\n",
      "\n",
      "mse_loss_y: 78648200.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600008279085159\n",
      "\n",
      "mse_loss_y: 24000074.0\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11200002580881119\n",
      "\n",
      "mse_loss_y: 4677475.5\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000900030136\n",
      "\n",
      "mse_loss_y: 6725335.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 901190.5\n",
      "mse_loss_func: 0.11400000751018524\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11400000751018524\n",
      "\n",
      "mse_loss_y: 113599.6171875\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400000214576721\n",
      "\n",
      "mse_loss_y: 7671232.5\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000102519989\n",
      "\n",
      "mse_loss_y: 469710208.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400047361850739\n",
      "\n",
      "mse_loss_y: 71919456.0\n",
      "mse_loss_func: 0.0820000022649765\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08200007677078247\n",
      "epoch : 38/50, loss = 0.09999011\n",
      "--- 10.041757822036743 seconds ---\n",
      "\n",
      "mse_loss_y: 360393.71875\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000000685453415\n",
      "\n",
      "mse_loss_y: 1134881.875\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700000822544098\n",
      "\n",
      "mse_loss_y: 40934696.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900003880262375\n",
      "\n",
      "mse_loss_y: 472283136.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700047224760056\n",
      "\n",
      "mse_loss_y: 610650.1875\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100000560283661\n",
      "\n",
      "mse_loss_y: 282709088.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800028800964355\n",
      "\n",
      "mse_loss_y: 72451032.0\n",
      "mse_loss_func: 0.0820000022649765\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08200007677078247\n",
      "\n",
      "mse_loss_y: 160502240.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600017011165619\n",
      "\n",
      "mse_loss_y: 5021623.0\n",
      "mse_loss_func: 0.1210000067949295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1210000142455101\n",
      "\n",
      "mse_loss_y: 491794.03125\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0990000069141388\n",
      "epoch : 39/50, loss = 0.09999011\n",
      "--- 9.201133251190186 seconds ---\n",
      "\n",
      "mse_loss_y: 47692756.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800004750490189\n",
      "\n",
      "mse_loss_y: 329699424.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600033074617386\n",
      "\n",
      "mse_loss_y: 282787936.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600028604269028\n",
      "\n",
      "mse_loss_y: 21158468.0\n",
      "mse_loss_func: 0.08500000089406967\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08500002324581146\n",
      "\n",
      "mse_loss_y: 24304418.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500002652406693\n",
      "\n",
      "mse_loss_y: 1804511.875\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600000083446503\n",
      "\n",
      "mse_loss_y: 1198285.875\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300000756978989\n",
      "\n",
      "mse_loss_y: 24325110.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600002855062485\n",
      "\n",
      "mse_loss_y: 4095047.75\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700001358985901\n",
      "\n",
      "mse_loss_y: 299433632.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800030082464218\n",
      "epoch : 40/50, loss = 0.09999011\n",
      "--- 10.5732581615448 seconds ---\n",
      "\n",
      "mse_loss_y: 111016.2421875\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0990000069141388\n",
      "\n",
      "mse_loss_y: 126387.4765625\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500000417232513\n",
      "\n",
      "mse_loss_y: 74371648.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100007802248001\n",
      "\n",
      "mse_loss_y: 42382872.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800004959106445\n",
      "\n",
      "mse_loss_y: 76407328.0\n",
      "mse_loss_func: 0.11500000208616257\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11500007659196854\n",
      "\n",
      "mse_loss_y: 4522130.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200001507997513\n",
      "\n",
      "mse_loss_y: 307673760.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200031101703644\n",
      "\n",
      "mse_loss_y: 4557986.0\n",
      "mse_loss_func: 0.11700000613927841\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11700001358985901\n",
      "\n",
      "mse_loss_y: 293871328.0\n",
      "mse_loss_func: 0.08400000631809235\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08400029689073563\n",
      "\n",
      "mse_loss_y: 232474992.0\n",
      "mse_loss_func: 0.08700000494718552\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08700023591518402\n",
      "epoch : 41/50, loss = 0.09999011\n",
      "--- 9.20504093170166 seconds ---\n",
      "\n",
      "mse_loss_y: 76285384.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000007599592209\n",
      "\n",
      "mse_loss_y: 5059655.0\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0910000130534172\n",
      "\n",
      "mse_loss_y: 194523.828125\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600000619888306\n",
      "\n",
      "mse_loss_y: 23514226.0\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11200002580881119\n",
      "\n",
      "mse_loss_y: 5749837.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000894069672\n",
      "\n",
      "mse_loss_y: 39452652.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1000000387430191\n",
      "\n",
      "mse_loss_y: 226439712.0\n",
      "mse_loss_func: 0.08700000494718552\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08700022846460342\n",
      "\n",
      "mse_loss_y: 295453984.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800030082464218\n",
      "\n",
      "mse_loss_y: 300794944.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300030022859573\n",
      "\n",
      "mse_loss_y: 63554588.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300007462501526\n",
      "epoch : 42/50, loss = 0.09999011\n",
      "--- 9.445346593856812 seconds ---\n",
      "\n",
      "mse_loss_y: 5786298.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0950000137090683\n",
      "\n",
      "mse_loss_y: 306459104.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400030761957169\n",
      "\n",
      "mse_loss_y: 4001321.75\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900000900030136\n",
      "\n",
      "mse_loss_y: 74087624.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200008004903793\n",
      "\n",
      "mse_loss_y: 25889636.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000251531601\n",
      "\n",
      "mse_loss_y: 285868576.0\n",
      "mse_loss_func: 0.11600000411272049\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11600028723478317\n",
      "\n",
      "mse_loss_y: 114344064.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500011593103409\n",
      "\n",
      "mse_loss_y: 194486880.0\n",
      "mse_loss_func: 0.09000000357627869\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0900001972913742\n",
      "\n",
      "mse_loss_y: 23955262.0\n",
      "mse_loss_func: 0.08800000697374344\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08800002932548523\n",
      "\n",
      "mse_loss_y: 1620766.5\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300000756978989\n",
      "epoch : 43/50, loss = 0.09999011\n",
      "--- 9.42466139793396 seconds ---\n",
      "\n",
      "mse_loss_y: 54076096.0\n",
      "mse_loss_func: 0.0820000022649765\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08200005441904068\n",
      "\n",
      "mse_loss_y: 286523904.0\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1040002852678299\n",
      "\n",
      "mse_loss_y: 33737528.0\n",
      "mse_loss_func: 0.08700000494718552\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0870000422000885\n",
      "\n",
      "mse_loss_y: 112635024.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500011801719666\n",
      "\n",
      "mse_loss_y: 504745.21875\n",
      "mse_loss_func: 0.09100000560283661\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09100000560283661\n",
      "\n",
      "mse_loss_y: 374019232.0\n",
      "mse_loss_func: 0.11800000816583633\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11800038069486618\n",
      "\n",
      "mse_loss_y: 5779372.5\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700001567602158\n",
      "\n",
      "mse_loss_y: 4762572.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 315284.78125\n",
      "mse_loss_func: 0.1120000034570694\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1120000034570694\n",
      "\n",
      "mse_loss_y: 164145744.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1020001694560051\n",
      "epoch : 44/50, loss = 0.09999011\n",
      "--- 9.752016544342041 seconds ---\n",
      "\n",
      "mse_loss_y: 303459872.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200031101703644\n",
      "\n",
      "mse_loss_y: 66459992.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300006926059723\n",
      "\n",
      "mse_loss_y: 68278200.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500007331371307\n",
      "\n",
      "mse_loss_y: 289619968.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900029748678207\n",
      "\n",
      "mse_loss_y: 6535718.5\n",
      "mse_loss_func: 0.12300000339746475\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.12300001084804535\n",
      "\n",
      "mse_loss_y: 29527678.0\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300003200769424\n",
      "\n",
      "mse_loss_y: 7687097.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600001364946365\n",
      "\n",
      "mse_loss_y: 193819.390625\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1080000028014183\n",
      "\n",
      "mse_loss_y: 97083688.0\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200010240077972\n",
      "\n",
      "mse_loss_y: 167653504.0\n",
      "mse_loss_func: 0.07900000363588333\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.07900016754865646\n",
      "epoch : 45/50, loss = 0.09999011\n",
      "--- 9.342386722564697 seconds ---\n",
      "\n",
      "mse_loss_y: 28990416.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400003403425217\n",
      "\n",
      "mse_loss_y: 306053248.0\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100030899047852\n",
      "\n",
      "mse_loss_y: 10492129.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800001233816147\n",
      "\n",
      "mse_loss_y: 283298208.0\n",
      "mse_loss_func: 0.08500000089406967\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08500028401613235\n",
      "\n",
      "mse_loss_y: 166640832.0\n",
      "mse_loss_func: 0.09600000083446503\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09600016474723816\n",
      "\n",
      "mse_loss_y: 70420776.0\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400007128715515\n",
      "\n",
      "mse_loss_y: 446601.6875\n",
      "mse_loss_func: 0.10100000351667404\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10100000351667404\n",
      "\n",
      "mse_loss_y: 28221926.0\n",
      "mse_loss_func: 0.1080000028014183\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10800003260374069\n",
      "\n",
      "mse_loss_y: 31038.96484375\n",
      "mse_loss_func: 0.11000000685453415\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11000000685453415\n",
      "\n",
      "mse_loss_y: 141904352.0\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300014704465866\n",
      "epoch : 46/50, loss = 0.09999011\n",
      "--- 9.510072469711304 seconds ---\n",
      "\n",
      "mse_loss_y: 1273629.75\n",
      "mse_loss_func: 0.09300000220537186\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09300000220537186\n",
      "\n",
      "mse_loss_y: 24958690.0\n",
      "mse_loss_func: 0.10700000822544098\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10700003057718277\n",
      "\n",
      "mse_loss_y: 5854705.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000894069672\n",
      "\n",
      "mse_loss_y: 201710352.0\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09500020742416382\n",
      "\n",
      "mse_loss_y: 39325412.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300004482269287\n",
      "\n",
      "mse_loss_y: 94174992.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200010448694229\n",
      "\n",
      "mse_loss_y: 1642481.5\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000149011612\n",
      "\n",
      "mse_loss_y: 7446681.5\n",
      "mse_loss_func: 0.09400000423192978\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09400001168251038\n",
      "\n",
      "mse_loss_y: 372404096.0\n",
      "mse_loss_func: 0.11800000816583633\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11800038069486618\n",
      "\n",
      "mse_loss_y: 287708480.0\n",
      "mse_loss_func: 0.09800000488758087\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09800029546022415\n",
      "epoch : 47/50, loss = 0.09999011\n",
      "--- 7.538285255432129 seconds ---\n",
      "\n",
      "mse_loss_y: 4641441.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300001502037048\n",
      "\n",
      "mse_loss_y: 44939540.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900005161762238\n",
      "\n",
      "mse_loss_y: 1458738.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000000149011612\n",
      "\n",
      "mse_loss_y: 294672544.0\n",
      "mse_loss_func: 0.10600000619888306\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10600030422210693\n",
      "\n",
      "mse_loss_y: 4102905.25\n",
      "mse_loss_func: 0.10200000554323196\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10200001299381256\n",
      "\n",
      "mse_loss_y: 4148949.5\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300001293420792\n",
      "\n",
      "mse_loss_y: 68836888.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600006997585297\n",
      "\n",
      "mse_loss_y: 29328598.0\n",
      "mse_loss_func: 0.10000000149011612\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10000003129243851\n",
      "\n",
      "mse_loss_y: 164793328.0\n",
      "mse_loss_func: 0.0860000029206276\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08600016683340073\n",
      "\n",
      "mse_loss_y: 419576640.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10500042140483856\n",
      "epoch : 48/50, loss = 0.09999011\n",
      "--- 11.123444318771362 seconds ---\n",
      "\n",
      "mse_loss_y: 287146176.0\n",
      "mse_loss_func: 0.09200000762939453\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09200029820203781\n",
      "\n",
      "mse_loss_y: 72129392.0\n",
      "mse_loss_func: 0.10500000417232513\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.1050000786781311\n",
      "\n",
      "mse_loss_y: 3740097.25\n",
      "mse_loss_func: 0.0950000062584877\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.0950000137090683\n",
      "\n",
      "mse_loss_y: 28301572.0\n",
      "mse_loss_func: 0.0990000069141388\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09900003671646118\n",
      "\n",
      "mse_loss_y: 3750753.0\n",
      "mse_loss_func: 0.11300000548362732\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.11300001293420792\n",
      "\n",
      "mse_loss_y: 261061504.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300026834011078\n",
      "\n",
      "mse_loss_y: 69970648.0\n",
      "mse_loss_func: 0.09700000286102295\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.09700006991624832\n",
      "\n",
      "mse_loss_y: 9000242.0\n",
      "mse_loss_func: 0.10300000756978989\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10300001502037048\n",
      "\n",
      "mse_loss_y: 1029498.25\n",
      "mse_loss_func: 0.10400000214576721\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.10400000214576721\n",
      "\n",
      "mse_loss_y: 300369632.0\n",
      "mse_loss_func: 0.08900000154972076\n",
      "l_val: 0.999999999999999\n",
      "total loss: 0.08900029957294464\n",
      "epoch : 49/50, loss = 0.09999011\n",
      "--- 12.800737380981445 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Func_Channels(functions=functions, num_params=num_params, x_data=x_values, input_channels=2, device=device).to(device)\n",
    "dataset = torch.utils.data.TensorDataset(input_data, function_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "total_epochs = 50\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    start_lambda = 0.999999999999999*100\n",
    "    end_lambda = 0.5*100\n",
    "    return start_lambda - (start_lambda - end_lambda) * min(epoch / total_epochs, 1.0)\n",
    "\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "l_val_sched = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "# loss_func = nn.MSELoss()\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 1\n",
    "    model.train()\n",
    "    \n",
    "    for train, labels in dataloader:\n",
    "        train = train.requires_grad_(True).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, pred_params = model(train)\n",
    "        #print(f\"\\npredicted: {best_indexes[0].item()}\")\n",
    "        #print(f\"actual: {labels[0].item()}\")\n",
    "        lambda_val = l_val_sched.get_last_lr()[0]\n",
    "        loss = training_loss_func(model, lambda_val, best_out, train[:,:,0], best_indexes, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{total_epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2201610/2087370462.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  other_model = torch.load('model.pth')\n"
     ]
    }
   ],
   "source": [
    "other_model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 100])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_func: [k*x]\n",
      "best_loss: tensor([5.2119], device='cuda:4', grad_fn=<MinBackward0>)\n",
      "best_params: [tensor([-0.2869], device='cuda:4', grad_fn=<SelectBackward0>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efb15786050>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "ran = np.random.randint(5000, y_values.shape[0])\n",
    "best_out, best_loss, best_func, best_indexes, best_params, stacked_preds, stacked_losses, pred_params = model(train)\n",
    "print(f\"best_func: {best_func}\")\n",
    "print(f\"best_loss: {best_loss}\")\n",
    "print(f\"best_params: {best_params}\")\n",
    "plt.plot(x_values.detach().cpu().numpy(), y_values[ran].detach().cpu().numpy(), label='True')\n",
    "plt.plot(x_values.detach().cpu().numpy(), best_out.squeeze(0)[0:100].detach().cpu().numpy(), label='Predicted')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
