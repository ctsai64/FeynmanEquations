{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2201610/1158532762.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load('hold_data.pth')\n",
      "/tmp/ipykernel_2201610/1158532762.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hessians = torch.load('hold_other.pth')['hessians'].to(device)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = torch.load('hold_data.pth')\n",
    "\n",
    "x_values = loaded_data['x_values'].to(device)\n",
    "y_values = loaded_data['y_values'].to(device)\n",
    "derivatives = loaded_data['derivatives'].to(device)\n",
    "params = loaded_data['param_values'].to(device)\n",
    "functions = loaded_data['formulas']\n",
    "symbols = loaded_data['symbols']\n",
    "num_params = loaded_data['num_params'].to(device)\n",
    "hessians = torch.load('hold_other.pth')['hessians'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_values: torch.Size([100])\n",
      "y_values: torch.Size([10000, 100])\n",
      "derivatives: torch.Size([10000, 100, 5])\n",
      "hessians: torch.Size([10000, 100, 25])\n",
      "param_values: torch.Size([10000, 5])\n",
      "formulas: 10\n",
      "symbols: 10\n",
      "num_params: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_values: {x_values.shape}\")\n",
    "print(f\"y_values: {y_values.shape}\")\n",
    "print(f\"derivatives: {derivatives.shape}\")\n",
    "print(f\"hessians: {hessians.shape}\")\n",
    "print(f\"param_values: {params.shape}\")\n",
    "print(f\"formulas: {len(functions)}\")\n",
    "print(f\"symbols: {len(symbols)}\")\n",
    "print(f\"num_params: {num_params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hessians.flatten(1,2)\n",
    "d = derivatives.flatten(1,2)\n",
    "d = F.pad(d, (0,h.shape[1]-d.shape[1]))\n",
    "y = F.pad(y_values, (0,h.shape[1]-y_values.shape[1]))\n",
    "full_data = torch.stack([y,d,h], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Func_Channels(nn.Module):\n",
    "    def __init__(self, functions, num_params, x_data, input_channels, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.functions = functions\n",
    "        self.x_data = x_data.to(self.device)\n",
    "        self.input_channels = input_channels\n",
    "        self.num_params = num_params\n",
    "        self.max_params = 5 #max(num_params)\n",
    "        self.total_params = sum(self.num_params)\n",
    "        self.symbols = symbols\n",
    "        self.epsilon = 1e-4\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.input_channels, out_channels=8, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8, out_channels=6, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(64)\n",
    "        )\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(16),\n",
    "            nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(8),\n",
    "            nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AdaptiveAvgPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, self.total_params),\n",
    "        )\n",
    "\n",
    "    def evaluate(self, params, index):\n",
    "        symbols = self.symbols[index]\n",
    "        formula = self.functions[index]\n",
    "        x = self.x_data\n",
    "        var_values = {str(symbols[j]): params[:, j] for j in range(len(symbols)-1)}\n",
    "        eval_func = sp.lambdify(symbols, formula, modules=\"numpy\")\n",
    "        #results = []\n",
    "        #for xi in x:\n",
    "        var_values[str(symbols[-1])] = x.unsqueeze(1)\n",
    "            #np_values = {str(sym): var_values[sym].detach().cpu().numpy() for sym in symbols}\n",
    "        results = eval_func(**var_values)\n",
    "        #results.append(eval_func(**var_values))\n",
    "        #tensor_results = [torch.tensor(r, device=device) for r in results]\n",
    "        return results.swapaxes(0,1)\n",
    "    \n",
    "    def derivative(self, params, index):\n",
    "        derivatives = torch.zeros((params.shape[0], self.x_data.shape[0], self.max_params))\n",
    "        params_n = params.clone().detach().requires_grad_(True)\n",
    "        for p in range(len(symbols[index])-1):\n",
    "            plus = params_n.clone()\n",
    "            minus = params_n.clone()\n",
    "            plus[:,p] += self.epsilon\n",
    "            forward_values = self.evaluate(plus, index)\n",
    "            minus[:, p] -= self.epsilon\n",
    "            backward_values = self.evaluate(minus, index)\n",
    "            derivatives[:, :, p] = (forward_values - backward_values) / (2 * self.epsilon)\n",
    "        return derivatives\n",
    "\n",
    "    def hessian(self, params, index):\n",
    "        hessians = torch.zeros((params.shape[0], self.x_data.shape[0], self.max_params, self.max_params))\n",
    "        params_f = params.clone().detach().requires_grad_(True)\n",
    "        for j in range(len(symbols[index])-1):\n",
    "            for k in range(len(symbols[index])-1):\n",
    "                plus_plus = params_f.clone()\n",
    "                plus_minus = params_f.clone()\n",
    "                minus_plus = params_f.clone()\n",
    "                minus_minus = params_f.clone()\n",
    "\n",
    "                plus_plus[:, j] += self.epsilon\n",
    "                plus_plus[:, k] += self.epsilon\n",
    "\n",
    "                plus_minus[:, j] += self.epsilon\n",
    "                plus_minus[:, k] -= self.epsilon\n",
    "\n",
    "                minus_plus[:, j] -= self.epsilon\n",
    "                minus_plus[:, k] += self.epsilon\n",
    "\n",
    "                minus_minus[:, j] -= self.epsilon\n",
    "                minus_minus[:, k] -= self.epsilon\n",
    "\n",
    "                forward_forward = self.evaluate(plus_plus, index)\n",
    "                forward_backward = self.evaluate(plus_minus, index)\n",
    "                backward_forward = self.evaluate(minus_plus, index)\n",
    "                backward_backward = self.evaluate(minus_minus,index)\n",
    "                hessians[:, :, j, k] = (forward_forward - forward_backward - backward_forward + backward_backward) / (4 * self.epsilon **2)\n",
    "        return hessians\n",
    "    \n",
    "    def loss_func(self, outputs, targets):\n",
    "        losses = []\n",
    "        outputs = outputs.permute(2,0,1)\n",
    "        targets = targets.permute(2,0,1)\n",
    "        for output, target in zip(outputs, targets):\n",
    "            loss = torch.mean(((target - output) ** 2), dim=1)\n",
    "            losses.append(loss)\n",
    "        total_losses = torch.mean(torch.stack(losses), dim=0)\n",
    "        #l2_reg = sum(p.pow(2.0).sum() for p in self.parameters())\n",
    "        #l1_reg = sum(p.abs().sum() for p in self.parameters())\n",
    "        #total_losses += 0.01 * l2_reg + 0.01 * l1_reg\n",
    "        return total_losses\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        target = inputs.squeeze(dim=2)\n",
    "        outs = torch.swapaxes(inputs, 1, 2).to(self.device)\n",
    "        outs = self.hidden_x1(outs)\n",
    "        xfc = torch.reshape(outs, (-1, 256))\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "\n",
    "        outs = torch.reshape(outs, (-1, 2, 128))\n",
    "        outs = self.hidden_x2(outs)\n",
    "        cnn_flat = self.flatten_layer(outs)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)\n",
    "        embedding = self.hidden_embedding(encoded)\n",
    "\n",
    "        start_index = 0\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        preds = []\n",
    "\n",
    "        for f in range(len(self.functions)):\n",
    "            params = embedding[:, start_index:start_index+self.num_params[f]]\n",
    "            y_vals = self.evaluate(params, f).to(self.device)\n",
    "            d_vals = self.derivative(params, f).to(self.device)\n",
    "            h_vals = self.hessian(params, f).to(self.device)\n",
    "            h_vals = h_vals.flatten(1,3)\n",
    "            d_vals = d_vals.flatten(1,2)\n",
    "            d_vals = F.pad(d_vals, (0,h_vals.shape[1]-d_vals.shape[1]))\n",
    "            y_vals = F.pad(y_vals, (0,h_vals.shape[1]-y_vals.shape[1]))\n",
    "            output = torch.stack([h_vals,d_vals,y_vals], dim=2).to(self.device)\n",
    "            outputs.append(output)\n",
    "            preds.append(y_vals)\n",
    "            loss = self.loss_func(output, target)\n",
    "            losses.append(loss)\n",
    "            start_index += self.num_params[f]       \n",
    "        stacked_losses = torch.stack(losses).to(self.device)\n",
    "        stacked_preds = torch.stack(preds).to(self.device)\n",
    "        best_loss, best_indexes = torch.min(stacked_losses, dim=0)\n",
    "        best_out = stacked_preds[best_indexes, -1]\n",
    "        best_func = [self.functions[idx] for idx in best_indexes]\n",
    "        return best_out, best_loss, best_func, stacked_preds, stacked_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loss_func(model, output, target):\n",
    "    target_max = torch.max(target, dim=-1, keepdim=True)[0]\n",
    "    target_min = torch.min(target, dim=-1, keepdim=True)[0]\n",
    "    target_range = torch.clamp(target_max - target_min, min=1e-6).squeeze(-1)\n",
    "    \n",
    "    mse_loss = torch.mean((output - target) ** 2, dim=-1)\n",
    "    normalized_loss = mse_loss / target_range\n",
    "    \n",
    "    l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    l1_reg = sum(p.abs().sum() for p in model.parameters())    \n",
    "    total_loss = torch.mean(normalized_loss) + 0.01 * l2_reg + 0.01 * l1_reg\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x**3\n",
      "epoch : 0/50, loss = 9.13264801\n",
      "--- 36.7505898475647 seconds ---\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "t/x\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "t/x\n",
      "epoch : 1/50, loss = 2.84328400\n",
      "--- 31.70383906364441 seconds ---\n",
      "k*x**3\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "epoch : 2/50, loss = 1.31043722\n",
      "--- 31.268638849258423 seconds ---\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "epoch : 3/50, loss = 0.71778477\n",
      "--- 30.863661527633667 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "k*x\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "t/x\n",
      "k*x\n",
      "epoch : 4/50, loss = 0.44373810\n",
      "--- 31.846604108810425 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "epoch : 5/50, loss = 0.31073710\n",
      "--- 32.75914025306702 seconds ---\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "t/x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 6/50, loss = 0.24263088\n",
      "--- 31.905547380447388 seconds ---\n",
      "k*x\n",
      "a*x**2/2\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "t/x\n",
      "k*x\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "epoch : 7/50, loss = 0.20660596\n",
      "--- 32.07942199707031 seconds ---\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "t/x\n",
      "t/x\n",
      "k*x**3\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "epoch : 8/50, loss = 0.18586531\n",
      "--- 32.30614376068115 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x**3\n",
      "epoch : 9/50, loss = 0.17329373\n",
      "--- 32.068838357925415 seconds ---\n",
      "a*x**2/2\n",
      "G*m1*m2/x**2\n",
      "G*m1*m2/x**2\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "epoch : 10/50, loss = 0.16525611\n",
      "--- 33.9225492477417 seconds ---\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "t/x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "epoch : 11/50, loss = 0.15990651\n",
      "--- 31.606294631958008 seconds ---\n",
      "k*x\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "epoch : 12/50, loss = 0.15619204\n",
      "--- 33.43437337875366 seconds ---\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x**3\n",
      "epoch : 13/50, loss = 0.15359257\n",
      "--- 31.089256525039673 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "G*m1*m2/x**2\n",
      "t/x\n",
      "k*x\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "epoch : 14/50, loss = 0.15173987\n",
      "--- 33.42554545402527 seconds ---\n",
      "k*x\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "k*x**3\n",
      "k*x\n",
      "G*m1*m2/x**2\n",
      "G*m1*m2/x**2\n",
      "k*x**3\n",
      "k*x\n",
      "epoch : 15/50, loss = 0.15047805\n",
      "--- 32.149996757507324 seconds ---\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "epoch : 16/50, loss = 0.14958347\n",
      "--- 30.471742153167725 seconds ---\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "epoch : 17/50, loss = 0.14895491\n",
      "--- 39.195539474487305 seconds ---\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "k*x\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "epoch : 18/50, loss = 0.14851257\n",
      "--- 40.122416973114014 seconds ---\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "t/x\n",
      "G*m1*m2/x**2\n",
      "t/x\n",
      "G*m1*m2/x**2\n",
      "t/x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "epoch : 19/50, loss = 0.14820645\n",
      "--- 40.76316547393799 seconds ---\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x\n",
      "k*x**3\n",
      "epoch : 20/50, loss = 0.14798977\n",
      "--- 29.64465618133545 seconds ---\n",
      "G*m1*m2/x**2\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "epoch : 21/50, loss = 0.14783885\n",
      "--- 32.296303033828735 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "epoch : 22/50, loss = 0.14773335\n",
      "--- 34.18152594566345 seconds ---\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "epoch : 23/50, loss = 0.14765935\n",
      "--- 33.28473901748657 seconds ---\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "epoch : 24/50, loss = 0.14760890\n",
      "--- 32.35913348197937 seconds ---\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 25/50, loss = 0.14757279\n",
      "--- 32.63757252693176 seconds ---\n",
      "k*x**3\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "k*x\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "epoch : 26/50, loss = 0.14754747\n",
      "--- 32.609134674072266 seconds ---\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "k*x**3\n",
      "k*x\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 27/50, loss = 0.14752975\n",
      "--- 32.626826763153076 seconds ---\n",
      "a*x**2/2\n",
      "t/x\n",
      "t/x\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x**3\n",
      "t/x\n",
      "epoch : 28/50, loss = 0.14751737\n",
      "--- 32.49483823776245 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x**3\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "epoch : 29/50, loss = 0.14750872\n",
      "--- 33.092944622039795 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x\n",
      "k*x\n",
      "G*m1*m2/x**2\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x\n",
      "k*x\n",
      "k*x\n",
      "epoch : 30/50, loss = 0.14750269\n",
      "--- 32.18341827392578 seconds ---\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "t/x\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "epoch : 31/50, loss = 0.14749845\n",
      "--- 32.42445516586304 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "G*m1*m2/x**2\n",
      "a*x**2/2\n",
      "t/x\n",
      "t/x\n",
      "k*x\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "epoch : 32/50, loss = 0.14749543\n",
      "--- 31.946911096572876 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 33/50, loss = 0.14749335\n",
      "--- 31.85512137413025 seconds ---\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "epoch : 34/50, loss = 0.14749188\n",
      "--- 32.56193137168884 seconds ---\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 35/50, loss = 0.14749087\n",
      "--- 31.401045083999634 seconds ---\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "epoch : 36/50, loss = 0.14749016\n",
      "--- 41.894535541534424 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "G*m1*m2/x**2\n",
      "epoch : 37/50, loss = 0.14748966\n",
      "--- 40.992632150650024 seconds ---\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x\n",
      "t/x\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x\n",
      "k*x\n",
      "k*x\n",
      "epoch : 38/50, loss = 0.14748931\n",
      "--- 40.31531071662903 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "a*x**2/2\n",
      "G*m1*m2/x**2\n",
      "t/x\n",
      "k*x\n",
      "G*m1*m2/x**2\n",
      "a*x**2/2\n",
      "epoch : 39/50, loss = 0.14748906\n",
      "--- 37.80760979652405 seconds ---\n",
      "a*x**2/2\n",
      "k*x\n",
      "t/x\n",
      "k*x**3\n",
      "t/x\n",
      "k*x\n",
      "k*x\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "epoch : 40/50, loss = 0.14748890\n",
      "--- 30.138400554656982 seconds ---\n",
      "a*x**2/2\n",
      "k*x\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "epoch : 41/50, loss = 0.14748877\n",
      "--- 31.050126552581787 seconds ---\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "G*m1*m2/x**2\n",
      "a*x**2 + b*x + c\n",
      "t/x\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "epoch : 42/50, loss = 0.14748868\n",
      "--- 31.223198175430298 seconds ---\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "epoch : 43/50, loss = 0.14748862\n",
      "--- 32.6745970249176 seconds ---\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "k*x\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x\n",
      "t/x\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "epoch : 44/50, loss = 0.14748858\n",
      "--- 31.36252450942993 seconds ---\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x\n",
      "t/x\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "G*m1*m2/x**2\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 45/50, loss = 0.14748855\n",
      "--- 32.54917001724243 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "k*x\n",
      "t/x\n",
      "t/x\n",
      "a*x**2/2\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "epoch : 46/50, loss = 0.14748853\n",
      "--- 32.15784931182861 seconds ---\n",
      "a*x**2 + b*x + c\n",
      "k*x\n",
      "k*x**3\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "t/x\n",
      "t/x\n",
      "k*x\n",
      "epoch : 47/50, loss = 0.14748852\n",
      "--- 30.557297945022583 seconds ---\n",
      "t/x\n",
      "k*x\n",
      "a*x**2 + b*x + c\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x**3\n",
      "k*x\n",
      "a*x**2/2\n",
      "epoch : 48/50, loss = 0.14748851\n",
      "--- 32.03194737434387 seconds ---\n",
      "k*x\n",
      "a*x**2/2\n",
      "t/x\n",
      "a*x**2/2\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "G*m1*m2/x**2\n",
      "k*x**3\n",
      "a*x**2/2\n",
      "k*x\n",
      "epoch : 49/50, loss = 0.14748850\n",
      "--- 31.127458572387695 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model = Multi_Func_Channels(functions=functions, num_params=num_params, x_data=x_values, input_channels=3, device=device).to(device)\n",
    "\n",
    "dataloader = DataLoader(full_data, batch_size=1000, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    total_num = 1\n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = batch.requires_grad_(True).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        best_out, best_loss, best_func, preds, losses = model(batch)\n",
    "        loss = training_loss_func(model, best_out, batch[:, :, 0]) #loss_func(best_out, batch[:, :, 0])\n",
    "        # FIX CRAZY LOSS VALUES\n",
    "        #print(f\"loss: {loss}\")\n",
    "        #plt.plot(x_values.detach().cpu().numpy(), best_out[0, 0:100].detach().cpu().numpy())\n",
    "        #plt.plot(x_values.detach().cpu().numpy(), y_values[0, 0:100].detach().cpu().numpy())\n",
    "        print(best_func[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * best_out.shape[0]\n",
    "        total_num += best_out.shape[0]\n",
    "    scheduler.step()\n",
    "    train_loss /= total_num\n",
    "    print(f\"epoch : {epoch}/{epochs}, loss = {train_loss:.8f}\")\n",
    "    print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2500, 3])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "full_data[0].unsqueeze(0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
